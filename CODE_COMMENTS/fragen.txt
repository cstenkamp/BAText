-Das DeepMind-Paper dass sich nen internes model der physik aufbaut?
-Geht Reinforcement-Learnng irgendwie mit lang- mittel- und kurzfristigen zielen?


One of the most relevant questions is: HOW do we define the reward at every timestep.
1) Feedbacks so oft setzen, dass er in jedem Frame reward (unterschied bestzeit) bekommt. Problem: Wahrscheinlich kaum langfristiges Planen
2) Reward gibts nur GANZ am ende der Runde. Problem: Er wird extrem langsam lernen [war das nicht nen bekanntes problem? Go? Schach? Irgendwas wo die auch in dem paper geschummelt haben dass er eher reward bekommt?]
3) Monte-Carlo-Fahren von hier an. Problem: Dafür wird er ewig brauchen, da er das ja immer wieder ausprobieren muss... also da gibts
   a) sich ne eigene Physik-Simulation bauen
   b) das tatsächlich fahren, Feedback angucken, Auto wieder resetten (Probleme: braucht ewig, bräuchte reset-funktion)


Andere Frage: Was soll das Network überhaupt optimieren?
-In the MNIST tutorial, this function is the sum of incorrect classifications. 
-Nvidia’s Autopilot uses the difference between the predicted and recorded steering angle.
-Euclidian distance between predicted and recorded output vector

-------------------------------------------------Design Decisions---------------------------------------------------------
-Bei DQN war ja die gesamte bisherige history der State.. wie ist das gecodet? Weil prinzipiell, wenn man das so macht, brauche ich recurrent zu sein und die history des speedstears...
Scheinen 2 verschiedene Möglichkeiten zu sein wenn ich das richtig sehe.

-Wie viel hilft das aktuelles Speed und desired speed zu haben UND actual throttle und desired throttle? Ist das dann quasi model-based? Sowohl mit als auch ohne lernen lassen?

-Welche Optimizer steht auf https://www.tensorflow.org/api_guides/python/train#optimizers ne Auswahl PLUS paper-referenz das erklärt wie die each funktionieren!







------------------------------------------EHEMALIGES----------------------------------------------
TODO:
-maybe nicht anhand vom vision-vektor, sondern vom lookahead vektor gas, bremse, discrete steering erstmal unabhängig lernen als baseline?
 ->so einfach ist das nicht, weil er ja bisheriges speed & steer braucht.... ODER die history der letzten, daraus könnte er sich das ziehen
