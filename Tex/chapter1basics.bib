
@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {{\textcopyright} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true},
	doi = {10.1038/nature14236},
	abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
	language = {en},
	number = {7540},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	keywords = {Computer science},
	pages = {529--533},
	file = {Full Text PDF:C\:\\Users\\Marie\\Zotero\\storage\\JP7IS4Y3\\Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf;Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\RC6ZEXKN\\nature14236.html:text/html}
}

@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2017-08-12},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Learning},
	annote = {Comment: AAAI 2016},
	file = {arXiv\:1509.06461 PDF:C\:\\Users\\Marie\\Zotero\\storage\\GD9HV9ZG\\van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\YKVGT94V\\1509.html:text/html}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2017-08-12},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1502.03167 PDF:C\:\\Users\\Marie\\Zotero\\storage\\4FADRD22\\Ioffe und Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\TXWEQWLN\\1502.html:text/html}
}

@article{bellemare_unifying_2016,
	title = {Unifying {Count}-{Based} {Exploration} and {Intrinsic} {Motivation}},
	url = {http://arxiv.org/abs/1606.01868},
	abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.},
	urldate = {2017-08-12},
	journal = {arXiv:1606.01868 [cs]},
	author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01868},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1606.01868 PDF:C\:\\Users\\Marie\\Zotero\\storage\\XSD9MJ4X\\Bellemare et al. - 2016 - Unifying Count-Based Exploration and Intrinsic Mot.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\ZGKNN8LQ\\1606.html:text/html}
}

@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2017-08-12},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages + supplementary},
	file = {arXiv\:1509.02971 PDF:C\:\\Users\\Marie\\Zotero\\storage\\45K854YJ\\Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\9IDFN2TX\\1509.html:text/html}
}

@article{sutton_learning_1988,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1007/BF00115009},
	doi = {10.1007/BF00115009},
	abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
	language = {en},
	number = {1},
	journal = {Machine Learning},
	author = {Sutton, Richard S.},
	month = aug,
	year = {1988},
	pages = {9--44},
	file = {Full Text PDF:C\:\\Users\\Marie\\Zotero\\storage\\URKRCQ8J\\Sutton - 1988 - Learning to predict by the methods of temporal dif.pdf:application/pdf;Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\M7UWLTRB\\BF00115009.html:text/html}
}

@book{bellman_dynamic_nodate,
	title = {Dynamic {Programming}},
	isbn = {978-0-691-14668-3},
	url = {http://press.princeton.edu/titles/9234.html},
	abstract = {This classic book is an introduction to dynamic programming, presented by the scientist who coined the term and developed the theory in its early stages. In Dynamic Programming , Richard E. Bellman introduces his groundbreaking theory and furnish . . .},
	publisher = {Princeton University Press},
	author = {Bellman, Richard},
	file = {Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\ZV4QNJLK\\9234.html:text/html}
}

@article{schaul_prioritized_2015,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
	urldate = {2017-08-12},
	journal = {arXiv:1511.05952 [cs]},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05952},
	keywords = {Computer Science - Learning},
	annote = {Comment: Published at ICLR 2016},
	file = {arXiv\:1511.05952 PDF:C\:\\Users\\Marie\\Zotero\\storage\\EGBZGCT2\\Schaul et al. - 2015 - Prioritized Experience Replay.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\DPVMKIHJ\\1511.html:text/html}
}

@article{wawrzynski_control_2015,
	title = {Control {Policy} with {Autocorrelated} {Noise} in {Reinforcement} {Learning} for {Robotics}},
	volume = {5},
	issn = {20103700},
	url = {http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=56&id=551},
	doi = {10.7763/IJMLC.2015.V5.489},
	number = {2},
	urldate = {2017-08-12},
	journal = {International Journal of Machine Learning and Computing},
	author = {Wawrzy{\'n}ski, Pawe{\l }},
	month = apr,
	year = {2015},
	pages = {91--95}
}

@article{wang_dueling_2015,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	urldate = {2017-08-12},
	journal = {arXiv:1511.06581 [cs]},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06581},
	keywords = {Computer Science - Learning},
	annote = {Comment: 15 pages, 5 figures, and 5 tables},
	file = {arXiv\:1511.06581 PDF:C\:\\Users\\Marie\\Zotero\\storage\\ZEQFPPPN\\Wang et al. - 2015 - Dueling Network Architectures for Deep Reinforceme.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\37J7YZZY\\1511.html:text/html}
}

@article{uhlenbeck_theory_1930,
	title = {On the {Theory} of the {Brownian} {Motion}},
	volume = {36},
	url = {https://link.aps.org/doi/10.1103/PhysRev.36.823},
	doi = {10.1103/PhysRev.36.823},
	abstract = {With a method first indicated by Ornstein the mean values of all the powers of the velocity u and the displacement s of a free particle in Brownian motion are calculated. It is shown that u-u0exp(-$\beta$t) and s-u0$\beta$[1-exp(-$\beta$t)] where u0 is the initial velocity and $\beta$ the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For s this gives the exact frequency distribution corresponding to the exact formula for s2 of Ornstein and F{\"u}rth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when $\beta$ is much larger than the frequency and for values of t>>$\beta$-1, the formula takes the form of that previously given by Smoluchowski.},
	number = {5},
	urldate = {2017-08-12},
	journal = {Physical Review},
	author = {Uhlenbeck, G. E. and Ornstein, L. S.},
	month = sep,
	year = {1930},
	pages = {823--841},
	file = {APS Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\HB9XRBHL\\PhysRev.36.html:text/html}
}

@article{mnih_playing_2013,
	title = {Playing atari with deep reinforcement learning},
	url = {https://arxiv.org/abs/1312.5602},
	urldate = {2017-08-12},
	journal = {arXiv preprint arXiv:1312.5602},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year = {2013},
	file = {dqn.pdf:C\:\\Users\\Marie\\Zotero\\storage\\TIJEV5VR\\dqn.pdf:application/pdf}
}

@inproceedings{silver_deterministic_2014,
	title = {Deterministic policy gradient algorithms},
	url = {http://www.jmlr.org/proceedings/papers/v32/silver14.pdf},
	urldate = {2017-08-12},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning} ({ICML}-14)},
	author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	year = {2014},
	pages = {387--395},
	file = {silver14.pdf:C\:\\Users\\Marie\\Zotero\\storage\\MA942SD4\\silver14.pdf:application/pdf}
}

@phdthesis{watkins_learning_1989,
	title = {Learning from {Delayed} {Rewards}},
	url = {http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf},
	urldate = {2017-08-10},
	school = {King's College},
	author = {Watkins, Christopher John Cornish Hellaby},
	month = may,
	year = {1989},
	file = {new_thesis.pdf:C\:\\Users\\Marie\\Zotero\\storage\\L8QWYYGV\\new_thesis.pdf:application/pdf}
}

@article{watkins_technical_1992,
	title = {Technical {Note} - {Q}-{Learning}},
	volume = {8},
	url = {http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. The paper presents and proves in detail a convergence theorem for Q-learning. It shows that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. Extensions to the cases of nondiscounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
	urldate = {2017-08-12},
	journal = {Machine Learning},
	author = {Watkins, Christopher John Cornish Hellaby and Dayan, Peter},
	year = {1992},
	pages = {279--292},
	file = {cjch.pdf:C\:\\Users\\Marie\\Zotero\\storage\\P3RA6ZUJ\\cjch.pdf:application/pdf}
}

@misc{abadi_tensorflow:_2015,
	title = {{TensorFlow}: {Large}-scale machine learning on heterogeneous systems},
	url = {http://tensorflow.org/},
	author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu},
	year = {2015},
	note = {Software available from tensorflow.org},
	annote = {Software available from tensorflow.org}
}

@article{yann_lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	url = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
	number = {11},
	urldate = {2017-08-12},
	journal = {Proceedings of the IEEE},
	author = {{Yann LeCun} and {L{\'e}on Bottou} and {Yoshua Bengio} and {Patrick Haffner}},
	year = {1998},
	pages = {2278--2324},
	file = {lecun-01a.pdf:C\:\\Users\\Marie\\Zotero\\storage\\9X4FFRZA\\lecun-01a.pdf:application/pdf}
}