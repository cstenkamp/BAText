
@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {{\textcopyright} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true},
	doi = {10.1038/nature14236},
	abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
	language = {en},
	number = {7540},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	keywords = {Computer science},
	pages = {529--533},
	file = {Full Text PDF:C\:\\Users\\Marie\\Zotero\\storage\\JP7IS4Y3\\Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf;Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\RC6ZEXKN\\nature14236.html:text/html}
}

@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2017-08-12},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Learning},
	annote = {Comment: AAAI 2016},
	file = {arXiv\:1509.06461 PDF:C\:\\Users\\Marie\\Zotero\\storage\\GD9HV9ZG\\van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\YKVGT94V\\1509.html:text/html}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2017-08-12},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1502.03167 PDF:C\:\\Users\\Marie\\Zotero\\storage\\4FADRD22\\Ioffe und Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\TXWEQWLN\\1502.html:text/html}
}

@article{bellemare_unifying_2016,
	title = {Unifying {Count}-{Based} {Exploration} and {Intrinsic} {Motivation}},
	url = {http://arxiv.org/abs/1606.01868},
	abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.},
	urldate = {2017-08-12},
	journal = {arXiv:1606.01868 [cs]},
	author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01868},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1606.01868 PDF:C\:\\Users\\Marie\\Zotero\\storage\\XSD9MJ4X\\Bellemare et al. - 2016 - Unifying Count-Based Exploration and Intrinsic Mot.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\ZGKNN8LQ\\1606.html:text/html}
}

@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2017-08-12},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages + supplementary},
	file = {arXiv\:1509.02971 PDF:C\:\\Users\\Marie\\Zotero\\storage\\45K854YJ\\Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\9IDFN2TX\\1509.html:text/html}
}

@article{sutton_learning_1988,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1007/BF00115009},
	doi = {10.1007/BF00115009},
	abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
	language = {en},
	number = {1},
	journal = {Machine Learning},
	author = {Sutton, Richard S.},
	month = aug,
	year = {1988},
	pages = {9--44},
	file = {Full Text PDF:C\:\\Users\\Marie\\Zotero\\storage\\URKRCQ8J\\Sutton - 1988 - Learning to predict by the methods of temporal dif.pdf:application/pdf;Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\M7UWLTRB\\BF00115009.html:text/html}
}

@book{bellman_dynamic_nodate,
	title = {Dynamic {Programming}},
	isbn = {978-0-691-14668-3},
	url = {http://press.princeton.edu/titles/9234.html},
	abstract = {This classic book is an introduction to dynamic programming, presented by the scientist who coined the term and developed the theory in its early stages. In Dynamic Programming , Richard E. Bellman introduces his groundbreaking theory and furnish . . .},
	publisher = {Princeton University Press},
	author = {Bellman, Richard},
	file = {Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\ZV4QNJLK\\9234.html:text/html}
}

@article{schaul_prioritized_2015,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
	urldate = {2017-08-12},
	journal = {arXiv:1511.05952 [cs]},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05952},
	keywords = {Computer Science - Learning},
	annote = {Comment: Published at ICLR 2016},
	file = {arXiv\:1511.05952 PDF:C\:\\Users\\Marie\\Zotero\\storage\\EGBZGCT2\\Schaul et al. - 2015 - Prioritized Experience Replay.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\DPVMKIHJ\\1511.html:text/html}
}

@article{wawrzynski_control_2015,
	title = {Control {Policy} with {Autocorrelated} {Noise} in {Reinforcement} {Learning} for {Robotics}},
	volume = {5},
	issn = {20103700},
	url = {http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=56&id=551},
	doi = {10.7763/IJMLC.2015.V5.489},
	number = {2},
	urldate = {2017-08-12},
	journal = {International Journal of Machine Learning and Computing},
	author = {Wawrzy{\'n}ski, Pawe{\l }},
	month = apr,
	year = {2015},
	pages = {91--95}
}

@article{wang_dueling_2015,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	urldate = {2017-08-12},
	journal = {arXiv:1511.06581 [cs]},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06581},
	keywords = {Computer Science - Learning},
	annote = {Comment: 15 pages, 5 figures, and 5 tables},
	file = {arXiv\:1511.06581 PDF:C\:\\Users\\Marie\\Zotero\\storage\\ZEQFPPPN\\Wang et al. - 2015 - Dueling Network Architectures for Deep Reinforceme.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\37J7YZZY\\1511.html:text/html}
}

@article{uhlenbeck_theory_1930,
	title = {On the {Theory} of the {Brownian} {Motion}},
	volume = {36},
	url = {https://link.aps.org/doi/10.1103/PhysRev.36.823},
	doi = {10.1103/PhysRev.36.823},
	abstract = {With a method first indicated by Ornstein the mean values of all the powers of the velocity u and the displacement s of a free particle in Brownian motion are calculated. It is shown that u-u0exp(-$\beta$t) and s-u0$\beta$[1-exp(-$\beta$t)] where u0 is the initial velocity and $\beta$ the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For s this gives the exact frequency distribution corresponding to the exact formula for s2 of Ornstein and F{\"u}rth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when $\beta$ is much larger than the frequency and for values of t>>$\beta$-1, the formula takes the form of that previously given by Smoluchowski.},
	number = {5},
	urldate = {2017-08-12},
	journal = {Physical Review},
	author = {Uhlenbeck, G. E. and Ornstein, L. S.},
	month = sep,
	year = {1930},
	pages = {823--841},
	file = {APS Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\HB9XRBHL\\PhysRev.36.html:text/html}
}

@article{mnih_playing_2013,
	title = {Playing atari with deep reinforcement learning},
	url = {https://arxiv.org/abs/1312.5602},
	urldate = {2017-08-12},
	journal = {arXiv preprint arXiv:1312.5602},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year = {2013},
	file = {dqn.pdf:C\:\\Users\\Marie\\Zotero\\storage\\TIJEV5VR\\dqn.pdf:application/pdf}
}

@inproceedings{silver_deterministic_2014,
	title = {Deterministic policy gradient algorithms},
	url = {http://www.jmlr.org/proceedings/papers/v32/silver14.pdf},
	urldate = {2017-08-12},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning} ({ICML}-14)},
	author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	year = {2014},
	pages = {387--395},
	file = {silver14.pdf:C\:\\Users\\Marie\\Zotero\\storage\\MA942SD4\\silver14.pdf:application/pdf}
}

@phdthesis{watkins_learning_1989,
	title = {Learning from {Delayed} {Rewards}},
	url = {http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf},
	urldate = {2017-08-10},
	school = {King's College},
	author = {Watkins, Christopher John Cornish Hellaby},
	month = may,
	year = {1989},
	file = {new_thesis.pdf:C\:\\Users\\Marie\\Zotero\\storage\\L8QWYYGV\\new_thesis.pdf:application/pdf}
}

@article{watkins_technical_1992,
	title = {Technical {Note} - {Q}-{Learning}},
	volume = {8},
	url = {http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. The paper presents and proves in detail a convergence theorem for Q-learning. It shows that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. Extensions to the cases of nondiscounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
	urldate = {2017-08-12},
	journal = {Machine Learning},
	author = {Watkins, Christopher John Cornish Hellaby and Dayan, Peter},
	year = {1992},
	pages = {279--292},
	file = {cjch.pdf:C\:\\Users\\Marie\\Zotero\\storage\\P3RA6ZUJ\\cjch.pdf:application/pdf}
}

@misc{abadi_tensorflow:_2015,
	title = {{TensorFlow}: {Large}-scale machine learning on heterogeneous systems},
	url = {http://tensorflow.org/},
	author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu},
	year = {2015},
	note = {Software available from tensorflow.org},
	annote = {Software available from tensorflow.org}
}

@article{yann_lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	url = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
	number = {11},
	urldate = {2017-08-12},
	journal = {Proceedings of the IEEE},
	author = {{Yann LeCun} and {L{\'e}on Bottou} and {Yoshua Bengio} and {Patrick Haffner}},
	year = {1998},
	pages = {2278--2324},
	file = {lecun-01a.pdf:C\:\\Users\\Marie\\Zotero\\storage\\9X4FFRZA\\lecun-01a.pdf:application/pdf}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2017-08-12},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv\:1412.6980 PDF:C\:\\Users\\Marie\\Zotero\\storage\\39GTZPRV\\Kingma und Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\RIFHPF7T\\1412.html:text/html}
}

@techreport{rummery_-line_1994,
	title = {On-{Line} {Q}-{Learning} {Using} {Connectionist} {Systems}},
	author = {Rummery, G. A. and Niranjan, M.},
	year = {1994},
	file = {rummery_tr166.pdf:C\:\\Users\\Marie\\Zotero\\storage\\BRLZJF88\\rummery_tr166.pdf:application/pdf}
}

@article{john_n._tsitsiklis_analysis_1997,
	title = {An {Analysis} of {Temporal}-{Difference} {Learning} with {Function} {Approximation}},
	volume = {42},
	url = {http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf},
	number = {5},
	urldate = {2017-08-14},
	journal = {IEEE TRANSACTIONS ON AUTOMATIC CONTROL},
	author = {{John N. Tsitsiklis} and {Benjamin Van Roy}},
	month = may,
	year = {1997},
	annote = {Tsitsiklis, J. \& Roy, B. V. An analysis of temporal-difference learning with functionapproximation. IEEE Trans. Automat. Contr. 42, 674{\textendash}690 (1997).},
	file = {J063-97-bvr-td.pdf:C\:\\Users\\Marie\\Zotero\\storage\\GYH6Y659\\J063-97-bvr-td.pdf:application/pdf}
}

@article{bellemare_arcade_2012,
	title = {The {Arcade} {Learning} {Environment}: {An} {Evaluation} {Platform} for {General} {Agents}},
	shorttitle = {The {Arcade} {Learning} {Environment}},
	url = {http://arxiv.org/abs/1207.4708},
	doi = {10.1613/jair.3912},
	abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
	urldate = {2017-08-16},
	journal = {arXiv:1207.4708 [cs]},
	author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.4708},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1207.4708 PDF:C\:\\Users\\Marie\\Zotero\\storage\\JFZYEQ8R\\Bellemare et al. - 2012 - The Arcade Learning Environment An Evaluation Pla.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\86J366TV\\1207.html:text/html}
}

@article{duan_benchmarking_2016,
	title = {Benchmarking {Deep} {Reinforcement} {Learning} for {Continuous} {Control}},
	url = {http://arxiv.org/abs/1604.06778},
	abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
	urldate = {2017-08-16},
	journal = {arXiv:1604.06778 [cs]},
	author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06778},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	annote = {Comment: 14 pages, ICML 2016},
	file = {arXiv\:1604.06778 PDF:C\:\\Users\\Marie\\Zotero\\storage\\EFIEXNFG\\Duan et al. - 2016 - Benchmarking Deep Reinforcement Learning for Conti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\4BM8WXL7\\1604.html:text/html}
}

@article{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2017-08-16},
	journal = {arXiv:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01540},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1606.01540 PDF:C\:\\Users\\Marie\\Zotero\\storage\\XE6IFP2J\\Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\L77T8JBI\\1606.html:text/html}
}

@misc{wymann_torcs_2013,
	title = {{TORCS}, the open racing car simulator},
	url = {http://www.torcs.org},
	author = {Wymann, Bernhard and Espi{\'e}, Eric and Guionneau, Christophe and Dimitrakakis, Christos and Coulom, R{\'e}mi and Sumner, Andrew},
	year = {2013}
}

@article{wymann_torcs_2000,
	title = {Torcs, the open racing car simulator},
	url = {https://pdfs.semanticscholar.org/b9c4/d931665ec87c16fcd44cae8fdaec1215e81e.pdf},
	urldate = {2017-08-16},
	journal = {Software available at http://torcs. sourceforge. net},
	author = {Wymann, Bernhard and Espi{\'e}, Eric and Guionneau, Christophe and Dimitrakakis, Christos and Coulom, R{\'e}mi and Sumner, Andrew},
	year = {2000},
	file = {torcs.pdf:C\:\\Users\\Marie\\Zotero\\storage\\R2IR2PGV\\torcs.pdf:application/pdf}
}

@inproceedings{konda_learning_2015,
	title = {Learning {Visual} {Odometry} with a {Convolutional} {Network}:},
	isbn = {978-989-758-089-5 978-989-758-090-1 978-989-758-091-8},
	shorttitle = {Learning {Visual} {Odometry} with a {Convolutional} {Network}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005299304860490},
	doi = {10.5220/0005299304860490},
	urldate = {2017-08-16},
	publisher = {SCITEPRESS - Science and and Technology Publications},
	author = {Konda, Kishore and Memisevic, Roland},
	year = {2015},
	pages = {486--490},
	file = {VISAPP_2015_145.pdf:C\:\\Users\\Marie\\Zotero\\storage\\24HC8WM9\\VISAPP_2015_145.pdf:application/pdf}
}

@article{bojarski_end_2016,
	title = {End to {End} {Learning} for {Self}-{Driving} {Cars}},
	url = {http://arxiv.org/abs/1604.07316},
	abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
	urldate = {2017-08-16},
	journal = {arXiv:1604.07316 [cs]},
	author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.07316},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1604.07316 PDF:C\:\\Users\\Marie\\Zotero\\storage\\76PYLJPN\\Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marie\\Zotero\\storage\\JFTZBUCH\\1604.html:text/html}
}