% Chapter Template

\chapter{Introduction} % Main chapter title

\label{ch:intro} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textit{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}
\newcommand{\batchnorm}{batch normalization }
\newcommand{\Batchnorm}{Batch normalization }

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------


\section{Motivation}

Self-driving cars are attracting more and more public attention. Precisely 125 years after the first long-distance car ride of \keyword{Bertha Benz} in 1888, a \keyword{Mercedes} drove the same distance fully autonomously in 2013\footnote{\url{https://www.mercedes-benz.com/en/mercedes-benz/innovation/autonomous-long-distance-drive/}}.
The first US driving license was issued to a self-driving car developed by \keyword{Waymo} (previously \keyword{Google}) already in 2012, with fully autonomous cars without any human interface devices driving on the streets since 2014. As of november 2016, their fleet of self-driving cars had driven more than $3.5$ million kilometers fully autonomously on public roads\footnote{\url{https://static.googleusercontent.com/media/www.google.com/en//selfdrivingcar/files/reports/report-1116.pdf}}. As of 2017, self-driving \keyword{Uber}-taxis are already driving on streets of the USA\footnote{\url{https://www.uber.com/cities/pittsburgh/self-driving-ubers/}}. 

The most famous case of self-driving cars is however the autopilot-feature that comes with almost every \keyword{Tesla}, that takes away more and more driving tasks from its human driver. Since the company introduced their \keyword{autosteer}-feature, crashes were reduced by an impressive $40\%$\footnote{\url{https://electrek.co/2017/01/19/tesla-crash-rate-autopilot-nhtsa/}}. For the aim of this thesis, Tesla is an especially interesting case: Many of their cars are equipped with the capability to drive autonomously, including all the necessary sensors. These sensor are active and produce data even when the AI is not driving, allowing Tesla to collect billions of miles of data\footnote{\url{https://electrek.co/2016/11/13/tesla-autopilot-billion-miles-data-self-driving-program/}} to their central hub. Most interestingly, the company incorporates \keyword{Deep Neural Networks} on this huge amount of data to improve their self-driving algorithms\footnote{\url{https://www.tesla.com/autopilot}, for more information about \keyword{Tesla Vision} see  \url{https://electrek.co/2016/11/18/tesla-self-driving-demonstration-video-real-time-tesla-vision/}.}, be it only for visual processing like modelling the scene around the car.

In other news, the research area of \keyword{Reinforcement Learning} has achieved incredible progress in the past years. While respective algorithms failed even in simple environments until the publication of the \keyword{Deep-Q-Network}\cite{mnih_playing_2013}, more and more environments are played by reinforcement-learning agents with superhuman performance\footnote{In the midst of writing this thesis, an algorithm by \keyword{openAI} for example defeated dozens of human professionals in the popular multi-player game \keyword{DOTA 2} (\url{https://blog.openai.com/more-on-dota-2/})}. 

As there are many approaches aiming at transforming virtually driving agents to ones that drive in the real world \cite{you_virtual_2017}, it is interesting to consider agents that are trained virtually, in the hope of translating their techniques and properties to actual self-driving cars. A huge advantage of training virtually is, that it is not necessary to keep up a \keyword{zero tolerance policy}. This is especially relevant in the context of race cars, as it allows to focus on \keyword{tactical decisions} instead.

\section{Goal of this thesis}

The goal of the work behind this thesis is to \keyword{Control Self-Driving Race Cars with Deep Neural Networks}. While this is a very coarse goal, the tasks of the work are clearly defined. First, a given car simulation was to be transformed into a platform that can be learned. Second, agents need to be developed that make good tactical race decisions inside this platform. Thirdly, research questions are answered about specific features of these agents. The following sections will describe the three tasks in more detail.

\subsection{Driving}

As put forward by \keyword{Lex Fridman} in his MIT lecture "\keyword{Deep Learning for Self-Driving Cars}"\footnote{MIT 6.S094, course website: \url{http://selfdrivingcars.mit.edu/}} the tasks for self-driving cars can be sub-divided into the following categories: 
\begin{itemize} 
	\item Localization and Mapping
	\item Scene Understanding
	\item Movement Planning
	\item Driver State
\end{itemize}

Similar categorization is provided by the developers of the race car simulation \keyword{TORCS}\cite{wymann_torcs_2015}, which divide the \keyword{racing problem} among others into \textit{trajectory planning}, which is finding an optimal trajectory on the fly while driving, and \textit{inference and vision}, the problem of how to infer useful information from high-dimensional input.

State-of-the-art self-driving cars rely on handcrafted algorithms to solve either of these problems individually in highly modularized systems. The problem of driving a car is seperated into tactical decisions, such as what speed to aim for to drive safely or whether or not to overtake, and on the other side operational low-level decisions for the actual motor commands. While many advances on the tactical side are made, offensive tactical profiles require good operational systems. In car racing, be it virtual or real motorsports, extreme tactical profiles are used, as the goal is to perform at the limits of the possible. Such tactical profiles require reliable operational performance.

Especially in virtual car racing, where mistakes are condoned far more than in real life where actual lives are involved, it is interesting to focus on the overall racing problem. In \keyword{end-to-end} approaches, this can be summarized as \keyword{minimal lap time}: Finding the policy that minimizes the excepted time for a given lap. 

While this problem can be solved analytically, it is also interesting to solve the racing problem \textit{on the fly}, where the agent learns over its task only through continuous interaction. If the situation of a single car on the track is given, the problem corresponds to a \keyword{partially observed markov decision process}. A formulation of the environment in such a way allows for \keyword{reinforcement learning}, a branch of artificial intelligence where many progresses are made in recent time.

In the course of this thesis, a virtual driving agent was developed that solves a given racing game using recent advances in \keyword{deep neural networks} as well as reinforcement learning. The goal of this agent is to achieve the best possible driving policy, advancing as far as possible without crashing, or even minimizing laptime of a given track. 

\subsection{Creating a research platform}

Creating an artificial driving agent is impossible without an environment to train the agent on. While numerous environments for car racing already exist (like some environments accessible through openAI's \keyword{gym} or the \keyword{TORCS} platform), in this thesis a proprieatary software will be used. The game to be played is a driving simulation programmed with the game engine \keyword{Unity 3D}. 

The basis of the environment was given by \leon of this thesis as a fully playable game. In the course of this thesis, this game was extended, such that artificial driving agents can communicate with this platform over \keyword{sockets}. The game sends high-level as well as low-level information about the state of the game in textual form and requires actions back fast from the agent. Additionally, all functionality to easily create agents using the programming language \keyword{Python} and the deep learning library \keyword{TensorFlow} was implemented. This communciation needs to be as efficient as possible, such that the performed action receives the environment in time.

There are several contrasts to other environments and other approaches that make this one interesting: For example, the game is live, inspectable and a user can intervene into what the agent does at any time. This makes it easy to assess the policy of an agent. Further, if agents specify a \keyword{reward} or some measure of \keyword{value of state or action}, these values can be inspected -- if the state-value or reward is high when driving into a wall at full speed, something is likely to be wrong.

Further, as source code of both game and agent is open and the game is programmed straight forward without unneeded features, it is very easy to change its code, such that any new information an agent could incorporate is easily added to it. This thesis describes the implemented components in detail, inviting interested parties to add functionality at their convenience.

As the implementation of such a research platform is by no means an easy task and requires profound software engineering, a substantial portion of this thesis describes the resulting implementation.

\subsection{Research questions}

\label{sec:researchquestions}

Additionally to implementing platform and agent, several research questions will be crystallized and answered. In doing so, different agents will be developed, and their performance compared. Some of the answered questions are:
\begin{itemize}
	\item How different models perform in comparison, and specifically if discretizing the action-space impairs performance
	\item What a good reward function looks like, that rewards the \textit{correct} behaviour at all times (including braking)
	\item How agents that rely purely on pretraining perform in comparison to reinforcement learning agents
	\item How to incorporate pretraining into reinforcedly learning agents
\end{itemize}

\section{Reading instructions}

This thesis is structured as follows:


\renewcommand{\arraystretch}{1.3}
\begin{flushleft}
\begin{tabular}{>{\em}p{2.2cm} p{\textwidth-3.2cm}} 
	\textbf{Chapter 1} & begins with the motivation for this topic. Afterwards, the goals of this thesis are presented. In the end, a short summary of the chapters is given as an overview.\\
	\textbf{Chapter 2} & provides an extensive theoretical foundation of reinforcement learning. The first section details how to correctly formalize an environment as a markov decision process. Afterwards, Q-learning will be explained. This leads over to the \keyword{Deep Q Network}, a recent advance in deep learning. Subsequently policy gradient techniques will be introduced, most notably a learning technique termed \keyword{Deep DPG}. The chapter concludes with an overview of exploration techniques.\\
	\textbf{Chapter 3} & gives an overview of related work of this field. At first, other frameworks for reinforcement learning and racing simulations will be introduced. Afterwards, a coarse overview of the state-of-the-art in machine learning for self-driving cars, in real life as well as in simulations, will be given.\\
	\textbf{Chapter 4} & details the architecture of the implementation provided in the course of this thesis. It will start with the general characteristics and design decisions, before providing a detailed explanation of the source code of environment and agent. The explanations are in enough detail to understand the complete code developed in the scope of this thesis.\\
	\textbf{Chapter 5} & explains the results of the implementation in the form of the features, models and agents that were developed on top of the platform from chapter 4.\\
	\textbf{Chapter 6} & presents the resulting performance of the agents from chapter 5, also answering the research questions as stated in chapter 1.\\
	\textbf{Chapter 7} & discusses the results of this thesis. For that, the chapter is divided into three parts, where the first part debates the platform and the second part the agents. Both of these parts also discuss open questions and future research directions. The last section of this chapter ends this thesis with the conclusion.\\
\end{tabular}
\end{flushleft}