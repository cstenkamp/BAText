% Chapter 1

\chapter{Reinforcement Learning} % Main chapter title

\label{Reinforcement Learning} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textit{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

As the task as hand was not only to provide a reinforcement learning agent, but also to convert a game itself into something the agent can successfully play, I will in this chapter go into detail about Reinforcement Learning in general, to give insights into why I did what I did. Also, I will try to keep this stuff as general as possible, getting into detail when speaking about the used algorithms.
[The sense of this chapter is to give an intro of MDPs and RL. It shall also go into enough details on how to specify an MDP such that an RL agent can learn on it, because a big part of  the work was exactly that. It’s supposed to end with SARSA and Q-learning as the two Ideas on how to perform RL]

\section{Reinforcement Learning Problems}

Machine Learning can mainly be subdivided into three main categories: Supervised Learning, Unsupervised Learning, and Semi-supervised learning. The first deals with direct classification or regression using labelled data (i.e. it uses pairs of datapoints with their corresponding category or value). In unsupervised learning, no such label exists, and the data must be clustered into meaningful parts without any knowledge, by for example grouping objects by similarity of their properties.\\
What will be mainly considered in this thesis will be a certain kind of semi-supervised learning: \keyword{Reinforcement learning}. In Reinforcement Learning (\textbf{RL}), instead of labels for the data, there is a \textit{weak teacher}, which provides feedback to the actions the agent took.

\subsubsection{Markov Decision Processes}

The metaphor behind RL is that of a decision maker (\keyword{agent}) and an \keyword{environment}. The agent makes observations in the environment (its input), takes actions (output) and receives rewards. In contrast to the classical ML approaches, in RL the agent is also responsible for exploration, as he has to acquire his knowledge actively. Thus, a reinforcement learning problem is given if the only way to collect information about the \keyword{underlying model} (the environment) is by interacting with it. As the environment does not explicitly provide actions the agent has to perform, its goal is to figure out the actions maximizing its cumulative reward until a training episode ends.
%"put very simply, the agent wants to repeat the actions that give the highest reward

In the classical RL approach, the environment is divided into discrete time steps. If that is the case, the environment corresponds to a \keyword{Markov Decision Process} (\textbf{MDP}). Formally, a MDP is a 5-tuple $\langle S, A, P, R, \gamma \rangle$, consisting of the following:\\
\begin{align*}
S &- \text{set of states } s\in S\\
A &- \text{set of actions } a \in A\\
P(s'|s, a) &- \text{transition probability function from state } s \text{ to state } s’ \text{ under action } a\\
R(r|s, a) &- \text{ reward probability function for action } a \text{ performed in state } s \\
\gamma &- \text{discount factor for future rewards } 0 \leq \gamma \leq 1
\end{align*}

%dass wir eigentlich noch ne initial state distrubition haben, und dass für den reward gilt S x A -> |R
In general, both the state transition function and the reward function may be indeterministic, meaning that neither reward nor the following state are in complete control of the decision maker. Because of that, it can always only be talked about the expected value depending on the random distribution of states. Given both $s$ and $s'$ however, the reward is assumed to be deterministic. I will refer to the actual result of a state transition at discrete point in time $t$ as $s_{t+1}$ and to the result of the reward function as $r_t$. If no point in time is explicitly specified, it is assumed that all variables use the same $t$.

While an \keyword{offline learner} gets as input the problem definition in the form of a complete MDP, where the only task left is to classify actions yielding high rewards from actions giving suboptimal results, the task for an \keyword{online reinforcement learning} agent is a lot harder, as it has to learn the MDP itself via trial and error. In the process of reinforcement learning, the agent will encounter states $s$ of the environment, performing actions $a$. The future state $s_{t+1}$ of the environment may be indeterministic, but depends on the history of previous states $s_0, .., s_t$ as well as the action of the agent $a_t$. It is assumed that the \keyword{Markov property} holds, which means that a state  $s_{t+1}$ depends only on the current state $s_t$ and currenct action $a_t$: $p(s_{t+1}|s_t,a_t) = p(s_{t+1}|s_0,a_0,..,s_t,a_t)$

Throughout interacting with the environment, the agent receives rewards $r$, depending on his action $a$ as well as the state of the environment $s$. In many RL problems, the full state of the environment is not known to the agent, and it only perceives an observation depending on the environment: $o_t := o(s_t)$\footnote{From now on, when I mean the state of the environment, I will explicitly refer to it as $s_e$, while reserving $s$ for the agent's obvervation of the enviroment $o(s_e)$}. This is referred to as \keyword{partial observability}, and the corresponding decision process is a \keyword{partially observable MDP}. Additionally, the agent knows when a final state of the environment is reached, terminating the current training episode. An episode thus consists for the agent of a sequence of observations, actions and rewards ($S \times A \times \mathds{R}$) until at time $t_t$ some terminal state $s_{t_t}$ is reached: $$Episode := \big((s_0, a_0, r_0), (s_1, a_1, r_1), (s_2,a_2,r_2), .., (s_{t_t}, a_{t_t}, r_{t_t})\big)$$
%A training example for the agent thus consists of the tuple  <o_t, a_t, r_t, o_{t+1}, t>. 

\subsubsection{Value of a state}
In the process of reinforcement learning, the agent tries to perform as well as possible in the previously unknown environment. For that, it uses an \mbox{action-policy $\pi$,} depending on some parameters $\theta$. The policy maps states to actions, which in the case of a \keyword{deterministic} policy leads to $\pi_\theta(s) = a$. Though a stochastic policy is possible, it will not be considered for now.\footnote{It is obvious, that the result of both the reward function and the state transition function depend on $\pi$. To be explicit about that, I will refer to a reward dependent on $\pi$ as $r^\pi$ and a state transition dependent on $\pi$ as $s^\pi$. If state or reward depends on an explicit action instead, I refer to it as $r^a$ and $s^a$. Whenever not necessary for clarity, I will also drop $\pi$s dependence on $\theta$.} As the agent does not have supervised data for what actions are the ground truth, it must learn some kind of measure for the value of being in a certain state or performing a certain action. The commonly used measure for the value of a state can be calculated by the immediate reward this state gives, summed with the expected value of the discounted future reward the agent will archieve by continuing to follow his policy from this state on: 
\begin{equation} \label{eq:1.1}
	V^\pi(s_t) := \mathds{E}_S \Big[ \sum_{t'=t}^{t_t} ( \gamma^{t'-t} * r^\pi_{t'} ) \Big]
\end{equation}
Using the discounted future reward is useful because in an indeterministic environment it gets less likely that the agent actually reaches this state, and to make the agents perform good actions as quickly as possible.

The actual, underlying Value of a state $s$ is defined as the value of the state when using the best possible policy, which corresponds to the maximally archievable reward starting in state $s$:
\begin{equation} \label{eq:1.15}
	V^*(s_t) := max_\pi V^\pi(s_t)
\end{equation}

While \keyword{passive reinforcement learning} simply tries to learn the Value-function without the need of action selection, an \keyword{active reinforcement learner} tries to estimate a good policy, using which those high-value states are actually reached. If the value of every state is known, then the optimal policy can be defined as the one archieving maximal value for every upcoming state: \mbox{$\pi^* := argmax_\pi V^\pi(s) \forall s \in S$}. Knowing what an optimal policy does, and using \ref{eq:1.1} and \ref{eq:1.15}, it is possible to re-write the definition of the value of a state recursively as 
\begin{align}
	V^*(s_t) &= max_\pi \mathds{E}_S \Big[ \big(\sum_{t'=t}^{t_t} ( \gamma^{t'-t} * r^\pi_{t'} )\big) \Big] \label{eq:1.16}\\
	&= max_\pi  \mathds{E}_S \big[ (r^\pi_t + \gamma * V^\pi(s_{t+1}^\pi)) \big] 
\end{align}
%again, dass r von pi abhängt wird nicht deutlich
This is known as the \keyword{Bellman Equation}, which allowed for the birth of dynamic programming. It rewrites the value of the decision problem at time $t$ in terms of the immediate reward at $t$ plus the value of the remaining decision problem at $t+1$, resulting from the initial choices.
% TODO CITE HINZUFÜGEN!!!!!!!!
%https://en.wikipedia.org/wiki/Bellman_equation#The_Bellman_equation

\subsubsection{Value of an action}
While the definition of a state-value is useful, it alone does not help an agent to perform optimally, as neither the successor function $P(s'|s,a)$, nor the reward function $R(r|s,a)$ are known to the agent. While so-called \keyword{model-based} reinforcement learning tries to learn both of those explicitly to reconstruct the entire MDP, \keyword{model-free} agents use a different measure of quality: the \keyword{Q-value}. It represents the expected value of performing action $a_t$ in a state $s_t$, afterwards following the policy $\pi$.
\begin{equation} \label{eq:1.2}
	Q^\pi(s_t,a_t) :=  \mathds{E}_S \big[ r_t^{a_t} + \gamma * V^\pi(s_{t+1}^{a_t}) \big]
\end{equation}
With the optimal, maximally archivable action-value $Q^*$ being respectively
\begin{align}
	Q^*(s_t,a_t) &=  \mathds{E}_S \big[ r_t^{a_t} + \gamma * V^*(s_{t+1}^{a_t}) \big] \\
	&= max_\pi \mathds{E}_S \big[ ( r_t^{a_t} + \gamma * V^\pi(s_{t+1}^{a_t})) \big]
\end{align}

For the Q-value, the Bellman equation holds as well: If the optimal value $Q^*(s_{t+1},a_{t+1})$ was known for all possible actions, then the optimal action at time $t$ is the one maximizing the sum of immediate reward and corresponding Q-value\footnote{This is because of the definition of Bellman's \keyword{Principle of Optimality}, which states that "An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision\cite{Bellman1957}"}:
\begin{equation} \label{bellman}
	Q^*(s_t,a_t) = \mathds{E}_S \big[ r_t + \gamma * max_{a_{t+1}} Q^*(s_{t+1},a_{t+1})  \big]
\end{equation}

As the Value of a state is defined as the maximally archievable reward from that state, the relation between $Q$ and $V$ can be expressed as 
\begin{equation} \label{eq:1.25}
V(s_t) = max_{a_t} Q(s_t, a_t)
\end{equation}

% (dadrunter noch ne formel haben wie V von Q berechnet wird). 
When an agent knows the Q-value for each action of a state, it can easily infer the optimal action in state $s_t$ as $a^*_t := argmax_{a_t}(Q(s_t, a_t))$ and thus the optimal policy $\pi^*$, %that takes action a* \forall s states 
guaranteeing maximum future reward at every state. The goal of a model-free RL agent is thus to get a maximally precise estimate of $Q^*$, yielding maximal reward for every state. For that, it does not need to explicitly learn the reward- and transition function, but instead can model only the Q-function. Its policy is then to simply always take the action yielding the highest value for every state (a \keyword{greedy} policy\footnote{in fact, the agent cannot act only according to the greedy policy, as it will need to explore the environment, The problem of exploration will be considered later in this thesis.}).  In RL settings with a highly limited amount of discrete states and actions, the respective Q-function estimate can be specified as a lookup table, whereas for areas of interest, the function is calculated using a kind of nonlinear function approximator. 

Throughout exploration of the environment, the agent collects more information of it, continually updating its estimate $Q^\pi$. For that, it uses samples from its episodes of interacting with the environment. % also s, a, r, s', a', t.... wie formulier ich das gut?


\section{Temporal difference Learning}

%die relevanten cites hier sind sutton1988 und watkins1989. Der proof is dayan1992... und sowieso bellman
%Reinforcement learning can solve Markov decision processes without explicit specification of the transition probabilities; the values of the transition probabilities are needed in value and policy iteration. In reinforcement learning, instead of explicit specification of the transition probabilities, the transition probabilities are accessed through a simulator that is typically restarted many times from a uniformly random initial state. Reinforcement learning can also be combined with function approximation to address problems with a very large number of states.

%The goal of a reinforcement-learning agent is to continually update its Q*-estimate, Qpi, such that it can follow the policy that always takes the action giving the highest cumulative reward. To update its Q-function, the agent can either use full pairs of State,action,reward,state,action-tuples (SARSA, source), or Q-learning (source).
%Temporal difference learning ist Sutton 1988 in ftp://mi.eng.cam.ac.uk/pub/reports/auto-pdf/rummery_tr166.pdf
%Q-learning is Watkins 1989 in ibidem

Throughout the process of reinforcement learning, the agent continually improves its estimates $\hat{Q}$ of $Q^*$. The loss of its current estimate could be seen as the squared difference $(\hat{Q}$-$Q^*)^2$, however as the agent has no knowledge of $Q^*$, it needs some way of approximating it. For that, a Q-learning agent performs \keyword{iterative approximation}, using the information about the environment, to continually update its estimates of $Q^*$.
\noindent Using the recursive definition of a Q-value given in the Bellman equation \ref{bellman} allows for a technique called \keyword{temporal difference learning}\cite{sutton1988}: At time $t+1$, the agent can compare its estimate of the Q-function of the last step, $\hat{Q}^\pi(s_t, a_t)$, with a new estimate using the new information it gained from the environment: $r_{t+1}$ and $s_{t+1}$.  Because of the newly gained information from the underlying model, the new estimate will be closer to the actual function $Q^*$ than the original value:
\begin{align} 
	\hat{Q}^\pi(s_t,a_t) &= r_t + \mathds{E}_S \big[ \gamma * max_{a_{t+1}} \hat{Q}^\pi(s_{t+1},a_{t+1})  \big]\\
	                     &\approx r_t + \gamma * r_{t+1} + \mathds{E}_S \big[ \gamma^2 * max_{a_{t+2}} \hat{Q}^\pi(s_{t+2},a_{t+2})  \big] \label{bellmanmal2}
\end{align}

Keeping in mind that $\hat{Q}^\pi$ is only an estimator of the $Q^*$-values of the underyling model, it becomes clear that equation \ref{bellmanmal2} is closer to the actual $Q^*$, as it incorporates more information stemming from the model itself. 

In temporal difference learning, the mean-squared error of the \keyword{temporal difference} from the Bellman equation, $r_t + \gamma * Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)$, gets minimized via iterative approximation. Because the optimal target-values $Q^*(s_{t+1},a_{t+1})$ are not available, they are substituted with the \textit{more informed guess}, stemming from observations in the environment. It is noteworthy, that each update of the Q-function using the temporal difference will affect not only the last prediction, but all previous predictions.

%Q^*(s_t,a_t) &= \mathds{E}_S \big[ r_t^{a_t} + \gamma * V^*(s_{t+1}^{a_t}) \big]\\
%% &= r_t^{a_t} + \gamma * \Big( r_{t+1}^{a_\pi} + \gamma * V^\pi(s_{t+2}^{a_\pi})\Big) \\
%&\approx \mathds{E}_S \big[ r_t^{a_t} + \gamma * V^\pi(s_{t+1}^{a_t}) \big] = \mathds{E}_S \big[ r_t^{a_t} + \gamma * \hat{Q}^\pi(s_{t+1}^{a_t},a_{t+1}) \big]

%dass The only difference between maxa Q(s, a) and maxa'Q(s',a') is the immediate reward and the discount factor, and this is the only thing we update. If the received reward is higher than the expected reward, we will increase Q(s, a), or decrease it if our estimate was too high

% TODO sarsa ist sutton and barto 1998
\subsubsection{\textsc{SARSA}}
The new knowledge about the environment can be incorporated in two different ways. For the first method, the agent samples a full tuple of $\langle s_t, a_t, r_t, s_{t+1}, a_{t+1} \rangle$ from the environment, to then calculate the temporal difference error in non-terminal states as $ TD := (r_t + \gamma * \hat{Q}_i^\pi(s_{t+1}, a_{t+1})) - \hat{Q}_i^\pi(s_t, a_t)  $. This algorithm of calculating the temporal difference error is known as  \keyword{\textsc{sarsa}}, and it is an example of \keyword{on-policy} learning. In on-policy learning, the agent uses his own policy in every estimate of the Q-value. 

\subsubsection{Q-learning}

In contrast to \textsc{sarsa} stands the \keyword{off-policy} algorithm \keyword{Q-learning}. This algorithm does not need to sample the action $a_{t+1}$, as it calculates the Q-update at iteration $i$ using the best possible action in state $s_{t+1}$.\footnote{A slight deviation from this \keyword{double-Q-learning}, an algorithm I will go into detail about lateron.} As the previous definition of Q-values was only correct in non-terminal states, a case differentiation must be introduced for terminals and non-terminal states. In the following, $y_t$ will stand for the updated estimate of the Q-value at $t$, sampling the necessary states, rewards and actions from the environment, almost resulting in the formula found in \cite{mnih_human-level_2015}:
\begin{equation} \label{eq:ycases}
	y_{t} = \begin{cases} 
		r_t & \text{if } t = t_t\\
		r_t + \gamma * max_{a_{t+1}} (\hat{Q}^\pi( s_{t+1}, a_{t+1})) & \text{otherwise}
\end{cases}
\end{equation}
The temporal difference error for time $t$ is accordingly defined as 
\begin{equation}
TD_t := y_t - \hat{Q}^\pi(s_t, a_t)
%TD_t := \big( r_t + \gamma * max_{a_{t+1}}(\hat{Q}_i^\pi(s_{t+1}, a_{t+1}))\big) - \hat{Q}_i^\pi(s_t, a_t)
\end{equation}

A Q-learning agent must thus for his learning step observe a snapshot of the environment, consisting of the following input: $\langle s_t, a_t, r_t, s_{t+1}, t+1==t_t \rangle$ (where the latter is the information if state $s_{t+1}$ was a terminal state). Q-learning is considered an off-policy algorithm, because it learns about the greedy policy $a=argmax_{a'}Q(s,a')$, while not necessarily following it.\footnote{This is because in its actual policy, the agent includes some way to explore the environment, more on that later.}

Using the above error straight away allows for the update-rule of an agent in a very limited setting: Consider an agent, specifying his approximation of the Q-function (his \keyword{model}) with a lookup-table, initialized to all zeros. It is proven by \cite{watkins_technical_1992} that for finite-state Markovian problems with nonnegative rewards the update-rule for the Q-table $\hat{Q}(s_t,a_t) \leftarrow r_t^{a_t} + \gamma * \hat{Q}^\pi(s_{t+1}^{a_t},a_{t+1}) $ converges to the optimal $Q^*$-function, making the greedy policy $\pi^*$ optimal\footnote{Of course the agent will need some kind of exploration technique first, more on that later}.  % (learning propagates backwards).
%man kann auch Q(s,a) <- Q(s,a) + alpha*TD als update rule nehmen (siehe unseren bericht für CANN), dann ists abhängig von der learning rate(und da TD->0 geht, hat das nen klares Limit)

As however in practice the problems using a table as the Q-functions are only very limited scenarios, an update rule like this is irrelevant. Instead, a better idea is to use this definition of the temporal difference error for a loss function, which is to be minimized throughout the process of RL. A commonly used loss-function is the \keyword{L2-Loss}, which allows gradient descent, updating the parameters of the Q-function into the direction of the newly acquired knowledge. The L2-Loss at iteration i with model-parameters $\theta_i$ is thus defined as the following:
\begin{equation} \label{l2loss}
	L_i(\theta_i) := \mathds{E}_{s,a,r} \Big[ \big( y_i(\theta_i) - \hat{Q}_i^\pi(s_t, a_t; \theta_i)\big)^2 \Big]
\end{equation}
%dass das später noch relevant wird dass er max_q nimmt: it doesn't quite learn from bad rewards if there are also good ones: https://www.youtube.com/watch?v=4MOx2_e5tug



%sources so far:
%Heidemanns slides - check.
%meine eigene präsi - check. Wobei ich in der noch die erwartungswerte hab, und hier nicht..
%https://en.wikipedia.org/wiki/Markov_decision_process - check
%https://github.com/ahoereth/ddpg/blob/master/exploration/FrozenLake.ipynb  - check
%https://en.wikipedia.org/wiki/Reinforcement_learning
%https://en.wikipedia.org/wiki/Bellman_equation#The_Bellman_equation
%Valentins und melisas präsi
%Nature paper
%
%Medium post
%Russel, norvig
%Die originalen RL und Q-learn paper
%Meine und bennis präsi
%Report von valentin melisa und mir und benni

% TODO: Ich bin hier noch nicht auf exploration eingegangen... Und außerdem nicht auf actor-critic stuff oder policy gradients, kann ich also jetzt schon mit DQN kommen oder ist das viel zu früh?
% TODO: sämtlich erwartungswerte fehlen noch, siehe meinen CANN report und das DQN-paper.
% TODO: offensichtlich, die references. Mainly originales Qlearnpaper, was von bellman, originales reinforcementlearnpaper
\section{Q-Learning with Neural Networks}

To understand this section, basic knowledge on how \keyword{Artificial Neural Networks} (\textbf{ANN}s) work and what they do is presupposed. A special focus must also lie on \keyword{Convolutional Neural Networks} (\textbf{CNN}s) \cite{yann_lecun_gradient-based_1998}, mainly used in image processing. As mentioned before, it is (in theory) not only possible to use a Q-table to estimate the $Q^*$-function, but any kind of function approximator. Thanks to the universality theorem\footnote{http://neuralnetworksanddeeplearning.com/chap4.html, I need a better source on this!}, it is known that ANNs are an example of such. The defining feature of ANNs in comparison to other Machine Learning techniques is their ability to store complex, abstract representations of their input when using a \keyword{deep} enough architecture.

\subsection{Deep Q-learning}

\noindent The reason to use neural function approximators instead of a simple Q-table approach for reinforcement learning problems is easy to see: While for a Q-table the states and actions of the Markov Decision Process must be discrete and very limited, this is not the case when using higher-level representations. If the agent's observation of a state of the game is high-dimensional (like for example an image) the chance for an agent to observe the exact same observation twice is extremely slight. Instead, an Artificial Neural Network can learn a higher-level representation of the state, grouping conceptually similar states, and thus generalize to new, previously unseen, states. It is no surprise that the success of \keyword{Deep-Q-Networks} started its journey shortly after the introduction of CNNs, which are able to learn abstract representations of similar images, by now used in countless Computer Vision Applications. \\%TODO: noch ne source?

\textit{Deep-Q-Network} refers to a family of off-policy, online, active, model-free Q-learning algorithms using Deep Neural Networks. % I made it! I used all of those terms! :D
When using ANNs as function approximators for the model of the environment, it will result in a Loss function depending on the Neural Network parameters, specified by $\theta$. The update rule in Deep Networks depends on the gradient with respect to its, $\nabla_{\theta_i}L(\theta_i)$. These weights correspond to the parameters of the $\hat{Q}$-function of the agent. %ist so nicht ganz wie bei DQN, da da im linken teil der gleichung \theta^- = \theta_{i-1} verwendet wird.. check grad nur nicht warum ind er linken und nicht in der rechten hälfte
While there are attempts to use Artificial Neural Networks for Q-learning as early as 1993[SOURCE], some key components of modern Deep-Q-Networks (\textbf{DQN}s) were missing, leading to satisfactory performance only in very limited settings. In standard online RL tasks, the update step minimizing the loss specified in \ref{l2loss} is performed right after the observation occured to the agent. As consecutive steps of MDPs tend to be correlated, the update using gradient descent is prone to oscillation in its result, thus never converging to an optimal $Q^*$-function. It was not until \keyword{Deepmind}'s famous papers in 2013\cite{mnih_playing_2013} and 2015\cite{mnih_human-level_2015}, that those issues were successfully adressed.

One important step when using ANNs instead of Q-tables is, to perform stochastic gradient descent using minibatches. In every gradient descent step of the Neural Network, neither only the last temporal difference error $TD_t$ is considered, nor the entire sequence $TD_0, .., TD_{t_t}$. Instead, as usual when dealing with ANNs, minibatches are sampled from the set of all observations. When performing the gradient descent step, the weights for the target $y_t$ are fixed, making the minimization of the temporal difference error a well-defined optimization problem, similar to that one of supervised learning, during the learning step.\\

%warum man generell bei ANNs in minibatches lernt
\noindent The two important innovations introduced in Deepminds DQN-architecture were the use of a \keyword{target network} as well as the technique of \keyword{experience replay}, which in combination successfully hindered the problem of oscillating and non-converging action-value function, even though still no formal mathematical proof of that is given. %TODO: or is it by now? I could quote razvan pascanu here

\subsubsection{Experience Replay}
%dass das auch data efficiency erhöht - alle 4 schritte wird ein 32er minibatch genommen, jedes wird mehrfach genutzt
As mentioned above, learning only from the most recent experiences biases the policy towards those situations, limiting convergence of the Q-function. To adress this issue, the DQN uses an experience replay memory: Every percept of the environment (the $\langle s_t, a_t, r_t, s_{t+1}, t+1==t_t \rangle$ - tuple) is added to a limited-size memory of the agent. When then performing the learning step, the agent samples random minibatches from this memory to perform learning on a maximally uncorrelated sample of experiences. In the original definition of DQN, those minibatches are drawn uniformely at random, while as of today, better techniques for sampling those minibatches are available\cite{schaul_prioritized_2015}, increasing the performance of the resulting algorithm significantly. %TODO dieses paper wirklich lesen und drauf eingehen, und am besten auch minimal coden, das ist superwichtig!

\subsubsection{Target Networks}
During the training procedure, the DQN-algorithm uses a separate network to generate the target-Q-values, used to compute the loss (\ref{l2loss}), necessary for the learning step of every iteration. The idea behind that is, that the Q-values of the \keyword{online network} shift in such a way, that a feedback loop can arise between the target- and estimated Q-values, shifting the Q-value more and more into a similar direction. %TODO: das ist sehr unwissenschaftlich ausgedrückt und geht auf jeden Fall besser erklärt, using eine der vorhergegangenen 
To lessen the risk of such feedback loops, the DQN algorithm introduced the use of a second network for calculating the loss: the target network. This is only periodically updated with the weights of the online network used for the policy, which reduces the risk of correlations in the action-value $Q_t$ and the corresponding target-value $y_t$ (see equation \ref{eq:ycases}).

The use of this two techniques leads to the Q-learning update rule as used in \cite{mnih_human-level_2015}:
\begin{equation}
	L_i(\theta_i) = \mathds{E}_{\langle s_t,a_t,r_t,s_{t+1} \rangle \sim U(D)} \Bigg[\Big( r + \gamma * max_{a_{t+1}} \hat{Q}(s_{t+1}, a_{t+1}; \theta^-_i) - \hat{Q}(s_t,a_t;\theta_i) \Big)^2\Bigg]
\end{equation}
\begin{flushright}
	\scriptsize
	Where $i$ stands for the current network update iteration, $\theta_i$ for the current weights of the target network (updated every $C$ iterations to be equal to the weights of the online network $\theta_i$), $Q(\cdot,\cdot;\theta)$ for the Q-value dependend on a ANN using the weights $\theta$, $\mathds{E}[\cdot]$ for the expected value in an indeterministic environment, D for the contents of the replay memory of length $\lvert D \rvert$ containing $\langle s_t,a_t,r_t,s_{t+1} \rangle$-tuples, and $U(\cdot)$ for a uniform distribution.
\end{flushright}
As is the case with the experience replay mechanism, the usage of a target network was improved as well by now - modern algorithms don't perform a hard update of the target network every $C$ steps, but instead perform \keyword{soft target network update}, where every iteration, the weights of the target network are defined as $\theta^-_i := \theta_i * \tau + \theta^-_i * (1-\tau)$ with $0 < \tau \ll 1$, first introduced in \cite{lillicrap_continuous_2015}. This improves the stability of the algorithm even more.\\

% TODO: Pseudocode mit den Improvements, double-Q und dueling-Q, aber ERST NACH exploration techniques!
% Loss function und pseudocode, HIER HIN

\subsubsection{Double-Q-Learning}
In DoubleQ, we still use the greedy policy to select actions, however we evaluate how good it is with another set of weights


\subsubsection{Dueling Q-Learning}

\subsubsection{Using Recurrent Networks}

\section{Policy Gradient Techniques}

\subsection{Actor-Critic architectures}

All previously introduced techniques are adaptations of the Q-learning algorithm \cite{sutton_learning_1988},\cite{watkins_learning_1989}. In Q-learning, the algorithms learns via temporal differences the state-action value Q for the greedy policy $\pi = argmax_a Q(s,a) \forall s \in S$. As purely using this policy hinders the agent from \keyword{exploration}, Q-learning algorithms must always be off-policy, and actually follow a slightly different policy than $\pi$. Learning with this approach is however generally limited: $argmax_aQ(\cdot, a)$ can only easily be found in settings where the action space $A$ is finite and discrete.\\

\noindent However in a lot of scenarios, the action space is not discrete, but continuous: \mbox{$A \subseteq \mathds{R}^n$}. In such situations, the approach is to model the policy explicitly, with another function approximator. This gives rise to \keyword{actor-critic} architectures. In an actor-critic approach, there are two function approximators: The \keyword{critic} uses temporal differences to estimate the Q-value of states $s \in S$ and actions $a \in A$. If the critic were perfect, it would return the true action-value function of the policy $\pi$, $Q^\pi(s,a)$. As that is not the case however, it is in fact similar to the Bellman-function-approximator from previous sections. In contrast to those however, the policy is now explicitly modeled by the \keyword{actor}. In the case of a stochastic policy, it would be represented by a parametric probability distribution $\pi_\theta(a|s) = \mathds{P}[a|s;\theta]$, however here we only consider the case of deterministic policies $a = \pi_\theta(s)$. Note however, that this will (again) lead to the necessesity of off-policy algorithms, as a purely deterministic policy does not allow for adequate exploration of state-space $S$ or action-space $A$.

Standard actor-critic algorithms necessarily rely on the \keyword{policy gradient theorem}, which states a relation between the gradient of the policy and the gradient of the performance function:
\begin{equation}
	\nabla_{\theta}J(\pi_\theta) = \mathds{E}_{s\sim\rho^\pi,a\sim\pi}[\nabla_\theta log \pi_\theta(a|s)\hat{Q}^\pi(s,a)]
\end{equation} %TODO: das ist nur richtig wenn ich den performance dingsi J, von unten, schon vorher erklärt habeeeee

%dass das eigentlich nur mit der richtigen Q* gehen würde, und dass wir aufpassen müssen dass unser ^Q  das nicht biased
\subsubsection{Deterministic Policy Gradient}

The idea in the Deterministic Policy Gradient technique is to use a relation between the gradient of the deterministic policy (estimated by the actor), and the gradient of the action-value function Q. The critic estimates Q-function using a differentiable function approximator, and then updates the \textit{policy} parameters (the ones of the actor) in the dirction of the approximate action-value gradient.\footnote{\textit{``The critic estimates the action-value function while the actor ascends the gradient of the action-value-function"} (cf. \cite{silver_deterministic_2014})}

%TODO den folgenden abschnitt an den Anfaaaang
As mentioned at the beginning, the state transition of MDPs is in general indeterministic. Because of that, one can only talk about state distributions, depending on the policy of the agent: $\rho^\pi(s)$. An agent's goal is to gain a policy that can follow the trajectory of that state distribution with the highest \keyword{expected} reward. Thus, we can only write the performance ojective $J$ of a policy $\pi$ as expectation, which is technically the integral over the state- as well as the action-distribution, each depending on the policy.
\begin{equation}
	J(\pi_\theta) = \mathds{E}_{s\sim\rho^\pi,a\sim\pi}[R(s,a)].
\end{equation}
%TODO: well, das hier ganz an den anfang. 
The idea behind policy gradient algorithms is accordingly to adjust the parameters $\theta$ of the policy in the direction of the performance gradient $\nabla_{\theta}J(\pi_\theta)$.
%TODO hier policy gradient theorem, bah.
%TODO die komplette rechte hälfte der zweiten seite vom silver-paper

\subsubsection*{Deep DPG}

The \keyword{Deep DPG Algorithm} is an off-policy actor-critic, online, active, model-free, deterministic policy gradient algorithm for continous action-spaces.


\subsection{Exploration techniques}

As mentioned in the beginning of this chapter, I only considered deterministic policies so far: $\pi(s) = a$. In practice however, using purely deterministic policies leads to a complete lack of \keyword{exploration} of the state space $S$ of the MDP. Once the agent found a path to a terminal state, it will continue \keyword{exploiting} this path. In order to explore the full state space, in fact a stochastic policy is necessary.

Blablabla, dass das der Grund ist warum alle unsere algorithmen bisher off-policy waren - in DQN steht "wir lernen über die greedy argmax policy while performing epsilon-greedy", und in DPG steht auch "the basic idea is to choose actions according to a stochastic behaviour policy (to ensure adequate exploration), but to learn about a deterministic target policy"

% TODO: HIER jetzt zwei pseudocodes, einmal von DQN, einmal von DDPG, mit entsprechenden exploration techniques

\chapter{Related work}

\section{Reinforcement Learning Frameworks}

Gym/Universe
Torcs
schreiben dass die Arcade Learning Environment (Bellemare et al., 2013 aus dem Dueling) zu Gym wurde (I guess)
torcs: im DDPG-paper steht "Torcs has previously been used as a testbed in other
policy learning approaches (Koutnik et al., 2014b). "!!!!!!!!!!!!!!!
fußnote dass ich auch im code nen evaluator für ddpg hab der gym und pendulum swingup nutzt

\section{Self-driving cars}

Nvidias deep-drive
RRT*
Tensorkart
Tesla
Lidar
hier in den fußnoten die ganzen non-scientific quellen wie tensorkart undso

\chapter{Program Architecture}


The program was written by the author of this work and is licensed under the GNU General Public License (GNU GPLv3). Its source code is attached in the appendix of this work and additionally can be found digitally on the enclosed CD-ROM. The machine learning part was written in \textsc{Python}, using the \textsc{Tensorflow}-library \parencite{abadi_tensorflow:_2015}.
%TODO: TF-version, dann noch was zu Unity!!!

\section{Design choices}

\subsection{The game as a reinforcement learning problem}

-ungefähres UML-diagramm

\subsection{The vectors}

\subsection{Exploration}

\subsection{Reward}

\subsection{Performance measure}

\section{Implementation}

\subsection{The game}

\subsubsection{What Leon did already}

\subsubsection{Communication}

-den sockets post
-das von leon gemalte ablaufdiagramm

\subsection{The agent}

Unbedingt auf jeden Fall UML-diagramm

\subsubsection{Challenges and Solutions}

DQN vs DDPG, sehend vs nicht-sehend, ...

\subsubsection{Pretraining}

\subsubsection{The different agents}

sehend vs nicht sehend, ...

\subsubsection{Network architecture}

1. dqn-algorithm
  - anzahl layer, Batchnorm, doubles dueling
  - clipping wieder rein, reference auf das dueling
  - grundsätzlich gegen batchnorm entschieden, siehe reddit post
  - MIT GRAFIK
  - Adam und tensorflow quoten, siehe zotero
2. ddpg
  - anzahl layer, Batchnorm
  - MIT GRAFIK

\chapter{Analysis, Results and open Questions}

testing took place on a win10 machine, ...
Answer all of the research questions explicitly!!!

\chapter{Discussion}

"Fragestellung aus der Einleitung wird erneut aufgegriffen und die Arbeitsschritte
werden resümiert"
Zusammen mit der Conclusion 10\% der Gesamtlänge

\chapter{Conclusion and future directions}


%in https://en.wikipedia.org/wiki/Markov_decision_process#CITEREFHoward1960 steht noch was zu policy iteration

%Deep Q Learning 
%While first ideas came up as early as 1993 (lin 1993 in ftp://mi.eng.cam.ac.uk/pub/reports/auto-pdf/rummery_tr166.pdf, ...


%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%\section{Getting Started with this Template}
%
%If you are familiar with \LaTeX{}, then you should explore the directory structure of the template and then proceed to place your own information into the \emph{THESIS INFORMATION} block of the \file{main.tex} file. You can then modify the rest of this file to your unique specifications based on your degree/university. Section \ref{FillingFile} on page \pageref{FillingFile} will help you do this. Make sure you also read section \ref{ThesisConventions} about thesis conventions to get the most out of this template.
%
%If you are new to \LaTeX{} it is recommended that you carry on reading through the rest of the information in this document.
%
%Before you begin using this template you should ensure that its style complies with the thesis style guidelines imposed by your institution. In most cases this template style and layout will be suitable. If it is not, it may only require a small change to bring the template in line with your institution's recommendations. These modifications will need to be done on the \file{MastersDoctoralThesis.cls} file.
%
%\subsection{About this Template}
%
%This \LaTeX{} Thesis Template is originally based and created around a \LaTeX{} style file created by Steve R.\ Gunn from the University of Southampton (UK), department of Electronics and Computer Science. You can find his original thesis style file at his site, here:
%\url{http://www.ecs.soton.ac.uk/~srg/softwaretools/document/templates/}
%
%Steve's \file{ecsthesis.cls} was then taken by Sunil Patel who modified it by creating a skeleton framework and folder structure to place the thesis files in. The resulting template can be found on Sunil's site here:
%\url{http://www.sunilpatel.co.uk/thesis-template}
%
%Sunil's template was made available through \url{http://www.LaTeXTemplates.com} where it was modified many times based on user requests and questions. Version 2.0 and onwards of this template represents a major modification to Sunil's template and is, in fact, hardly recognisable. The work to make version 2.0 possible was carried out by \href{mailto:vel@latextemplates.com}{Vel} and Johannes Böttcher.
%
%%----------------------------------------------------------------------------------------
%
%\section{What this Template Includes}
%
%\subsection{Folders}
%
%This template comes as a single zip file that expands out to several files and folders. The folder names are mostly self-explanatory:
%
%\keyword{Appendices} -- this is the folder where you put the appendices. Each appendix should go into its own separate \file{.tex} file. An example and template are included in the directory.
%
%\keyword{Chapters} -- this is the folder where you put the thesis chapters. A thesis usually has about six chapters, though there is no hard rule on this. Each chapter should go in its own separate \file{.tex} file and they can be split as:
%\begin{itemize}
%\item Chapter 1: Introduction to the thesis topic
%\item Chapter 2: Background information and theory
%\item Chapter 3: (Laboratory) experimental setup
%\item Chapter 4: Details of experiment 1
%\item Chapter 5: Details of experiment 2
%\item Chapter 6: Discussion of the experimental results
%\item Chapter 7: Conclusion and future directions
%\end{itemize}
%This chapter layout is specialised for the experimental sciences, your discipline may be different.
%
%\keyword{Figures} -- this folder contains all figures for the thesis. These are the final images that will go into the thesis document.
%
%\subsection{Files}
%
%Included are also several files, most of them are plain text and you can see their contents in a text editor. After initial compilation, you will see that more auxiliary files are created by \LaTeX{} or BibTeX and which you don't need to delete or worry about:
%
%\keyword{example.bib} -- this is an important file that contains all the bibliographic information and references that you will be citing in the thesis for use with BibTeX. You can write it manually, but there are reference manager programs available that will create and manage it for you. Bibliographies in \LaTeX{} are a large subject and you may need to read about BibTeX before starting with this. Many modern reference managers will allow you to export your references in BibTeX format which greatly eases the amount of work you have to do.
%
%\keyword{MastersDoctoralThesis.cls} -- this is an important file. It is the class file that tells \LaTeX{} how to format the thesis. 
%
%\keyword{main.pdf} -- this is your beautifully typeset thesis (in the PDF file format) created by \LaTeX{}. It is supplied in the PDF with the template and after you compile the template you should get an identical version.
%
%\keyword{main.tex} -- this is an important file. This is the file that you tell \LaTeX{} to compile to produce your thesis as a PDF file. It contains the framework and constructs that tell \LaTeX{} how to layout the thesis. It is heavily commented so you can read exactly what each line of code does and why it is there. After you put your own information into the \emph{THESIS INFORMATION} block -- you have now started your thesis!
%
%Files that are \emph{not} included, but are created by \LaTeX{} as auxiliary files include:
%
%\keyword{main.aux} -- this is an auxiliary file generated by \LaTeX{}, if it is deleted \LaTeX{} simply regenerates it when you run the main \file{.tex} file.
%
%\keyword{main.bbl} -- this is an auxiliary file generated by BibTeX, if it is deleted, BibTeX simply regenerates it when you run the \file{main.aux} file. Whereas the \file{.bib} file contains all the references you have, this \file{.bbl} file contains the references you have actually cited in the thesis and is used to build the bibliography section of the thesis.
%
%\keyword{main.blg} -- this is an auxiliary file generated by BibTeX, if it is deleted BibTeX simply regenerates it when you run the main \file{.aux} file.
%
%\keyword{main.lof} -- this is an auxiliary file generated by \LaTeX{}, if it is deleted \LaTeX{} simply regenerates it when you run the main \file{.tex} file. It tells \LaTeX{} how to build the \emph{List of Figures} section.
%
%\keyword{main.log} -- this is an auxiliary file generated by \LaTeX{}, if it is deleted \LaTeX{} simply regenerates it when you run the main \file{.tex} file. It contains messages from \LaTeX{}, if you receive errors and warnings from \LaTeX{}, they will be in this \file{.log} file.
%
%\keyword{main.lot} -- this is an auxiliary file generated by \LaTeX{}, if it is deleted \LaTeX{} simply regenerates it when you run the main \file{.tex} file. It tells \LaTeX{} how to build the \emph{List of Tables} section.
%
%\keyword{main.out} -- this is an auxiliary file generated by \LaTeX{}, if it is deleted \LaTeX{} simply regenerates it when you run the main \file{.tex} file.
%
%So from this long list, only the files with the \file{.bib}, \file{.cls} and \file{.tex} extensions are the most important ones. The other auxiliary files can be ignored or deleted as \LaTeX{} and BibTeX will regenerate them.
%
%%----------------------------------------------------------------------------------------
%
%\section{Filling in Your Information in the \file{main.tex} File}\label{FillingFile}
%
%You will need to personalise the thesis template and make it your own by filling in your own information. This is done by editing the \file{main.tex} file in a text editor or your favourite LaTeX environment.
%
%Open the file and scroll down to the third large block titled \emph{THESIS INFORMATION} where you can see the entries for \emph{University Name}, \emph{Department Name}, etc \ldots
%
%Fill out the information about yourself, your group and institution. You can also insert web links, if you do, make sure you use the full URL, including the \code{http://} for this. If you don't want these to be linked, simply remove the \verb|\href{url}{name}| and only leave the name.
%
%When you have done this, save the file and recompile \code{main.tex}. All the information you filled in should now be in the PDF, complete with web links. You can now begin your thesis proper!
%
%%----------------------------------------------------------------------------------------
%
%\section{The \code{main.tex} File Explained}
%
%The \file{main.tex} file contains the structure of the thesis. There are plenty of written comments that explain what pages, sections and formatting the \LaTeX{} code is creating. Each major document element is divided into commented blocks with titles in all capitals to make it obvious what the following bit of code is doing. Initially there seems to be a lot of \LaTeX{} code, but this is all formatting, and it has all been taken care of so you don't have to do it.
%
%Begin by checking that your information on the title page is correct. For the thesis declaration, your institution may insist on something different than the text given. If this is the case, just replace what you see with what is required in the \emph{DECLARATION PAGE} block.
%
%Then comes a page which contains a funny quote. You can put your own, or quote your favourite scientist, author, person, and so on. Make sure to put the name of the person who you took the quote from.
%
%Following this is the abstract page which summarises your work in a condensed way and can almost be used as a standalone document to describe what you have done. The text you write will cause the heading to move up so don't worry about running out of space.
%
%Next come the acknowledgements. On this page, write about all the people who you wish to thank (not forgetting parents, partners and your advisor/supervisor).
%
%The contents pages, list of figures and tables are all taken care of for you and do not need to be manually created or edited. The next set of pages are more likely to be optional and can be deleted since they are for a more technical thesis: insert a list of abbreviations you have used in the thesis, then a list of the physical constants and numbers you refer to and finally, a list of mathematical symbols used in any formulae. Making the effort to fill these tables means the reader has a one-stop place to refer to instead of searching the internet and references to try and find out what you meant by certain abbreviations or symbols.
%
%The list of symbols is split into the Roman and Greek alphabets. Whereas the abbreviations and symbols ought to be listed in alphabetical order (and this is \emph{not} done automatically for you) the list of physical constants should be grouped into similar themes.
%
%The next page contains a one line dedication. Who will you dedicate your thesis to?
%
%Finally, there is the block where the chapters are included. Uncomment the lines (delete the \code{\%} character) as you write the chapters. Each chapter should be written in its own file and put into the \emph{Chapters} folder and named \file{Chapter1}, \file{Chapter2}, etc\ldots Similarly for the appendices, uncomment the lines as you need them. Each appendix should go into its own file and placed in the \emph{Appendices} folder.
%
%After the preamble, chapters and appendices finally comes the bibliography. The bibliography style (called \option{authoryear}) is used for the bibliography and is a fully featured style that will even include links to where the referenced paper can be found online. Do not underestimate how grateful your reader will be to find that a reference to a paper is just a click away. Of course, this relies on you putting the URL information into the BibTeX file in the first place.
%
%%----------------------------------------------------------------------------------------
%
%\section{Thesis Features and Conventions}\label{ThesisConventions}
%
%To get the best out of this template, there are a few conventions that you may want to follow.
%
%One of the most important (and most difficult) things to keep track of in such a long document as a thesis is consistency. Using certain conventions and ways of doing things (such as using a Todo list) makes the job easier. Of course, all of these are optional and you can adopt your own method.
%
%\subsection{Printing Format}
%
%This thesis template is designed for double sided printing (i.e. content on the front and back of pages) as most theses are printed and bound this way. Switching to one sided printing is as simple as uncommenting the \option{oneside} option of the \code{documentclass} command at the top of the \file{main.tex} file. You may then wish to adjust the margins to suit specifications from your institution.
%
%The headers for the pages contain the page number on the outer side (so it is easy to flick through to the page you want) and the chapter name on the inner side.
%
%The text is set to 11 point by default with single line spacing, again, you can tune the text size and spacing should you want or need to using the options at the very start of \file{main.tex}. The spacing can be changed similarly by replacing the \option{singlespacing} with \option{onehalfspacing} or \option{doublespacing}.
%
%\subsection{Using US Letter Paper}
%
%The paper size used in the template is A4, which is the standard size in Europe. If you are using this thesis template elsewhere and particularly in the United States, then you may have to change the A4 paper size to the US Letter size. This can be done in the margins settings section in \file{main.tex}.
%
%Due to the differences in the paper size, the resulting margins may be different to what you like or require (as it is common for institutions to dictate certain margin sizes). If this is the case, then the margin sizes can be tweaked by modifying the values in the same block as where you set the paper size. Now your document should be set up for US Letter paper size with suitable margins.
%
%\subsection{References}
%
%The \code{biblatex} package is used to format the bibliography and inserts references such as this one \parencite{Reference1}. The options used in the \file{main.tex} file mean that the in-text citations of references are formatted with the author(s) listed with the date of the publication. Multiple references are separated by semicolons (e.g. \parencite{Reference2, Reference1}) and references with more than three authors only show the first author with \emph{et al.} indicating there are more authors (e.g. \parencite{Reference3}). This is done automatically for you. To see how you use references, have a look at the \file{Chapter1.tex} source file. Many reference managers allow you to simply drag the reference into the document as you type.
%
%Scientific references should come \emph{before} the punctuation mark if there is one (such as a comma or period). The same goes for footnotes\footnote{Such as this footnote, here down at the bottom of the page.}. You can change this but the most important thing is to keep the convention consistent throughout the thesis. Footnotes themselves should be full, descriptive sentences (beginning with a capital letter and ending with a full stop). The APA6 states: \enquote{Footnote numbers should be superscripted, [...], following any punctuation mark except a dash.} The Chicago manual of style states: \enquote{A note number should be placed at the end of a sentence or clause. The number follows any punctuation mark except the dash, which it precedes. It follows a closing parenthesis.}
%
%The bibliography is typeset with references listed in alphabetical order by the first author's last name. This is similar to the APA referencing style. To see how \LaTeX{} typesets the bibliography, have a look at the very end of this document (or just click on the reference number links in in-text citations).
%
%\subsubsection{A Note on bibtex}
%
%The bibtex backend used in the template by default does not correctly handle unicode character encoding (i.e. "international" characters). You may see a warning about this in the compilation log and, if your references contain unicode characters, they may not show up correctly or at all. The solution to this is to use the biber backend instead of the outdated bibtex backend. This is done by finding this in \file{main.tex}: \option{backend=bibtex} and changing it to \option{backend=biber}. You will then need to delete all auxiliary BibTeX files and navigate to the template directory in your terminal (command prompt). Once there, simply type \code{biber main} and biber will compile your bibliography. You can then compile \file{main.tex} as normal and your bibliography will be updated. An alternative is to set up your LaTeX editor to compile with biber instead of bibtex, see \href{http://tex.stackexchange.com/questions/154751/biblatex-with-biber-configuring-my-editor-to-avoid-undefined-citations/}{here} for how to do this for various editors.
%
%\subsection{Tables}
%
%Tables are an important way of displaying your results, below is an example table which was generated with this code:
%
%{\small
%\begin{verbatim}
%\begin{table}
%\caption{The effects of treatments X and Y on the four groups studied.}
%\label{tab:treatments}
%\centering
%\begin{tabular}{l l l}
%\toprule
%\tabhead{Groups} & \tabhead{Treatment X} & \tabhead{Treatment Y} \\
%\midrule
%1 & 0.2 & 0.8\\
%2 & 0.17 & 0.7\\
%3 & 0.24 & 0.75\\
%4 & 0.68 & 0.3\\
%\bottomrule\\
%\end{tabular}
%\end{table}
%\end{verbatim}
%}
%
%\begin{table}
%\caption{The effects of treatments X and Y on the four groups studied.}
%\label{tab:treatments}
%\centering
%\begin{tabular}{l l l}
%\toprule
%\tabhead{Groups} & \tabhead{Treatment X} & \tabhead{Treatment Y} \\
%\midrule
%1 & 0.2 & 0.8\\
%2 & 0.17 & 0.7\\
%3 & 0.24 & 0.75\\
%4 & 0.68 & 0.3\\
%\bottomrule\\
%\end{tabular}
%\end{table}
%
%You can reference tables with \verb|\ref{<label>}| where the label is defined within the table environment. See \file{Chapter1.tex} for an example of the label and citation (e.g. Table~\ref{tab:treatments}).
%
%\subsection{Figures}
%
%There will hopefully be many figures in your thesis (that should be placed in the \emph{Figures} folder). The way to insert figures into your thesis is to use a code template like this:
%\begin{verbatim}
%\begin{figure}
%\centering
%\includegraphics{Figures/Electron}
%\decoRule
%\caption[An Electron]{An electron (artist's impression).}
%\label{fig:Electron}
%\end{figure}
%\end{verbatim}
%Also look in the source file. Putting this code into the source file produces the picture of the electron that you can see in the figure below.
%
%\begin{figure}[th]
%\centering
%\includegraphics{Figures/Electron}
%\decoRule
%\caption[An Electron]{An electron (artist's impression).}
%\label{fig:Electron}
%\end{figure}
%
%Sometimes figures don't always appear where you write them in the source. The placement depends on how much space there is on the page for the figure. Sometimes there is not enough room to fit a figure directly where it should go (in relation to the text) and so \LaTeX{} puts it at the top of the next page. Positioning figures is the job of \LaTeX{} and so you should only worry about making them look good!
%
%Figures usually should have captions just in case you need to refer to them (such as in Figure~\ref{fig:Electron}). The \verb|\caption| command contains two parts, the first part, inside the square brackets is the title that will appear in the \emph{List of Figures}, and so should be short. The second part in the curly brackets should contain the longer and more descriptive caption text.
%
%The \verb|\decoRule| command is optional and simply puts an aesthetic horizontal line below the image. If you do this for one image, do it for all of them.
%
%\LaTeX{} is capable of using images in pdf, jpg and png format.
%
%\subsection{Typesetting mathematics}
%
%If your thesis is going to contain heavy mathematical content, be sure that \LaTeX{} will make it look beautiful, even though it won't be able to solve the equations for you.
%
%The \enquote{Not So Short Introduction to \LaTeX} (available on \href{http://www.ctan.org/tex-archive/info/lshort/english/lshort.pdf}{CTAN}) should tell you everything you need to know for most cases of typesetting mathematics. If you need more information, a much more thorough mathematical guide is available from the AMS called, \enquote{A Short Math Guide to \LaTeX} and can be downloaded from:
%\url{ftp://ftp.ams.org/pub/tex/doc/amsmath/short-math-guide.pdf}
%
%There are many different \LaTeX{} symbols to remember, luckily you can find the most common symbols in \href{http://ctan.org/pkg/comprehensive}{The Comprehensive \LaTeX~Symbol List}.
%
%You can write an equation, which is automatically given an equation number by \LaTeX{} like this:
%\begin{verbatim}
%\begin{equation}
%E = mc^{2}
%\label{eqn:Einstein}
%\end{equation}
%\end{verbatim}
%
%This will produce Einstein's famous energy-matter equivalence equation:
%\begin{equation}
%E = mc^{2}
%\label{eqn:Einstein}
%\end{equation}
%
%All equations you write (which are not in the middle of paragraph text) are automatically given equation numbers by \LaTeX{}. If you don't want a particular equation numbered, use the unnumbered form:
%\begin{verbatim}
%\[ a^{2}=4 \]
%\end{verbatim}
%
%%----------------------------------------------------------------------------------------
%
%\section{Sectioning and Subsectioning}
%
%You should break your thesis up into nice, bite-sized sections and subsections. \LaTeX{} automatically builds a table of Contents by looking at all the \verb|\chapter{}|, \verb|\section{}|  and \verb|\subsection{}| commands you write in the source.
%
%The Table of Contents should only list the sections to three (3) levels. A \verb|chapter{}| is level zero (0). A \verb|\section{}| is level one (1) and so a \verb|\subsection{}| is level two (2). In your thesis it is likely that you will even use a \verb|subsubsection{}|, which is level three (3). The depth to which the Table of Contents is formatted is set within \file{MastersDoctoralThesis.cls}. If you need this changed, you can do it in \file{main.tex}.
%
%%----------------------------------------------------------------------------------------
%
%\section{In Closing}
%
%You have reached the end of this mini-guide. You can now rename or overwrite this pdf file and begin writing your own \file{Chapter1.tex} and the rest of your thesis. The easy work of setting up the structure and framework has been taken care of for you. It's now your job to fill it out!
%
%Good luck and have lots of fun!
%
%\begin{flushright}
%Guide written by ---\\
%Sunil Patel: \href{http://www.sunilpatel.co.uk}{www.sunilpatel.co.uk}\\
%Vel: \href{http://www.LaTeXTemplates.com}{LaTeXTemplates.com}
%\end{flushright}
