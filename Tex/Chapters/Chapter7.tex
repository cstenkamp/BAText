

\chapter{Discussion and future directions}

\section{Platform}

-dass sich herausgestellt hat das simultaneous inference and learning a lot slower ist!
-die performance vom server
-multiple unity instances (feier-text)
-unity zeit verdoppeln
-feed dict performance and getting rid of it https://github.com/tensorflow/tensorflow/issues/2919
-dank sockets können agent und env easypeasy auf verschiedenen maschinen sein
-wie praktisch das mit dem info senden ist, dass das mit torcs-env-accessen nicht so einfach wäre
-wie viel arbeit die socketkommunikation war und wie überraschend effizient die ist
%scalability: mit minimal arbeit könnte man mehrere unity-instanzen haben die sich am reclistenport anmelden, an nen eigenen receiverthread weitergeleitet werden, eigenen inputval haben um das bottleneck instanzen erzeugen zu umgehen
%den anderen emma-text wie man weitere instanzen hinzufügen kann
-dass es zur beschleuniung möglich wäre mehrere aggregatoren zu haben, die immer die recht aktuellste policy haben und mit unity interagieren und das nem learn-thread schicken, auf verschiedenen Maschinen
-wie einfach man gute reward-definitionen finden kann mit -die funktion wo man sich den durchgängingen reward in ner farbe plotten kann (auch bei \keystroke{H})

\section{Agents}

Nächsten Sachen die ich machen würde...
-count-based exploration stuff: https://arxiv.org/pdf/1706.08090.pdf / https://arxiv.org/abs/1703.01310
-prioritized experience replay: https://arxiv.org/abs/1511.05952
-auf der dqn-front https://arxiv.org/pdf/1602.01783.pdf A3C

-dass die ddpg-gaussiandistance-pretraining function nicht funktioniert

\colorbox{red}{das experimentelle getstatecountfeaturevec, das aber noch nix tun will}
getstatecountfeaturevec macht für das was ich mache mit meiner netzwerk-struktur keinen sinn (256 neurons direkt bevor es in die actions geht)

-dass wir beim pretraining die andere methode genommen hätten aaaaaaber die nicht gekappt hat

-dass man mit supervised agents testen kann ob dataset separable (-dataset is separable as can be tested by dqn\_sv\_agent)
-und gym_test

-da qualitative änderungen sich literally immer erst nach tagen bemerkbar machen war paramter-tuning EXTREM langwierig und ist noch nicht done
-was die nächsten schritte sind, was ich erwarte und warum. Klar machen was save ist und was vermutugn.
-it is obvious, that DQN cannot be as good as DDPG - (wheel must be turned xy to avoid the obstacle because either note enough movement (z < xy) or course of dimensionality)
-die neuesten openAi algorithmen...!

-man kann a3c nutzen statt dqn
-wie schlecht die anderen eigentlcih funktioniert haben so "fleigt aus der ersten kurve, klappt bei konstant 1.8kmh speed"... meins fährt gut bei max-speed, und fährt hin und wieder ganze runden I mean thats good!!

-dass man mit dem SV-agent testen kann ob dataset separable, praktischerweise kann das supervisednet ja die gleiche struktur haben wie das dqn (gucken kann ob dieseundjene state-definition sinnvoll ist)

-dass er definitiv lernt vor ner wand zu bremsen, dank der reward function, war nen langer weg bis dahin.

-dass actions als input (beosnder bei pretraining) sehr schlecht ist da der dann lernt immer die vorherige zu machen

Die paper die bei openAI erwähnt waren die das ganze in viel kreasser sind, die Dota spielen können etc!
dass ich gerne hätte dass das am anfang der policy folgt und am ende weiter exploriert, damit es auchdie letzte kurve lernt... genau dafür ware halt intrinsic motivation und prioritized experience replay

\chapter{Conclusion}


[wennde testen wilslt ob dataset seperable ist, guckste in sv\_agent, wennde wissen willst ob das model funktioniert, guckste in gym\_text]


-wie gut dieses framework mittlerweile funktioniert, und was für ein langer weg das dahin war (dass das definitiv was war das ne ganze arbeit füllt, aber zukünftige arbeiten jetzt super gut darauf gehen, weil wegen bam framework.)
-das das framework definitiv comparable zu dem given in den beiden torcs-papern (BEIDE NOCHMAL ZITIEREN!!) ist.
-dass ich kein anderes paper gesehen habe dass sinvolle sachen macht... das virutal to real macht 9 discrete actions, das ddpg-torcs-ding macht "sometimes useful pocicies", ...

Normally when dealing with self-driving cars, there are countless additional issues, each making up a whole new challenge on their own, like wheather conditions (snow, rain, fog), pedestrians, other cars, reflections, merging into ongoing traffic, ...
we make it easier here.

"Fragestellung aus der Einleitung wird erneut aufgegriffen und die Arbeitsschritte
werden resümiert"
Zusammen mit der Conclusion 10\% der Gesamtlänge



