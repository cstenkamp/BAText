\chapter{Results -- Analysis and Performance}

As stated in the introduction, in the course of this thesis not only platform and agents were implemented, but the agents were also tested to answer the research questions from section~\ref{sec:researchquestions}. The testing took place on a \keyword{Windows 10 Pro} machine, using GPU-accelerated Tensorflow with an \keyword{NVIDIA GeForce GTX 970} graphics card.

Answer all of the research questions explicitly!!!

-parameter selection gehört hier rein!!


\section{General Performance}

The first and foremost result is, that some were able to learn successful policies that are able to drive complete laps in a reasonable time. 
-aus zeitgründen nur bis die das erste mal schaffen, auf zeit wird nicht geschaut
-random agent
-human
-nur geradeaus
\textit{How agents that rely purely on pretraining perform in comparison to reinforcement learning agents}

\begin{figure}[h]
	{%
		\setlength{\fboxsep}{0pt}%
		\setlength{\fboxrule}{1pt}%
		\fbox{\includegraphics[width=.6\textwidth]{performance_plots/humandrive_average1_50eps}}%
	}%
	\centering
	\caption{Exemplary laps driven by a human}
	\label{fig:humandrive}
\end{figure}

% TODO -dass halt eigentlich zu jedem zeitpunkt compared zu rrt oder human sein müsste

\subsection{Supervised Agents}

-wie schwer es ist die performance von sv-agent zu assessen weil selbst wenn 60\% richtig sind kann das heißen dass er entweder nur geradeaus fährt oder, weil er im ersten frame falsch leigt, noch so viel richtig machen kann und nix davon sieht weil er direkt in die wand fährt
-dass purely pretraining agents nur mit max-speed klappen
-dass nen random agent offensichtlich scheiße ist
-dass man mit supervised agents testen kann ob dataset separable (-dataset is separable as can be tested by dqn\_sv\_agent)
-das headstart scheint helfen
-wie schnell humans fahren
-die beiden offensichtlichen local maxima, erste kurve 16prozent, zweite kurve 30something... wie man sehr deutlich in den plots sieht
-dass ein supervised agent nicht lernen kann da er immer in die wand brettert

\subsection{RL Agents}

-dass alle cars useful driving policies haben: q-werte vor wänden gering, steeren von wänden weg, fahren widely geradeaus... 
-gym\_test klasse existiert, models klappen damit
-mit dem aktuellem reward lernt er das vor der kurve bremsen besser ist als fahren, wie man sieht wenn man davor steht
-dass die rewardfunktion vom ddpgpaper nicht klappt



\section{Discretizing actions}

\textit{How different models perform in comparison, and specifically if discretizing the action-space impairs performance}

-plot von ddpg vs dqn

\begin{figure}[h!]
		{%
			\setlength{\fboxsep}{0pt}%
			\setlength{\fboxrule}{1pt}%
			\fbox{\includegraphics[width=\textwidth]{performance_plots/dqn_bestresult_meinreward_average10_3500ep}}%
		}%
	\centering
	\caption[Exemplary performance of the dqn\_novison\_rl\_agent]{Exemplary performance of the dqn\_novison\_rl\_agent. Plots are smoothed by averaging over 10 episodes.}
	\label{fig:dqn_result}
\end{figure}


\begin{figure}[h!]
{%
	\setlength{\fboxsep}{0pt}%
	\setlength{\fboxrule}{1pt}%
	\fbox{\includegraphics[width=\textwidth]{performance_plots/ddpg_bestresult_meinreward_average10_1400ep}}%
}%
\centering
\caption[Exemplary performance of the ddpg\_novison\_rl\_agent]{Exemplary performance of the ddpg\_novison\_rl\_agent. Plots are smoothed by averaging over 10 episodes.}
\label{fig:ddpg_result}
\end{figure}


\section{Incorporating pretraining}
\textit{How to incorporate pretraining into reinforcedly learning agents}

ddpg-result: ...nope.
-dass, wenn man actiona slinput gibt, er nur die lernt :o
-dass supervised pretraining nicht übernommen werden kann wegen exorbitanter q-werte
-dass auch q-pretraining nicht funktioniert ohne meinen twist
-dass die ddpg-gaussiandistance-pretraining function nicht funktioniert
-plots von der performance nur pretraining, performance nur ohne, performance nach pretraining einbauen
-dass dqn-models auch leider nicht als base für ddpg taugen, weil sie halt nen output für jede action haben

\begin{figure}[h]
	{%
		\setlength{\fboxsep}{0pt}%
		\setlength{\fboxrule}{1pt}%
		\fbox{\includegraphics[width=\textwidth]{performance_plots/ddpg_meinward_average5_1900eps_2500pretrainepisodesauf17datasets}}%
	}%
	\centering
	\caption[Exemplary performance of the ddpg\_novison\_rl\_agent after 40000 pretrain episodes]{Exemplary performance of the ddpg\_novison\_rl\_agent after 40000 pretraining episodes. Plots are smoothed by averaging over 5 episodes.}
	\label{fig:ddpg_incorpPre}
\end{figure}

\section{Reward function}

\textit{What a good reward function looks like, that rewards the \textit{correct} behaviour at all times (including braking)}
-meine reward-funktion die bremsen belohnt

\begin{figure}[h]
	{%
		\setlength{\fboxsep}{0pt}%
		\setlength{\fboxrule}{1pt}%
		\fbox{\includegraphics[width=\textwidth]{performance_plots/dqn_rewardinrelationtowalldist_smooth1_900ep}}%
	}%
	\centering
	\caption[Exemplary performance of the dqn\_novison\_rl\_agent with SpeedInRelationToWallDist as only reward.]{Exemplary performance of the dqn\_novison\_rl\_agent with SpeedInRelationToWallDist as only reward. Plots are smoothed over 10 episodes.}
	\label{fig:dqnrewardinrelationtowall}
\end{figure}


\begin{figure}[h]
	{%
		\setlength{\fboxsep}{0pt}%
		\setlength{\fboxrule}{1pt}%
		\fbox{\includegraphics[width=\textwidth]{performance_plots/dqn_rewardspeed_smooth10_2600ep}}%
	}%
	\centering
	\caption[Exemplary performance of the dqn\_novison\_rl\_agent with the reward function from \cite{lillicrap_continuous_2015}]{Exemplary performance of the dqn\_novison\_rl\_agent with the reward function from \cite{lillicrap_continuous_2015}. Plot is not averaged.}
	\label{fig:dqnrewardspeedstuff}
\end{figure}



\chapter{Discussion and future directions}

\section{Platform}

-dass sich herausgestellt hat das simultaneous inference and learning a lot slower ist!
-die performance vom server
-multiple unity instances (feier-text)
-unity zeit verdoppeln
-feed dict performance and getting rid of it https://github.com/tensorflow/tensorflow/issues/2919
-dank sockets können agent und env easypeasy auf verschiedenen maschinen sein
-wie praktisch das mit dem info senden ist, dass das mit torcs-env-accessen nicht so einfach wäre
-wie viel arbeit die socketkommunikation war und wie überraschend effizient die ist
%scalability: mit minimal arbeit könnte man mehrere unity-instanzen haben die sich am reclistenport anmelden, an nen eigenen receiverthread weitergeleitet werden, eigenen inputval haben um das bottleneck instanzen erzeugen zu umgehen
%den anderen emma-text wie man weitere instanzen hinzufügen kann
-dass es zur beschleuniung möglich wäre mehrere aggregatoren zu haben, die immer die recht aktuellste policy haben und mit unity interagieren und das nem learn-thread schicken, auf verschiedenen Maschinen

\section{Agents}

-da qualitative änderungen sich literally immer erst nach tagen bemerkbar machen war paramter-tuning EXTREM langwierig und ist noch nicht done
-was die nächsten schritte sind, was ich erwarte und warum. Klar machen was save ist und was vermutugn.
-it is obvious, that DQN cannot be as good as DDPG - (wheel must be turned xy to avoid the obstacle because either note enough movement (z < xy) or course of dimensionality)
-die neuesten openAi algorithmen...!

-man kann a3c nutzen statt dqn
-wie schlecht die anderen eigentlcih funktioniert haben so "fleigt aus der ersten kurve, klappt bei konstant 1.8kmh speed"... meins fährt gut bei max-speed, und fährt hin und wieder ganze runden I mean thats good!!

-dass man mit dem SV-agent testen kann ob dataset separable, praktischerweise kann das supervisednet ja die gleiche struktur haben wie das dqn (gucken kann ob dieseundjene state-definition sinnvoll ist)

-dass er definitiv lernt vor ner wand zu bremsen, dank der reward function, war nen langer weg bis dahin.

-dass actions als input (beosnder bei pretraining) sehr schlecht ist da der dann lernt immer die vorherige zu machen

Die paper die bei openAI erwähnt waren die das ganze in viel kreasser sind, die Dota spielen können etc!
dass ich gerne hätte dass das am anfang der policy folgt und am ende weiter exploriert, damit es auchdie letzte kurve lernt... genau dafür ware halt intrinsic motivation und prioritized experience replay

\chapter{Conclusion}


[wennde testen wilslt ob dataset seperable ist, guckste in sv\_agent, wennde wissen willst ob das model funktioniert, guckste in gym\_text]


-wie gut dieses framework mittlerweile funktioniert, und was für ein langer weg das dahin war (dass das definitiv was war das ne ganze arbeit füllt, aber zukünftige arbeiten jetzt super gut darauf gehen, weil wegen bam framework.)
-das das framework definitiv comparable zu dem given in den beiden torcs-papern (BEIDE NOCHMAL ZITIEREN!!) ist.
-dass ich kein anderes paper gesehen habe dass sinvolle sachen macht... das virutal to real macht 9 discrete actions, das ddpg-torcs-ding macht "sometimes useful pocicies", ...

Normally when dealing with self-driving cars, there are countless additional issues, each making up a whole new challenge on their own, like wheather conditions (snow, rain, fog), pedestrians, other cars, reflections, merging into ongoing traffic, ...
we make it easier here.

"Fragestellung aus der Einleitung wird erneut aufgegriffen und die Arbeitsschritte
werden resümiert"
Zusammen mit der Conclusion 10\% der Gesamtlänge



