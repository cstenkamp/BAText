\chapter{Related work}

\label{ch:relatedwork}
%----------------------------------------------------------------------------------------

The aim of this thesis is to build a good agent for simulated self-driving cars. While this constitutes one task, it requires another task, namely to transform a specific game into a reinforcement learning problem. To show how such environments usually look, the first seciton of this chapter elaborates on related work of that domain. The second section will deal with related work in the domain of self-driving cars, starting with real-life scenarios before leading over to algorithms for comparable simulations. The latter of those has a special focus on the used functions for exploration, reward and observation to serve as basis for comparison with the agent developed in the course of this thesis.

\section{Reinforcement learning frameworks} \label{ch:rlframeworks}

The general structure of a reinforcement learning problem as a \textbf{POMDP} was outlined in the previous chapter. The main interaction between agent and environment is depicted in figure~\ref{fig:agentenvironment}.
\begin{figure}[h]
	\centering %https://tex.stackexchange.com/questions/57958/how-to-position-the-labels-of-the-path-in-automata
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,
	semithick, scale = .8, transform shape]
	\node[punkt] (env) {Environment};
	\node[punkt, inner sep=5pt, right=of env] (agent) {Agent};
	\path[->] (agent.north) edge[bend right=35] node[above]{action} (env.north);
	\path[->] (env.south) edge[bend left=-20] node[below]{reward} (agent.south);
	\path[->] (env.south) edge[bend left=-65] node[below]{observation} (agent.south);
	\end{tikzpicture}
	\caption{Interaction between agent and environment in RL}
	\label{fig:agentenvironment}
\end{figure}

\subsection{openAI gym}

When developing RL agents, agent and environment must allow for a dataflow as described in the above figure. In the original Deep-Q-Network \cite{mnih_playing_2013} as well as in its follow-ups \cite{van_hasselt_deep_2015, wang_dueling_2015}, the agents were trained on several ATARI games using the \keyword{Arcade Learning Environment} (\textbf{ALE}) \cite{bellemare_arcade_2012}. This environment converts the ATARI-games into partially observed reinforcement learning problems, and therefore providing a simple common interface to over a hundred different tasks. Doing that, it provided the accumulated score so far (corresponding to the reward), the information whether game ended (indicating the end of a training episode), as well as a $160 \times 210$  2D array of 7-bit pixels (corresponding to the agent's observation). As the game screen does not correspond to the internal state of the simulator, the ALE corresponds to a POMDP. Environments with discrete actions only are however severely limited, and most of the interesting real-world applications, as for example autonomous driving, require real-valued action spaces. The test scenarios for the Deep-DPG algorithm consisted thus of a number of simulated physics-tasks, using the \textit{MuJoCo physics environment}. 

Both of the aforementioned environments are by now, among many others, merged into the \keyword{OpenAI gym}\footnote{\url{https://gym.openai.com}} environment \cite{brockman_openai_2016}, a toolkit helping reinforcement learning research by including a collection of benchmark problems with a common interface\footnote{Aside the previously mentioned tasks, the openAI gym contains for example the boardgame Go, two-dimensional continous control tasks (\keyword{Box2D games}), a 3D-shooter by the name of \keyword{Doom}, and several other tasks, varying in complexity, input and output.}.

The goal of openAI gym is to be as convenient and accessible as possible. For that, one of their design decisions was to make a clear cut between agent and environment, only the latter of which is provided by OpenAI. The exemplary sourcecode found in algorithm \ref{alg:gym}, taken from \url{https://gym.openai.com/docs}, outlines the ease of creating an agent working in the gym framework.
\begin{algorithm}[h]
\lstset{
	numberblanklines=false
	,breaklines=true%
	,tabsize=1%
	,showstringspaces=false%
	,postbreak=\ding{229}\space%
	,escapeinside={*(}{*)}
}
\begin{lstlisting}[style=Python]
import gym
env = gym.make('CartPole-v0')
for i_episode in range(20): *($\label{algline:gym_episode}$*)
	observation = env.reset()
	for t in range(100):
		env.render()
		print(observation)
		action = env.action_space.sample()
		observation, reward, done, info = env.step(action) *($\label{algline:gym_envstep}$*)
		if done:
			print("Episode finished after {} timesteps".format(t+1))
			break
\end{lstlisting}%
\caption{Interaction with the openAI gym environment}
\label{alg:gym}
\end{algorithm}


The code outlines how the general dataflow between agent and environment usually takes place: After a reset, the environment provides the first \keyword{observation} to the agent. Afterwards, it is the agent's turn to provide an action. Even though not featured in this easy example, the action is performed by usage of the observation. Once an agent has calculated the action and provided it to the environment, it can perform another simulation step, with the tuple $\langle observation, reward, done, info\rangle$, corresponding to $s_{t+1}$, $r_{t+1}$ and $t+1==t_t$ from section~\ref{sec:qlearn} in addition to debug-information $info$. In the remainder of this work, I will refer to this dataflow as a baseline on how the interaction of environment and agent should look like.

\subsection{TORCS}

TORCS is short for \keyword{The Open Source Race Car Simulator}\cite{wymann_torcs_2013, wymann_torcs_2015}. It is a multi-agent car simulator, used as research platform for general AI and machine learning. The implementation is open source and provides an easy way to add artificial driving agents as components of the game and assess their performance. 

Ontop of TORCS, several APIs exist to provide a common interface for agents, such that they can communicate with the game while being in a separate thread, even for agents programmed in other programming languages. TORCS is also incorporated in openAI's gym platform, even though accessing it from there is a non-trivial task.\footnote{The GitHub-repository \url{https://github.com/ahoereth/ddpg} provides an instruction how to install and use the TORCS-environment in python using openAI gym.}. 

Another approach is given for the \keyword{Simulated Car Racing Championship Competition}, as presented in their manual\cite{loiacono_simulated_2013}. In that framework, agents communicate over a UDP-server with the game-environment. To do so, the game functions as a server, that sends observations of the current game-state to connected agents in an interval of $20ms$. Further, it provides an abstract \keyword{BaseDriver} class. Agents that extend this class by implementing the methods to \keyword{init} and \keyword{drive} can thus communicate as a client with the TORCS-environment, receiving an observation of the game-state and sending their action using a UDP-connection. This approach creates a physical sepearation between game engine and agents, which can thus even run over remote machines. To develop such an agent, no knowledge of the TORCS engine or the game-internal data is necessary. In this thesis, a very similar approach will be taken, where game and agents run in different threads and communicate over sockets. 

Further, the game data streamed by this environment is much sparser than what is used in the approach developed in the course of this thesis. It is however worth noting that much of the game data that is streamed from game to agent overlaps with it. The discussed manual \cite{loiacono_simulated_2013} contains a table providing detailed overview of the vectors sent to an agent (denoted \keyword{sensors}).

\section{Self-driving cars}

As mentioned in the introduction, the overall driving problem can be split into many subcomponents, not all of which are relevant for this thesis. For example, while assessing the \keyword{driver's state} is necessary in semi-autonomous vehicles, the used approach does not consider a driver. 

There is a lot of progress currently being made in the realm of \keyword{scene detection and scene understanding}. While many of these approaches utilize many recent advances of machine learning and artificial neural networks, giving an overview of those would be far beyond the scope of this thesis.

As mentioned in chapter~\ref{ch:RL}, reinforcement learning algorithms are used when the transition dynamics of the environment is unknown. If complete knowledge of the racing problem was given, optimal control motion-planning algorithms could be used to solve the problem of movement planning. An example of an asymptocially optimal algorithm that does so is the sampling-based \keyword{RRT$^*$} algorithm, short for \keyword{rapidly-exploring random trees}. In \cite{hwan_jeon_anytime_2011}, the authors use this algorithm to generate optimal motion policies, given complete knowledge of the physics and concrete starting conditions. They use the motion planning method to generate optimal trajectories for minimum-time maneuvering of high-speed vehicles, finding the fastest policy that drives without any collisions. In contrast to previous optimal control methods, their system runs in real-time, given enough computing performance. In that paper, they give a clear definition of the motion-planning problem, evaluting the algorithm with the cost function of minimal time to complete the maneuver, showing that aggressive skidding maneuvers (\textit{trail-braking}) as performed by humans are actually optimal in situations of loose surface with low friction.

\subsection{Supervised learning}

Knowing the full underlying physics is however nearly impossible, and even if it was known, optimal control is computationally very complex and unlikely to be incorporated in actual driving agents. Therefore, it is interesting to focus on the overall racing strategy in an \keyword{end-to-end} fashion, combining trajectory planning, robust control and tactical decisions into one module. Further, it makes sense to learn the problem automatically, without the need of hand-crafting a solution for every imaginable situation. The idea of these end-to-end approaches is to automatically learn internal representations of road features, optimizing all processing steps simultaneously. The hope is that the learned features are better representations than hand-crafted criteria like lane-detection, used for the ease of human interpretation. In end-to-end approaches using neural networks, no clear differentiation between feature extraction and controlling can be made as the semantics of the individual network layers remain largely unknown.

One of the first approaches to learn how to drive using an end-to-end neural network is the \keyword{Autonomous Land Vehicle In a Neural Network}, short \textsc{Alvinn}\cite{pomerleau_alvinn:_1989}. Published as early as 1989, it uses a three-layer neural network to directly learn steering commands. The input to the network consists of a front-facing $30\times32$ pixel gray-scale camera as well as a matrix of $8\times32$ values from a laser range finder. The steering-output it produces (it does not learn acceleration or braking) is discretized into a smoothed one-hot vector of 45 units. After training with artificially generated data, it learned with an accuracy of 90\% in simulations. In a real testing, it drove a real car for 400 meters at a speed of $1.8 km/h$. 

% TODO oben gegenwart, unten vergangenheit, wtf!!
A modernized version, doing essentially the same thing with modern techniques and far more computing power is NVIDIA's \keyword{End to End Learning for Self-Driving Cars}\cite{bojarski_end_2016}. In this approach, they used a convolutional neural network producing direct steering commands from a single front-facing camera. For that, a labelled dataset of 72 hours of real driving data was collected to train a 9-layer convolutional network (1 normalization layer, 5 convolutional layer, 3 dense layers), producing the steering command as a single output neuron (hence continuous, but again no throttles or brake). The network was trained supervisedly, minimizing the mean-squared error between the output and the command of the human driver saved in the dataset. To remove bias towards driving straight, the training data included a higher proprotion of frames representing curves. 
The performance of the resulting network was tested in a simulation that presented testing data to the network, comparing the produced steering to the real driving command. If the simulated car drove too far off-road, an intervention was simulated setting the virtual car back to the ground truth of the corresponding frame of the original dataset. In this testing, the simulated car had statistically two interventions per ten minutes of driving. It is worth noting, that creating a huge labelled dataset postulates no problem in modern times anymore\footnote{\keyword{Tesla} for example generates thousands of hours of driving data each day: \url{https://qz.com/694520/tesla-has-780-million-miles-of-driving-data-and-adds-another-million-every-10-hours/}}.


\subsection{Reinforcement learning}

While there are many successful approaches that use supervised learning to copy manual steering commands, such approaches have severe limitations. First of all, no statement about their ability to adapt to unknown situations can be made. It is obvious, that it is next to impossible to get enough data of \textit{extreme} situations, in which for example an accident is prevented in the last milliseconds.

Further, it is impossible for a supervised network to become better than its teacher. Especially in the domain of car racing however, it can easily be seen that the ultimate goal is an agent that drives better than its human teacher.

Another factor is, that the presented end-to-end approaches learn in a \textit{short-sighted} manner, where they predict the action solely based on the current observation -- without taking into account future implications of their actions. Reinforcement learning in contrast maximizes some long-term reward, trying to predict implications to plan trajectories.

Because of these reasons, it is interesting to look at driving agents that learn through their own interactions with the environment, via the technique of reinforcement learning as described in chapter~\ref{ch:RL}.

While reinforcement learning is a promising approach for training autonomous cars, it requires a huge amount of trial and error, which is why it is reasonable to train in simulations, rather than in real-live. The presented \keyword{DQN} and \keyword{DDPG} algorithms require interaction with their environment to calculate their reward, which cannot be provided in real-life situations because of the accident risk.

There are many approaches in recent literature aiming at translating such agents to subsequently perform successfully in real-world situations, as for example \cite{you_virtual_2017}. In this paper, the authors propose a neural network that translates the image generated by a race car simulation (specifically, they use the introduced TORCS engine) into a a realistic scene with similar structure, using a network architecture known as \keyword{SegNet}\cite{badrinarayanan_segnet:_2015}. 
Furthermore, they provide a self-driving agent which uses a discrete version of the  \keyword{A3C}\cite{mnih_asynchronous_2016}-algorithm to train throttle, brake and steering-commands, discretized into nine concrete actions. While their result is worse than a supervisedly trained baseline (the dataset of which was deemed the ground truth), a successful driving policy was learned that can adapt to real world driving data.

\subsubsection{Available input-data}

In simulations, the ground truth of the car's physics can easily be taken as input to an agent. This apparently leads to a far richer set of presentations than what can be utilized in real-life. 

However, there are many successful approaches in the literature which learn solely using visual input, comparable to that of a front-facing camera. Further, today's semi-autonomous vehicles have many components that represent a diverse range of possible input. These components include, but are not limited to: Radar, Visible-light-camera, LIDAR, infrared-camera, stereo vision, GPS or Audio. 

Interesting is for example the 3D-scanning \textit{LIDAR} sensor that can produce very high-level information of the surroundings of the car\footnote{A video visualizing the data is available under \url{https://www.youtube.com/watch?v=nXlqv_k4P8Q}}. In this work \keyword{minimap-cameras}, which provide a topview of road ahead of the car (see annotations \textbf{H} and \textbf{I} of figure~\ref{fig:aidriveshot} in appendix~\ref{AppendixB}) are used. It can be argued that Segnet\cite{badrinarayanan_segnet:_2015} could be used to convert the result of the respetive sensor into a comparable input. Similar reasoning can be given for many of the other used input-data, which will be explained in section~\ref{ch:thevectors}.


\subsubsection{Related implementations}

\label{sec:relatedimplements}

There are several known implementations that perform reinforcement learning on driving simulations. A lot of them use the metioned TORCS as their environment, while also some other, more or less realistic driving simulations are incorporated. The following two tables summarize some of the known implementation. 

\colorbox{red}{Note that this table also contains non-reinforcement training agents. AM ARSCH NOTE DASS DIE EINE HIER FALSCH STEHT}

\colorbox{red}{dass der torcs-keras die ddpg-reward funktion verbessert hat
die O-U-werte vom torcs-keras-guy}

\colorbox{red}{wie wichtig der ornstein-uhlenbeck ist}

\colorbox{red}{dass man auch im video vom keras-torcs-guy sieht wie krass der shaked!}


\newcolumntype{P}[1]{>{\RaggedRight\hspace{0pt}}p{#1}}

DDPG auf torc hat übrigens im pixel-case nen sehrsehr ählnichen punktestand wie im low-dimensional case (1840 vs 1876 im best case, -393 vs -401 im average case).

\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\begin{flushleft}
	\scriptsize
	\begin{tabular}{P{1.4cm}P{1.7cm}P{2cm}P{2.3cm}P{2.3cm}P{1.8cm}}
		Project & trained on & model & input & output & optimization function \\
		\hline
		Nvidia Autopilot\tablefootnote{\url{https://github.com/SullyChen/Autopilot-TensorFlow}} & Annotated real-world data & TensorFlow-implementation of \cite{bojarski_end_2016} & vision of front-facing camera & continuous steering-command & MSE to actual steering\\
		TensorKart by Kevin Hughes\tablefootnote{\url{https://kevinhughes.ca/blog/tensor-kart}} & Mariokart 64 & Tensorflow-model similar to Nvidia Autopilot & console screen & joystick command as vector & euclidian distance to recorded action\\
	\end{tabular}
\end{flushleft}
\label{tb:svapproaches}
\caption{Supervised approaches to learn autonomous driving}
\end{table}

\begin{table}[h]
	\begin{flushleft}
		{\def\arraystretch{2}\tabcolsep=3pt
		\scriptsize
		\begin{tabular}{P{1cm}P{1.3cm}P{1.1cm}P{2.3cm}P{2.3cm}P{2.8cm}P{2.1cm}}
		Project & trained on & model & input & output & reward & performance\\
		\hline
		DDPG \cite{lillicrap_continuous_2015} & TORCS & DDPG & visual input as provided by TORCS & (throttle, brake, steer) $\subset \mathds{R}^{n \in \mathds{N}}$ & velocity along the track direction, penalty of -1 for collisions &  \multirow{2}{1.8cm}{\textit{some replicas were able to learn reasonable policies that re able to complete a circuit around the track}. \cite{lillicrap_continuous_2015}}\\
		DDPG \cite{lillicrap_continuous_2015} & TORCS & DDPG & low-dimensional, similar to \cite{loiacono_simulated_2013} & (throttle, brake, steer) $\subset \mathds{R}^{n \in \mathds{N}}$ & velocity along the track direction, penalty of -1 for collisions & \\
		DDPG-Keras-Torcs\tablefootnote{\url{https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html}} & TORCS & DDPG implemented in Keras & angle, 19 range finder sensors, distance between car and track axis, speed along x,y,z axis, wheel rotation, car engine rotation (subset of \cite{loiacono_simulated_2013}) & (throttle, brake, steer) $\subset \mathds{R}^{n \in \mathds{N}}$ & velocity along the track-direction minus velocity in transverse direction & reasonable policy after 2000 episodes\tablefootnote{see \url{https://www.youtube.com/watch?time_continue=4&v=4hoLGtnK_3U}}\\
		A3C \cite{mnih_asynchronous_2016} & \colorbox{red}{TODO}
	\end{tabular}
	}
	\end{flushleft}
\label{tb:rlapproaches}
\caption{RL approaches to learn autonomous driving}
\end{table}