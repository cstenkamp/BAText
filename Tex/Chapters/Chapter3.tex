\chapter{Related work}

\label{ch:relatedwork}
%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 

[as mentioned before, there are two levels in the thesis - on the abstract level, the aim is to build a good agent for self-driving cars. On the practical level, it is still one specific simulation of a game, and one at first has to make the given game a RL problem, before you can eben talk about trying to solve that. To show how that is normally done, there is the first section. In the second section I will then talk about successful approaches of self-driving cars in reality. That is at first subdivided into real-life and games. Our game is (like so many others) supposed to work as a bridge between reallife and games. As for that, I will also talk about what data is normally available, ...

\section{Reinforcement Learning Frameworks} \label{ch:rlframeworks}

In the previous chapter, I outlined the general structure of a reinforcement learning problem. In summary, it consists of agent and environment, where the environment is discretized into timesteps and only partially observed by the agent (\textbf{POMDP}). Each timestep, the agent gets an observation of the environment's state (making up its \keyword{internal state} and a scalar reward, and chooses an action to return to the environment. A graphical description of this interaction is depicted in figure \ref{fig:agentenvironment}.
\begin{figure}[h]
	\centering %https://tex.stackexchange.com/questions/57958/how-to-position-the-labels-of-the-path-in-automata
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,
	semithick, scale = .8, transform shape]
	\node[punkt] (env) {Environment};
	\node[punkt, inner sep=5pt, right=of env] (agent) {Agent};
	\path[->] (agent.north) edge[bend right=35] node[above]{action} (env.north);
	\path[->] (env.south) edge[bend left=-20] node[below]{reward} (agent.south);
	\path[->] (env.south) edge[bend left=-65] node[below]{observation} (agent.south);
	\end{tikzpicture}
	\caption{Interaction between agent and environment in RL}
	\label{fig:agentenvironment}
\end{figure}

\subsection{openAI gym}

When developing RL agents, it is not enough to create algorithms, but also to have a specification of agent and environment that allows for a dataflow as described above. The original Deep-Q-Network \cite{mnih_playing_2013} as well as its follow-ups \cite{van_hasselt_deep_2015}\cite{wang_dueling_2015} trained their agents on several \textsc{Atari} games using the \keyword{Arcade Learning Environment} (\textbf{ALE}) \cite{bellemare_arcade_2012}, which was developed with the intention to evaluate the generality of several AI techniques, especially general game playing, reinforcement learning and planning. The important contribution of \cite{bellemare_arcade_2012} was to provide a testbed for any kind of agent, by providing a simple common interface to more than a hundred different tasks -- converting games into reinforcement learning problems. Doing that, it provided the accumulated score so far (corresponding to the reward), the information whether game ended (indicating the end of a training episode), as well as a $160 \times 210$  2D array of 7-bit pixels (corresponding to the agent's observation). As the game screen does not correspond to the internal state of the simulator, the ALE corresponds to a POMDP. 

Environments with discrete actions only are however severly limited, and most of the interesting real-world applications, as for example autonomous driving, require however real-valued action spaces. The test scenarios for the Deep-DPG algorithm consisted thus of a number of simulated physics-tasks, using the MuJoCo physics environment. \\

\noindent Both of the above mentioned environments are by now, among many others, merged into the \keyword{OpenAI gym}\footnote{\url{https://gym.openai.com}} environment \cite{brockman_openai_2016}, a toolkit helping reinforcement learning research by including a collection of benchmark problems with a common interface. Aside the previously mentioned ALE as well as a number of physics tasks using the MuJoCo physics-engine, the openAI gym contains the boardgame Go, two-dimensional continous control tasks (\keyword{Box2D games}), a 3D-shooter by the name of \keyword{Doom}, and several other tasks, varying in complexity, input and output.

The goal of openAI gym is to be as convenient and accessible as possible. For that, one of their design decisions was to make a clear cut between agent and environment, only the latter of which is provided by OpenAI. The exemplary sourcecode found in algorithm \ref{alg:gym}, taken from \url{https://gym.openai.com/docs}, outlines the ease of creating an agent working in the gym framework.
\begin{algorithm}[h]
\lstset{
	numberblanklines=false
	,breaklines=true%
	,tabsize=1%
	,showstringspaces=false%
	,postbreak=\ding{229}\space%
	,escapeinside={*(}{*)}
}
\begin{lstlisting}[language=Python,frame=none]
import gym
env = gym.make('CartPole-v0')
for i_episode in range(20): *($\label{algline:gym_episode}$*)
	observation = env.reset()
	for t in range(100):
		env.render()
		print(observation)
		action = env.action_space.sample()
		observation, reward, done, info = env.step(action) *($\label{algline:gym_envstep}$*)
		if done:
			print("Episode finished after {} timesteps".format(t+1))
			break
\end{lstlisting}%
\caption{Interaction with the openAI gym environment}
\label{alg:gym}
\end{algorithm}


The code outlines how the general dataflow between agent and environment usually takes place: After a reset, the environment provides the first \keyword{observation} to the agent. Afterwards, it is the agent's turn to provide an action. Even though not featured in this easy example, the action is performed by usage of the observation. Once an agent has calculated the action and provided it to the environment, it can perform another simulation step, returning $\langle observation, reward, done, info\rangle$, which is a tuple consisting of another observation of the environment's state, a scalar reward, the information if the episode is finished and it is time to reset the environment, as well as some information for debugging, typically not used in the final description of an agent. In the remainder of this work, I will refer to this dataflow as a baseline on how the interaction of environment and agent should look like.\\

\colorbox{red}{It is worth noting, that the openAI gym is not even}


\subsection{TORCS}

TORCS is short for \keyword{The Open Source Race Car Simulator}\cite{wymann_torcs_2013, wymann_torcs_2015}. It is a multi-agent car simulator, used as research platform for general AI and machine learning. The implementation is open source and provides an easy way to add artificial driving agents as components of the game and assess their performance. 

Ontop of TORCS, several APIs exist to provide a common interface for agents, such that they can communicate with the game while being in a separate thread, even for agents programmed in other programming languages. TORCS is also incorporated in openAI's gym platform, even though accessing it from there provides a challenge on its own\footnote{This GitHub-repository (\url{https://github.com/ahoereth/ddpg}), implemented by a fellow student from the author of this work, provides an instruction how to install and use the TORCS-environment in python using openAI gym.}. 

Another approach is given for the \term{Simulated Car Racing Championship Competition}, as presented in their manual\cite{loiacono_simulated_2013}. In that framework, agents communicate over a UDP-server with the game-environment. To do so, the game functions as a server, that in an interval of $20ms$ sends an observation of the current game-state to connected agents. Further, it provides an abstract \keyword{BaseDriver} class. Agents that extend this class by implementing the methods to \keyword{init} and \keyword{drive} can thus communicate as a client with the TORCS-environment, receiving an observation of the game-state and sending their action using a UDP-connection. This approach creates a physical sepearation between game engine and agents, which can thus even run over remote machines. To develop such an agent, no knowledge of the TORCS engine or the game-internal data is necessary. 

In this thesis, a very similar approach will be taken, where game and agents run in different threads and communicate over sockets. A notable difference to the presented approach is however, that their implementation saw agents as clients, such that multiple agents can simultanously connect to the same game engine. In the developed approach here however, the intention is to use possibly multiple copies of an environment to train one agent, such that the focus lies on the agent, which will thus make up the server. 

Further, the game data streamed by this environment is much sparser than what is used in the approach developed in the course of this thesis. It is however worth noting that much of the game data that is streamed from game to agent overlaps with it. The discussed manual \cite{loiacono_simulated_2013} contains a table providing detailed overview of the vectors sent to an agent (\keyword{sensors}).

\section{Self-driving cars}

As mentioned in the introduction, the overall driving problem can be split into many subcomponents, not all of which are relevant for this thesis. For example, while assessing the divers state is necessary in semi-autonomous vehicles, the used approach does not consider a driver. 

There is a lot of progress currently being made in the realm of scene detection and scene understanding. While many of these approaches utilize many recent advances of machine learning and artificial neural networks, giving an overview of those would be far beyond the scope of this thesis.

As mentioned in chapter~\ref{ch:RL}, reinforcement learning algorithms are used when the transition dynamics of the environment are unkown. If complete knowledge of the racing problem was given, optimal control motion-planning algorithms could be used to solve the problem of movement planning. An example of an asymptocially optimal algorithm that does so is the sampling-based \term{RRT$^*$} algorithm, short for rapidly-exploring random trees. In \cite{hwan_jeon_anytime_2011}, the authors use this algorithm to generate optimal motion policies, given complete knowledge of the physics and concrete starting conditions. They use the motion planning mehtod to generate optimal trajectories for minimum-time maneuvering of high-speed vehicles, finding the fastest policy that drives without any collisions. In contrast to previous optimal control methods, their system runs in real-time, given enough computing performance. In that paper, they give a clear definition of the motion-planning problem, evaluting the algorithm with the cost function of minimal time to complete the maneuver, showing that aggressive skidding maneuvers (\textit{trail-braking}) as performed by humans are actually optimal in situations of loose surface with low friction.

\subsection{Supervised learning}

Knowing the full underlying physics is however nearly impossible, and even if it was known, optimal control is computationally very complex and unlikely to be incorporated in acutal driving agents. Therefore, it is interesting to focus on the overall racing strategy in an end-to-end fashion, combining trajectory planning, robust control and tactical decisions into one module. Further, it makes sense to learn the problem automatically, without the need of hand-crafting a solution for every imaginable situation. The idea of these end-to-end approaches is to automatically learn internal representations of road features, optimizing all processing steps simultaneously. The hope is that the learned features are better representations than hand-crafted criteria like lane-detection, used for the ease of human interpretation. In end-to-end approaches using neural networks however, no clear differentiation between feature extraction and controlling can be made as the semantics of the individual network layers remain largely unkown.

One of the first approaches to learn how to drive using an end-to-end neural network is the \keyword{Autonomous Land Vehicle In a Neural Network}, short \textsc{Alvinn}\cite{pomerleau_alvinn:_1989}. Published as early as 1989, it used a three-layer neural network to directly learn steering commands. The input to the network consisted of a front-facing $30\times32$ pixel gray-scale camera as well as a matrix of $8\times32$ values form a laser range finder. The steering-output it produces (it did not learn acceleration or brakes) is discretized into a smoothed one-hot vector of 45 units. The data used as input was artificially generated, it learned with a (self-defined) accuracy of 90\% in simulations. In a real testing, it drove a real car for 400 meters at a speed of $1.8 kph$. 

A modernized version, doing essentially the same thing with modern techniques and far more computing power is NVIDIA's \keyword{End to End Learning for Self-Driving Cars}\cite{bojarski_end_2016}. In this approach, they used a convolutional neural network producing direct steering commands from a single front-facing camera. For that, a labelled dataset of 72 hours of real driving data was collected to train a 9-layer convolutional network (1 normalization layer, 5 convolutional layer, 3 dense layers) on, producing the steering command as a single output neuron (hence continuous, but again no steering or brake). The network was trained supervisedly, minimizing the mean-squared error between the output and the command of the human driver saved in the dataset. To remove bias towards driving straight, the training data included a higher proprotion of frames representing curves. 
The performance of the resulting network was tested in a simulation that presented testing data to the network, comparing the produced steering to the real driving command. If the simulated car drove too far off-road, an intervention was simulated setting the virtual car back to the ground truth of the corresponding frame of the original dataset. In this testing, the simulated car had statistically two interventions per ten minutes of driving. It is worth noting, that creating a huge labelled dataset postulates no problem in modern times anymore\footnote{\keyword{Tesla} for example generates thousands of hours of driving data each day: \url{https://qz.com/694520/tesla-has-780-million-miles-of-driving-data-and-adds-another-million-every-10-hours/}}


\subsection{Reinforcement learning}

While there are many successful approaches that use supervised learning to copy manual steering commands, such approaches have severe limitations. First of all, no statement about their ability to adapt to unknown situations can be made. It is obvious, that it is next to impossible to get enough data of \textit{extreme} situations, in which for example an accident is prevented in the last milliseconds.

Further, it is impossible for a supervised network to become better than its teacher. Especially in the domain of car racing racing however, it can easily be seen that the ultimate goal is an agent that drives better than its human teacher.

Another factor is, that the presented end-to-end approaches learn in a \textit{short-sighted} manner, where they predict the action solely based on the current observation -- without taking into account future implications of their actions. Reinforcement learning in contrast maximizes some log-term reward, trying to predict implications to plan trajectories.

Because of these reasons, it is interesting to look at driving agents that learn through their own interactions with the environment, via the technique of reinforcement learning as described in chapter~\ref{ch:RL}.

While reinforcement learning is a promising approach for training autonomous cars, it requires a huge amount of trial and error, which is why it is reasonable to train in simulations, rather than in real-live. The presented \keyword{DQN} and \keyword{DDPG} algorithms require interaction with their environment to calculate their reward, which cannot be provided in real-life situations because of the accident risk.

There are many approaches in recent literature aiming at translating such agents to subsequently perform successfully in real-world situations, as for example \cite{you_virtual_2017}. In this paper, the authors propose a neural network that translates the image generated by a race gar simulation (specifically, they use the introduced TORCS engine) into a a realistic scene with similar structure, using a network architecture known as \keyword{SegNet}\colorbox{red}{https://arxiv.org/pdf/1511.00561.pdf}}. 
Furthermore, they provide a self-driving agent which uses a discrete version of the  \keyword{A3C}\cite{mnih_asynchronous_2016}-algorithm to train throttle, brake and steering-commands, discretized into nine concrete actions. While their result is worse than a supervisedly trained baseline (the dataset of which deemed as ground truh), a successful driving policy was learned that can adapt to real world driving data.

\subsubsection{Available input-data}

In simulations, the ground truth of the car's physics can easily be taken as input to an agent. This apparently leads to a far richer set of presentations than what can be utilized in real-life. 

However, first of all, there are many successful approaches in the literature which learn solely using visual input, comparable to that of a front-facing camera. Further, today's semi-autonomous vehicles have many components that represent a diverse range of possible input. These compoents include, but are not limited to: Radar, Visible-light-camera, LIDAR, infrared-camera, stereo vision, GPS or Audio. 

Interesting is for example the \textit{LIDAR} sensor, \colorbox{red}{ultrakurzbeschreibung} that can produce very high-level information of the surroundings of the car\footnote{A video visualizing the data is available under \url{https://www.youtube.com/watch?v=nXlqv_k4P8Q}}. In this work \keyword{minimap-cameras}, which provide a topview of road ahead of the car (see annotations \textbf{H} and \textbf{I} of figure~\ref{fig:aidriveshot} in appendix~\ref{AppendixB}) are used. It can be argued that Segnet\colorbox{red}{https://arxiv.org/pdf/1511.00561.pdf}} could be used to convert the result of the respetive sensor into a comparable input. Similar reasoning can be given for many of the other used input-data, which will be explained in section~\ref{ch:thevectors}.


\subsubsection{Related implementations}

There are several known implementations that perform reinforcement learning on driving simulations. A lot of them use the metioned TORCS as their environment, while also some other, more or less realistic driving simulations are incorporated. The following table will summarize some of the known implementation. Note that this table also contains non-reinforcement training agents.

\colorbox{red}{dass der torcs-keras die ddpg-reward funktion verbessert hat
die O-U-werte vom torcs-keras-guy}

\colorbox{red}{wie wichtig der ornstein-uhlenbeck ist}

\colorbox{red}{dass man auch im video vom keras-torcs-guy sieht wie krass der shaked!}


\newcolumntype{P}[1]{>{\RaggedRight\hspace{0pt}}p{#1}}

DDPG auf torc hat übrigens im pixel-case nen sehrsehr ählnichen punktestand wie im low-dimensional case (1840 vs 1876 im best case, -393 vs -401 im average case).

\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\begin{flushleft}
	\scriptsize
	\begin{tabular}{P{1.4cm}P{1.7cm}P{2cm}P{2.3cm}P{2.3cm}P{1.8cm}}
		Project & trained on & model & input & output & optimization function \\
		\hline
		Nvidia Autopilot\tablefootnote{\url{https://github.com/SullyChen/Autopilot-TensorFlow}} & Annotated real-world data & TensorFlow-implementation of \cite{bojarski_end_2016} & vision of front-facing camera & continuous steering-command & MSE to actual steering\\
		TensorKart by Kevin Hughes\tablefootnote{\url{https://kevinhughes.ca/blog/tensor-kart}} & Mariokart 64 & Tensorflow-model similar to Nvidia Autopilot & console screen & joystick command as vector & euclidian distance to recorded action\\
	\end{tabular}
\end{flushleft}
\label{tb:svapproaches}
\caption{Supervised approaches to learn autonomous driving}
\end{table}

\begin{table}[h]
	\begin{flushleft}
		{\def\arraystretch{2}\tabcolsep=3pt
		\scriptsize
		\begin{tabular}{P{1cm}P{1.3cm}P{1.1cm}P{2.3cm}P{2.3cm}P{2.8cm}P{2.1cm}}
		Project & trained on & model & input & output & reward & performance\\
		\hline
		DDPG \cite{lillicrap_continuous_2015} & TORCS & DDPG & visual input as provided by TORCS & (throttle, brake, steer) $\subset \mathds{R}^{n \in \mathds{N}}$ & velocity along the track direction, penalty of -1 for collisions &  \multirow{2}{1.8cm}{\textit{some replicas were able to learn reasonable policies that re able to complete a circuit around the track}. \cite{lillicrap_continuous_2015}}\\
		DDPG \cite{lillicrap_continuous_2015} & TORCS & DDPG & low-dimensional, similar to \cite{loiacono_simulated_2013} & (throttle, brake, steer) $\subset \mathds{R}^{n \in \mathds{N}}$ & velocity along the track direction, penalty of -1 for collisions & \\
		DDPG-Keras-Torcs\tablefootnote{\url{https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html}} & TORCS & DDPG implemented in Keras & angle, 19 range finder sensors, distance between car and track axis, speed along x,y,z axis, wheel rotation, car engine rotation (subset of \cite{loiacono_simulated_2013}) & (throttle, brake, steer) $\subset \mathds{R}^{n \in \mathds{N}}$ & velocity along the track-direction minus velocity in transverse direction & reasonable policy after 2000 episodes\tablefootnote{see \url{https://www.youtube.com/watch?time_continue=4&v=4hoLGtnK_3U}}\\
		A3C \cite{mnih_asynchronous_2016} & \colorbox{red}{TODO}
	\end{tabular}
	}
	\end{flushleft}
\label{tb:rlapproaches}
\caption{RL approaches to learn autonomous driving}
\end{table}