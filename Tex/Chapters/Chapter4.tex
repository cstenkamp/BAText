\chapter{Program Architecture}

\label{ch:program}

\newcommand{\term}[1] {{\spaceskip=.95\fontdimen2\font minus \fontdimen4\font
	\xspaceskip=0pt\relax \large\texttt{#1}}}

\renewcommand{\inlinecode}[1]{\colorbox{red}{\lstinline[basicstyle=\ttfamily\color{black}]{#1}}}


\newcommand{\codefunc}[1]{\colorbox{evenmorelightgray}{\lstinline[basicstyle=\ttfamily\color{black},keywordstyle=\ttfamily]{#1}}}

\newcommand{\codeobj}[1]{\colorbox{evenmorelightgray}{{\spaceskip=.95\fontdimen2\font minus \fontdimen4\font	\xspaceskip=0pt\relax \large\texttt{#1}}}}

\newcommand{\codeother}[1]{\colorbox{evenmorelightgray}{\lstinline[basicstyle=\ttfamily\color{black},keywordstyle=\ttfamily]{#1}}}


\newcommand{\filename}[1] {{\spaceskip=.95\fontdimen2\font minus \fontdimen4\font
		\xspaceskip=0pt\relax \large\texttt{#1}}}


The aim of this thesis is to convert a given racing game into an environment that can be played by reinforcement learning agents and to analyze the performance of different agents. In this chapter, I will explain the implementation that was developed throughout this process. I will start by explaining the design decisions that shaped this implementation in section~\ref{ch:projectcharacteristicschap}. In the following section (\ref{ch:implementation}) I will explain the particular source code, starting with game as it was given as starting point, most parts of which are not implemented by the author of this thesis. Then the implemented extensions of the game are explained, before leading over to the agent. 

\section{Characteristics and design decisions}

\label{ch:projectcharacteristicschap}

As stated in chapter~\ref{ch:rlframeworks}, the usual framework for solving reinforcement learning tasks is fairly rigid and has to be adjusted for the task in this work. The differences causing this will be discussed in this section, alongside further design decisions and challenges with their respective solution. For that, I will explain general principles in the first section of this chapter, and go into detail about some of the specific details about the implementation in the subsequent section.

\subsection{Characteristics of this project} \label{ch:projectcharacteristics}

A difference of this implementation to the agents presented in chapter~\ref{ch:RL} is, that those are developed as general-purpose problem solvers, with the intention to solve any arbitrary task given to them. In this approach, that is not the case -- the goal is to solve the specific, given game. This allows in principle to incorporate expert knowledge of the task domain, to for example forbid certain combinations of the output or to use particular types of exploration, which are specifically useful in this scenario. 

This thesis' game is a racing game, which has implications in several domains, for example making the standard $\epsilon$-greedy approach for exploration much worse as in many other domains. Next to that the game is, in contrast to games solved in openAI's gym-environment (\ref{ch:rlframeworks}), live. While the game could in theory be manually paused for every RL step, it provides advantages to let the agent run ``live'', such that it runs fluently and its progress can manually be inspected. Another challenge was the fact, that the game is coded in a programming language for which no efficient deep learning architectures exist, which led to the necessity of a propietary communication between the environment and its agent.

The fact that there is a game that is supposed to be \textit{solved} also extended the entire problem domain: While usually the focus of developing RL agents can lie purely on the agent, assuming that the environment and consequently its definition of state/observation and reward is fixed, for the focus of this work it is also necessary to test what a useful definition of observation or reward looks like. Many of the subsequent design decisions are made with the idea in mind, that it must be as easy as possible to compare different agents using a unique combination of \keyword{observation}-definition, \keyword{reward}-definition, \keyword{model} and \keyword{exploration technique}. 

Furthermore, the question of how important supervised pretraining is will be addressed in this thesis. For that, the game must provide a way of recording manual actions and their corresponding observation and to record that in a way, that a learner can read it and train on this data. Doing so allows to compare agents relying purely on supervised training to reinforcement-learning. Further, the question arises if there is a way to combine pre-training with reinforcement training in racing tasks, as famously done in the board game \keyword{Go}\cite{silver_mastering_2016}.

%differences from mine to others with their implications
%-geht nur um 1 spiel 
%	-settozero sinnvoll (expert knowledge)
%	-andere exploration möglich
%-spiel ist live (nochmal im diagramm zeigen)
%	-dass das härter für den agent ist/er schneller ist
%	-humanTakingControl möglich, man kann sich Q-werte/rewards/.. in situationen angucken
%-verschiedene sprachen
%	-proprietäre kommunikation
%	-spiel ändert sich ebenfalls, was anderes als proprietär wäre dumm
%-human taking control ("wie sind die q-werte vor der wand",..)
%-geht um ein RENNspiel 
%	-epsilon-greedy sucks
%	-sieht tendenziell viel mehr den anfang
%- dieses EINE spiel soll möglichst gut gelernt werden
%	-geht ebenfalls darum was für vektoren sinnvoll sind (freier als torcs)
%	-geht ebenfalls darum was für reward definitionen sinnvoll sind
%	-wie viel FPS er hbaen muss
%	-agent ist für resetten verantwortlich
%	-state und reward gehen nicht fix an python, sondern agent sucht aus
%		-sowas wie annealing reward möglich
%		-man könnte sogar den reward selbst lernen lassen anhand von "mit welcher reward-funktion minimiert er nach 2millionen iterationen die rundenzeit am besten"
%	-Pretraining möglich, vergleichbarkeit mit pretraining, kombination
%
%
%Dinge die also verglichen werden
%	-reward
%	-inputs
%	-model
%	-was den state beendet
%	-exploration techniques
%	-pretraining ja/nein


\subsection{Characteristics of the game}

%at the end of this section a clear distinction between agent and environment should be known. Where does the agent stop, where does the environment begin. 

%-3 actions as input (and braking is necessary)
%-no opponents
%-realistic physics
%-slip
%-dass throttle und brake nur torques applien, was halt realistisch ist, thanks unity
%-indeterministic
%-partially observable
%-huge, non-tabular solvable, continuous state space
%-of course, at best, contiuous action space
%-goal is to finish 1 round in as short time as possible, while staying on track
%-der agent ended mit den actions - das auto ist teil der environment
%-track consists of track, off-track and curb with different friction properties

The given game is a simple racing simulation with realistic slip-behaviour. As of now, it consists of one car driving one track. While it possible to implement additional AI drivers or different tracks, no such thing is considered in the current implementation. As the physics of this game more realistic than that of all known \term{Atari}-games, this game is much harder to predict and master than what the original DQN was tested on.

The track itself is a circular course with a combination of unique curves, requiring the car to steer left as well as right. Along every point of the course, there are three different surfaces providing different friction -- the \textit{track} (inside) itself provides the most grip, while the \textit{off-track} (outside) surface is far more slippery. Between the track-surface and the off-track-surface there is the \textit{curb}, which manifests as a small bump with separate friction properties. The track, curb and off-track each have a consistent width throughout the circuit. To the outside of the off-track there is a wall that cannot be traversed. On this track, there is a car which is controlled by the agent.

It is the task of an agent to provide commands for the values of throttle, brake and steering, which are continous in their respective domains: $throttle \in [0,1]$, $brake \in [0,1]$ and $steer \in[-1,1]$. The \keyword{throttle} increases the simulated motor-torque, which leads faster rotation of the tires and thus applying forward force to the car. The \keyword{brake} simulates a continuous brake force slowing down the rotation of the tires. It is not possible to complete a lap while constantly accelerating as much as possible without braking. It is important to note that slip and spin is also simulated: While the rotation of the tires can be accelerated or decelerated fairly abrupt due to their small mass, the car itself has a higher mass and thus more intertia, with forbids abrupt changes in movement. As the tires are rigidly attached to the car, they lose grip on the street, which lessens the impact of consequent forces applied to the tires. The last command an agent must output is the \keyword{steering}, which turns the front tires of the car, leading the car applying force to the respective side the tires steered towards.

As is the general case in reinforcement learning problems, the agent does not know the implications of its actions in advance but must learn them via trial and error. Important to note is that the agent provides those actions \textit{to the car}, which must thus be seen as part of the environment. Because of the described simulation of slip and spin, an action does not have a reliable impact on the car's behaviour. Consequences of the implemented physics are for instance smaller turning circles for slower speeds, or a reduced effect for simultaneous braking and steering at high velocities -- if the inertia is high, the car's tires will slip, loose grip and the impact of the steering will vanish almost completely.

\subsubsection{The game as a reinforcement learning problem}

For a reinforcement-learning agent to successfully learn how to operate in an unknown environment, it must correspond precisely to an MDP, which is a tuple of $\langle S, A, P, R, \gamma \rangle$ (details explained in section \ref{ch:mdps}). Because in this simulation only the case of a single agent without any other cars on the track is considered, the racing problem can be formalized as a Markov Decision Processes with a similar reasoning than \citet[chapter 4]{wymann_torcs_2015} put forward for \term{TORCS}. 

As is the general case in simulations, while every update-step of the physics aims to simulate a continous process, those updates must be discretized in the temporal domain. As however both agent and environment run live, the temporal discretization of the agent can not always correspond to that of the environment if an update-step in the agent takes longer than in the environment. To put that into numbers, the fixed timestep for the environment's physics is at 500 updates per second, while the agent discretizes to maximally 20 actions per second.

As mentioned in the previous section, the action space required by the agent is continuous with $\mathcal{A} \subset \mathds{R}^3$. 

The environment's state is a linear combination of different factors, as for example the car's speed, absolute position, current slip-values and much more. While certainly finite, it consists of many high-dimensional values: $\mathcal{S}_e \subset \mathds{R}^{n \in \mathds{N}}$. Because of certain factors in the implementation of the environment itself, the environment is indeterministic\footnote{The game is programmed in Unity, which has non-predictable physics. As discussed for example in \url{https://forum.unity3d.com/threads/is-unity-physics-is-deterministic.429778/}, this is because of the fact that any random-number-generator depends on the current system time, which is never fully equal in subsequent trials. Because the calculation of some states can be more complex than others, this effect can snowball even more -- longer calculation in one step leads to later timing of a subsequent update step, which can lead to a whole other trajectory along the state space, even though the start state was equal.}. As can in principle be argued that the environment's state corresponds to all information stored in the game's section of the computer's RAM, the game trivially fulfills the markov property. As the environment is only a single-agent system, the transition function of the environments can be expressed as a stochastic function of state and action: $\mathcal{S}_e \times \mathcal{A} \rightarrow \mathcal{S}_e$. 
In contrast to many existing approaches(see section~\ref{sec:relatedimplements}), it was decided that the start-state is no distribution over all possible states, but always set to the same position, with speed and inertia set to zero. While this makes the game harder to be learned, it is more realistic and comparable to real situations.

There is no formal definition of a reward returned by the environment in the game. While it could in theory be argued that the inverse of the time needed to complete a lap could be taken as the reward, it is obvious that this is infeasible due to many reasons: Finishing one lap takes several seconds, which means there are hundreds of iterations between start state and final state. Next to the obviously arising \keyword{credit assignment problem}, the chance of an agent even getting to the finish line without crashing is practically zero without rewarding intermediate progress. Instead, it makes sense to give more \keyword{dense} rewards. As mentioned in section~\ref{ch:projectcharacteristics}, the reward is also subject of experiments in the scope of this thesis. For the game to correspond to a proper environment, this reward must be a scalar value depending on state and action.

Though it is in theory possible to implement an agent that takes the entire underlying state of the environment as its input, an approach like that is far from feasible, as this state also contains much information only necessary for rendering the game. Instead, in the chosen approach the agent only receives an $observation$ of the environment's state. 

Summarizing all those factors, it becomes obvious that the given game can clearly be defined in terms of a \keyword{Partially Observed Indeterministic Markov Decision Process}.

It is worth noting, that while the dimensionality of the observation is likely much smaller than the dimensionality of the state, any feasible observation will be high-dimensional or real-valued. The chance for any particular combination of parameters to appear multiple times is vanishingly low, which makes the use of function approximation necessary.

While the notion of \keyword{POMDPs} itself contains no final state definition, they are necessary to provice a hard limit on Q-value calculation. A design decision on this was made to let the environment only provide candidates of what could terminate an episode (e.g. a time limit or steering into a wall). As the current work wanted to test which definition of final states was most efficient, the agent decides in which states it resets the environment \\

On start-up, the game lets a user choose between three different game modes: In the \term{Drive} mode, users manually drive the car via the keyboard. The \term{Train\,AI\,supervisedly} mode records information about the manual driving, which can be used as (pre-) training data for the agents. In the \term{Drive\,AI} mode the car is controlled by an agent (until the user actively interferes). A screenshot of the start menu can be found in figure~\ref{fig:overviewshot} in appendix~\ref{AppendixB}. Note that the background of the menu provides a bird-eye view of the track.

\subsection{Characteristics of the agent}
\label{ch:agentchars}

As stated above, it was a design decision to leave as many options open to the agent as possible. To summarize from the above chapters, the features unique to every agent are:
\begin{itemize}
	\item The definition of the agent's \keyword{observation-function}, providing its internal state: $s = o(s_e)$
	\item Definition of the \keyword{reward-function}, returning a scalar from state, action and subsequent state: $\mathcal{S}_e \times \mathcal{A} \times \mathcal{S}_e \rightarrow \mathds{R}$
	\item Definition of its internal \keyword{model}, basing on which it calculates its policy
	\item Under which conditions an episode is considered to be terminated 
	\item Definition of the agent's \textit{exploration technique}
	\item If and how the agent relies on pretraining with supervised data	
\end{itemize}
\begin{flushright}
	\scriptsize
	In the following sections, I will refer to the definitions of these functions/options as the \keyword{features} of the agent. In the implementation, there are many possible features, not all of which are actually used by an agent. I will refer to those as \keyword{possible features}. When I talk specifically about the possible observations influencing the observation-function, I refer to those as \keyword{(possible) vectors}. Furthermore, the specific implementation of the agent's memory will be considered a feature. This has however no further consequences as it only differs in some agents in order to save its replay memory more efficiently.
\end{flushright}

This design decision is why differences arise between this project and the structure put forward in algorithm~\ref{alg:gym}. For example, the outer loop (line~\ref{algline:gym_episode}) becomes obsolete, as the reset conditions are decided by the agent. It additionally becomes possible to experiment with rewards changing over time, allowing e.g. first learning to move forward and only later to stay strictly on the track.

Because there are many features in which agents can differ, it makes sense to use the object-oriented programming concept of \term{inheritance} for the different agents. In such an implementation, the main methods common to every agent as well as basic definitions of the features are implemented in a superclass. Particular agents inherit from this class, while overwriting the attributes/functions for the feature in which they differ.\\

In this chapter, only the general implementation of agent and environment are described. Because the definition of the \term{features} is replaceable, with the specific implementation being one of many possibilieties, it is considered the \keyword{resulting implementation} and thus described in chapter~\ref{ch:implementationresults}

\section{Implementation}

\label{ch:implementation}

The associated programs are, as will be explicitly stated, written by the author of this work or its \leonbase, and are licensed under the GNU General Public License (GNU GPLv3). Their source codes can be found digitally on the enclosed CD-ROM as well as online:. Version control of this project relied on \term{GitHub}\footnote{\url{https://github.com/}}, and was split into three repositories: The source code of the actual game written with the game engine \term{Unity 3D} (\textit{BA-rAIce}\footnote{\url{https://github.com/cstenkamp/BA-rAIce}}), the source code of the implementation, written in \term{Python} (\textit{Ba-rAIce-ANN}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN}}), as well as the present text, written in \LaTeX ~ (\textit{BAText}\footnote{\url{https://github.com/cstenkamp/BAText}}). To ease connections between the following descriptions and their correspondenced in the actual source code, footnotes will refer as hyperlink to the files on GitHub (the relative path on the enclosed CD is equal to those on GitHub). In order to ensure that no work after the deadline is considered, it is referred to the signed commits \colorbox{red}{THE, SIGNED, COMMITS}.
%TODO: signed github commit!

The game is programmed using Unity Personal with a free license for students\footnote{\url{https://store.unity.com/products/unity-personal}}. It is tested under version \term{2017.1.0f3}\footnote{As of now, \today, there is a bug in Unity that causes it to crash due to a memory leak if UI components are updated too often (which happens after a few hours of running). Because of that, in the current release of this project, all updates of the Unity UI are disabled in AI-Mode. A bug report to Unity was filed (case \href{https://fogbugz.unity3d.com/default.asp?935432_h1bir10rkmbc658k}{935432}) on \formatdate{27}{7}{2017}, and it was promised that this issue will soon be fixed. Once that is the case, the variable \codeobj{DEBUG\_DISABLEGUI\_AIMODE} in \href{https://github.com/cstenkamp/BA-rAIce/blob/ef2dc018f36cd9ad65df90e65d8ab840c822567e/Assets/Scripts/AiInterface.cs\#L12-L13}{BA-rAIce/AiInterface.cs} can be set to \codeobj{false}.}.
Scripts belonging to the game are coded using the programming language \term{C\#}. The agent was programmed with \term{Python 3.5}, relying on the open-source machine learning library \term{TensorFlow}\cite{abadi_tensorflow:_2015}\footnote{\url{https://www.tensorflow.org/}} in version \term{1.3}. For a listing of all further used python-packages and their versions, it is referred to the \href{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/requirements.txt}{BA-rAIce-ANN/requirements.txt}-file. 

While the original framework of the game was coded in advance to this thesis by the \leonbase, \keyword{Leon Sütfeld}, most of this work was contributed by the author of this thesis to enable learning of the game and making it playable by a machine. While it will be explicitly stated what was already given later in this chapter, it is also referred to the respective branch on Github (\textit{Ba-rAIce -- LeonsVersion}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/tree/LeonsVersion}}). The implementation of the agent was not influenced by any other people than the author of this work.

In the description of the sourcecode, code-exerpts will be merged with the text. To distinguish code from text, these exerpts will be shaded in gray. A further distinction is made between \codefunc{methods()} and \codeobj{objects}, \codeobj{variables} and \codeobj{classes}. If not made obvious otherwise, classes are capitalized and objects lower-case. For the sake of brevity, the parameter list of \codefunc{methods} will be left out if not explicitly needed.\\

\noindent The game, making up the environment, is completely independent of the agent and runs as a separate \keyword{process} on the machine. The agent is written in another programming language and must thus make up a distinct process as well. Because of that, it is necessary that agent and environment communicate over a protocoll that allows inter-process-communication. In this work, it was decided to use \keyword{Sockets} as means of communication. While explained in following section, it is for now important to know that sockets are best implemented as running in a separate \keyword{thread}, where they can send textual information to another socket running in another process.

\subsection{The game as given}
\label{ch:gamedescription}

The program flow of the game is encapsuled by the framework that the \term{Unity 3D} provides: To ease the implemention of games, Unity provides numerous \term{game objects} with pre-implemented properties like friction or gravity, as well as drag-and-drop functionality to add 3D components or cameras to the Graphical User Interface (\textbf{GUI}).

To implement additional behaviour or features not predefined by Unity, it allows for scripts, written in object-oriented \keyword{C\#}. Such a file will only be instantiated during runtime if it is specified in Unity's \term{Object Hierarchy}. For that, the script has to provide a class that \term{extends}\footnote{In the following sections, I will use the terms \term{extends}, \term{implements}, \term{knows} and \term{has}. When I use those, I mean them in the strict sense in the context of object-oriented programming languages (and the concepts inheritance, interfaces, references and part-of relations).} Unity's class \codeobj{MonoBehavior} or be used by such a class.

After staring the program, Unity will create all objects specified in the object hierarchy. To enable the possibility of instanciations \term{know}ing each other during runtime, they must provide public variables of the type of the respective subclass of \codeobj{MonoBehaviour}, which can via drag-and-drop be assigned to the respective future instance specified in the hierachy. 

\codeobj{MonoBehaviour} provides a number of functions that are automatically called by Unity at different times during runtime. The most important ones are \codefunc{void Start()}, called when first instanciating the respective object, as well as \codefunc{void Update()} and \codefunc{void FixedUpdate()}, called every Update-step or Physics-update-step, respectively\footnote{Concerning the difference between Update and Fixedupdate: \codefunc{Update} is called once per frame, in other words as often as possible. It is generally not used to update physics, as its call frequency depends on the current \textbf{FPS} -- if calculations here take to long, the FPS of the game will decrease. \codefunc{FixedUpdate} is called precisely in a fixed interval of game-internal time. If calculations in FixedUpdate are too slow, the progress of the game-internal time is delayed until FixedUpdate catches up. The update-interval of FixedUpdate can be freely chosen and is $0.002$ seconds in the current implementation. If Unity's \codeobj{Time.timeScale} is set to zero, FixedUpdate() will not be called at all.}. If a subclass of \codeobj{MonoBehaviour} is attached to a game object, it can provide specific additional functions that are called when specific events occur during runtime -- an example would be \codefunc{OnCollisionEnter(Collision)}.\\

In the following, I will describe the game in chunks corresponding roughly to implemented classes, which is referred to by a footnote in the headline. Keep in mind that the structure of this files as well as most of their content is not created by the author of this thesis. Note also, that I will sometimes mention optional features. As those options are relevant not to a User of the game but to its developers, those options are specified in the code, more precisely in a static class called \codeobj{Consts} in \term{AiInterface.cs}\footnote{\label{aiint} \url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/AiInterface.cs}}.

%TODO was hiervon noch übrig ist \noindent It is hard to describe the game in a way that is understandable for somebody who has not seen any of the code before, as many functions depend on each other, sometimes in circular ways. As however there has to be a start somewhere, it may be the case that some concepts are referenced to before they are fully explained. An example for this is the \inlinecode{QuickPause()}-function: This function's purpose is to pause all physics processes of the game, such that other functionalities running in parallel to it get time to catch up with their calculations. Also, for the game to be played by an agent a \inlinecode{ResetCar}-functionality that can be triggered by another piece of code is useful. While it will be referenced to those functions, their purpose may become clear only later in the text. For an informal description of what each specific file does, it is referred to the table in 


\subsubsection{Game modes\footnote{\label{gamescript}\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/GameScript.cs}}}

On the surface, there are three modes the user can choose from in the main menu: \term{Drive}, \term{Drive\,AI} and \term{train\,AI\,supervisedly}. The UI of the menu can be seen in figure~\ref{fig:overviewshot} in appendix~\ref{AppendixB}. 

In the actual implementation however, the game mode is handled a bit different. \codeobj{mode} is a public string-array of the globally known object \codeobj{Game} (an instanciation of a \term{GameScript}, a subclass of \codeobj{MonoBehaviour} specified in \term{GameScript.cs}\textsuperscript{\ref{gamescript}}). \codeobj{mode} contains one or more strings of the following group: \codeother{"driving"}, \codeother{"menu"}, \codeother{"train\_AI"}, \codeother{"drive\_AI"} and \codeother{"keyboarddriving"}. If the game's main menu is opened, \codeother{mode = ["menu"]}, and in all other cases it is a set consisting of \codeother{"driving"} as well as the respectively obvious elements. This implementation is advantagous, because some behaviour needs to be triggered in multiple modes -- the car's movement is for instance calculated if \codeother{Game.mode.Contains("driving")}, which is the case in all three of the above mentioned modes. The current implementation also makes it easier to add further behaviour: If for example an AI-agent shall also function to generate supervised data, one can simply add the respective mode in the \term{Gamescript.cs} file.

The functionality for switching the game mode is specified in the \codefunc{SwitchMode(newMode)} function, found in \filename{GameScript.cs}. After setting the respective mode as described above, this function disconnects any connected Sockets and  activates the required cameras for the mode, updates the UI's Display indicating the mode and calls some further initializing functions (\codefunc{AiInt.StartedAIMode()} or \codefunc{Rec-StartedSV\_SaveMode()}), if applicable. Because particularly the initialization of the \codeobj{drive\_AI}-mode involves connecting with an external Socket, it is done in part in a side thread. This means that the main thread does not wait for the initialization to be finished -- because of that, the \codefunc{StartedAIMode()}-function sets the variable \codeobj{AiInt.AIMode} to \codeobj{true} once it is done. Any behaviour that depends on a successful initialization of the mode can thus simply check for this variable instead.

The object \codeobj{Game} is responsible for switching the \codeobj{mode} to \codeobj{menu} in its \codefunc{Start()}-method or after the press of the \keystroke{Esc}-button at any time during the runtime of the game. Besides this, its definition in \term{GameScript.cs} also contains the methods \codefunc{QuickPause(string reason)} and \codefunc{UnQuickPause(string reason)}. The purpose of QuickPause is to pause all physics processes of the game, such that other functionalities running in parallel get time to catch up with their calculations. For that, the \codefunc{QuickPause(string reason)}-function sets the Game's \codeobj{Time.timeScale} to zero, which freezes all of Unity's internal physics, as well as stopping future \codefunc{FixedUpdate()}-calls. Further, this function removes \codeobj{driving} from \codeobj{mode}, so that other driving-related functions in \codefunc{Update()} are also stopped. Lastly, the QuickPause-mode changes the game's GUI to make the difference visible to the User. 

While \codefunc{QuickPause(string reason)} is a public function and can in principle be called from every method inside Unity, it contains non-thread-safe functionalities (due to design concepts of Unity), which only the main-thread can call safely. Asynchronous threads use the public variable \codeobj{shouldQuickpauseReason}, to request the activation of the QuickPause-mode and \codeobj{Game} checks every \codefunc{Update()}-step if a side thread made such a request. To make sure that multiple requests are treated sequentially, \codefunc{QuickPause} has to be called with a \codeobj{string reason}, which is pushed on Game's list \codefunc{FreezeReasons}. When one of the reasons to request QuickPause is resolved, the method \codefunc{UnQuickPause(string reason)} must be called with the same \codeobj{reason}. This method then removes this \codeobj{reason} from the list, but the normal game process is only continued if this results in the list to be empty. 

\subsubsection{User Interface\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/UIScript.cs}}}

The job of the Game's \codeobj{UI} (an instance of type UIScript) is to update the user interface, which overlayed over the current scene. In its \codefunc{MenuOverLayHandling()}-Method, \codeobj{UI} specifies the view of the game's menu-mode (as can be seen in figure~\ref{fig:overviewshot} in appendix~\ref{AppendixB}), as well as the key bindings to activate the required mode. In the method \codefunc{DrivingOverlayHandling()}, it sets visibility, content, color or position of numerous UI components (defined in Unitys Object Hierachy), that are seen in non-menu-modes. Both \codefunc{DrivingOverlayHandling()} and \codefunc{MenuOverLayHandling()} are called every \codefunc{Update()}, such that the view elements are always contemporary. Further, the \codeobj{UI} specifies the \codefunc{void onGUI()}-function, which Unity calls every time it re-renders the GUI. This function overlays Debug information on the screen and changes the view if the QuickPause-mode is activated.\\

\noindent The User Interface of the game while driving can be seen in the figures~\ref{fig:humandriveshot} and~\ref{fig:aidriveshot} in appendix~\ref{AppendixB}, which are screenshots for the \codeobj{keyboarddriving} and \codeobj{drive\_AI} mode, respectively. The latter of those screenshots is annotated with labelled boxes around each UI component, with page~\pageref{fig:aidriveshot} explaining each component a bit further\footnote{Note that the entire UI, besides the content behind labels \textbf{A}, \textbf{F}, \textbf{H} and \textbf{I}, was already implemented like this by \leon.}.

\subsubsection{Controls}

If \codeother{Game.mode.Contains("keyboarddriving")}, the game is steered with the arrow keys \keystroke{$\leftarrow$} and \mbox{\keystroke{$\rightarrow$}.} The throttle is triggered via the \keystroke{A}-key, whereas the brake is called via \keystroke{Y}. The \keystroke{R} - key flips the reverse gear. Note that as long as the \codeobj{pedalType} in \term{CarController} is set to \codeother{"digital"}, throttle and brake are binary when controlled via keyboard. 

If \codeother{Game.mode.Contains("drive\_AI")} the car is usually controlled by the agent. It is however possible to re-gain control over it via pressing the \keystroke{H}-key. Once that occurs, the variable \codeobj{AiInt.HumanTakingControl} is set to true, indicating the program that keyboard-inputs must be accepted. This is useful for example if one wants to check if rewards or Q-values are realistic. If human interference of the \codeobj{drive\_AI} mode is active, it is possible to simulate speeds to the agent (meaning that not the actual speed of the car, but a specified value is sent to the agent). This can be done with the number keys, where the pretended speed is evenly spread between $0~ kph$ (\keystroke{0}) and $250~ kph$ (\keystroke{9}). The \keystroke{P} key is reserved to simulate a full throttle value. To hand control back to the agent, \keystroke{H} must be pressed again. 

In the \codeobj{drive\_AI}-mode, a user can also manually disconnect or attempt a connection-trial with an agent. The keys to do that are \keystroke{D} and \keystroke{C}, respectively. During any \codeobj{mode} containing \codeobj{"driving"}, \keystroke{Q} can be pressed to activate the \term{QuickPause}-mode, which allows to spectate the current screen. QuickPause is ended with another hit of \keystroke{Q}. 

\subsubsection{The car\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/CarController.cs}}}

The \codeobj{car} itself is a \codeobj{Rigidbody}, which is a Unity-gameObject with certain properties like spatial expanse, mass and gravity. Attached to the \codeobj{car} is, next to no further mentioned visible components, a Unity \codeobj{BoxCollider} as well as four \codeobj{WheelColliders} gameObjects. While the \codeobj{BoxCollider}'s purpose is to trigger the call of particular functions in scripts attached to other gameObjects upon simulated physical contact, the \codeobj{WheelColliders} are predefined with certain physical properties, allowing for precise simulation of the behaviour of actual tires. How the car moves is specified in an instance of the class CarController, which is attached to the respective Rigidbody.

All functions of the \codeobj{CarController} are only called in modes containing \codeobj{driving}. In its \codefunc{FixedUpdate()}-step, the script adjusts the wheelCollider's friction according to the current surface the wheels are on, and checks if the car moved outside the street's surface. Furthermore the car's velocity as well as some other values are calculated. Finally, the torques for acceleration and braking are applied and the front wheels are turned according to the steering-value. The amount of those torques and angles depend on three values: $steeringValue \in [-1,1]$, $throttlePedalValue \in [0,1]$ and $brakePedalValue \in [0,1]$. If \codeother{Game.mode.Contains("keyboarddriving")} or \codeother{AiInt.HumanTakingControl == true}, those values depend on the User's keyboard input. Otherwise, if \codeobj{AiInt.AIMode} is enabled, the values are defined as \term{know}n values from the \codeobj{AiInt}, namely \codeobj{nn\_steer}, \codeobj{nn\_throttle} and \codeobj{nn\_throttle}. \codeobj{AiInt} is an instance of class AiInterface\textsuperscript{\ref{aiint}} that will be elaboreated on later.

In \codefunc{CarController.Update()}, the outer appearance of the car is updated, consisting of wheel height, wheel rotation and wheel rotation.

As explained in section~\ref{ch:agentchars}, a connected agent must be able to reset the car at any time during runtime. To allow for that, the \codeobj{CarController} provides a \codefunc{ResetCar} method. Additionally, there is a \codefunc{ResetToPosition}-function that resets the \codeobj{car} to any specified position and rotation. To reset the car, it is necessary to completely reset its inertia without the respective values being overwritten in the next \codefunc{FixedUpdate()} call. \codefunc{CarController.FixedUpdate()} therefore checks the boolean variable \codeobj{justrespawned} on every call, which is set to \codeobj{true} by \codefunc{ResetToPosition}, to reliably remove all of the car's inertia before setting the boolean to \codeobj{false} again.


% TODO hier wallcolliderscript

\subsubsection{Position tracking\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/PositionTracking.cs}}}

To successfully learn useful driving policies, the agent must get precise knowledge of the car's position which goes beyond its mere coordinates. Additional useful information is for example the car's position in relation to the street, or information about the course of the road ahead of the it. To allow for that, the game incorporates a \codeobj{TrackingSystem}, which is an instance of the class \codeobj{PositionTracking}). The \codeobj{TrackingSystem} knows the gameObject \codeobj{trackOutline} and converts it to an array of coordinates located regularly along the track, each one respectively located at the middle of the street -- the \codeobj{Vector3[] anchorVector}. Using this array, much high-level information about the track can be calculated. As almost all of the respective functionality was however not implemented was not implemented by the author of this thesis, a short scetch of how it can be used shall suffice.

The total length of the track can be calculated by summing up all distances between all \codeobj{anchorVector} coordinates and their respective successor. By putting this in relation to the current advancement of the car (which is the sum of all distances up to the coordinates closest to the car, calculated in \codefunc{GetClosestAnchor(Vector3 position)}), one can calculate the rough driving progress in percent.

As every successive coordinate in \codeobj{anchorVector} is in the middle of the street, one can calculate the direction of the street at position \codeobj{p} by calculating the vector \codeobj{anchorVector[GetClosestAnchor(p)+1] - anchorVector[GetClosestAnchor(p)]}. This can be used as basis for many further calculations: For instance, the car's distance to the center of the street can be found by calculating the norm of the orthogonal projection from the car's coordinate onto this vector. The direction of the car relative to the street can be found by calculating the angle between its \codeobj{direction}-vector and the previosly explained vector.

Besides defining the \codeobj{anchorVector}-array and other helper-arrays depending on it in its \codefunc{Start()}-method, the \term{TrackingSystem} calculates the car's current \codeobj{progress} at every \codefunc{Update()}-step and triggers the \codefunc{UpdateList()}-method of the \codeobj{RecordingSystem} in regular progress-intervals. Furthermore, the \term{TrackingSystem} provides certain public methods that can be used by an agent, namely \codefunc{getCarAngle()}, \codefunc{GetSpeedInDir()} and \codefunc{GetCenterDist()}, the precise content of which will be explained in a later section.

\subsubsection{Tracking time\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/Recorder.cs}}}

As visible in the game screenshots (more precisely annotations \textbf{B}, \textbf{C}, \textbf{P} and \textbf{Q} of Figure~\ref{fig:aidriveshot}), the game displays information about the current laptime, the last laptime as well as the time needed for the fastest lap. Furthermore the game provides visual feedback on the difference in time needed for a specific section of the street in the current lap versus the fatest lap (annotation \textbf{E}). This is possible because the game records current laptime multiple times throughout the course. As mentioned above, \codefunc{RecordingSystem.UpdateList()} gets called regularly by the \codeobj{TrackingSystem}. \codeobj{RecordingSystem} is an instance of the type \term{Recorder}. It contains three lists of \codeobj{PointInTime}s, for \codeobj{thisLap}, \codeobj{lastLap} and \codeobj{fastestLap}. A \codeobj{PointInTime} is a \term{serializable} object (also defined in \filename{Recorder.cs}) that contains two floats, for a progress and a corresponding time.

An instance of a separate class, \term{TimingScript} (found in \term{TimingScript.cs}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/TimingScript.cs}}) is attached to a permeable Collider right on the start/finish line that functions as trigger. As a subclass of \term{MonoBehaviour}, \term{TimingScript} has a \codefunc{void OnTriggerExit(Collider other)}, that is invoked as soon as another Collider stops contact with it. As the only movable collider is the car's boxCollider, this method is called as soon as the car starts a lap. A lap is considered valid under two conditions: First, the car needs to pass a second collider (\codeobj{confirmCollider}, with its attached \term{ConfirmColliderScript}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/ConfirmColliderScript.cs}}), which ensures that the car did in fact drive a complete lap instead of backing up right back on the start/finish line. The second condition for validity is, that at no time all four tires of the car left the street's surface.

If a lap is considered valid, the \term{TimingSystem}'s \codefunc{onTriggerExit}-procedure calls \term{RecordingSystem}'s \codefunc{Rec.FinishList()}-method. Afterwards and under no restrictions, it prepares the start of a new lap by calling \codefunc{Rec.StartList()}. Once \term{StartList} is called, the \term{RecordingSystem} creates a new List of \codeobj{PointInTime}s, to which the \term{TrackingSystem} then regularly adds new tuples of progress and corresponding time. Once FinishList is called, the \term{RecordingSystem} checks if the lap just now is a new record, and saves it on the computer's disk if so. In its methods \codefunc{GetDelta()} and \codefunc{GetFeedback()}, which are called every \codefunc{Update()}-step of the \codeobj{UI}, it can then compare the time of the currently latest progress with the corresponding time of \codeobj{fastestLap}. \\

\subsection{The game -- extensions to serve as environment}

The code explained so far is sufficient for the game to work in the \codeobj{"keyboarddriving"}-mode. The framework for this mode was working entirely when the author of this thesis received it, as it was implemented by the \leonbase. The major additions implemented in the scope of this thesis that were mentioned so far is the behaviour following game modes other than \codeobj{"keyboarddriving"} or \codeobj{"menu"}, the \term{QuickPause}-functionality, the mentioned additions to the User Interface, the means to \term{reset} the car as well as the functions \codefunc{GetCarAngle()} and \codefunc{GetSpeedInDir()} of the \term{TrackingSystem}, which will be more thoroughly explained lateron.

\subsubsection{The minimap cameras\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/MiniMapScript.cs}}}
\label{ch:minimap}

The content of the minimap cameras can be seen behind annotations \textbf{H} and \textbf{I} of figure~\ref{fig:aidriveshot}. They are implemented to serve as an exclusive or additional input to agents, in the hope of providing enough information to learn sucessful policies. As can be seen, the minimap cameras provide a bird-eye view of the track ahead of the car, by filming vertically downwards. In contrast to the foreshortened main-camera, the minimap cameras are orthogonal, which means that distances are true to scale, irrespectively of their position. Because the cameras are attached to the \codeobj{car}'s Rigidbody, they are always in the same position relative to the car. In the current implementation, up to two cameras can be used (it is however possible to disable one or both cameras by setting a corresponding value in the class \codeobj{Consts} in \term{AiInterface.cs}). When both cameras are active, one of them is mounted further away from the car, such that one provides high accuracy whereas the other provides a greater field of view. If only one camera is enabled, its distance is set for tradeoff of accuracy and field of view. As both cameras must be handled separately, this happens in the \codefunc{Start()}-method of the \term{Gamescript.cs}, which \term{know}s both cameras.

While a previous implementation of a similar functionality was provided by \leon using a complex and inefficient ray-tracing, in this implementation the minimaps base on Unity \codeobj{Camera}-objects, which are efficiently calculated on the computer's GPU. Attached to each camera is a respective instance of \term{MiniMapScript}. While ordinarily the content of Unity's cameras is directly rendered to the game's main screen, this script contains methods to convert the image of the camera to a format that can be sent to an agent. That is made possible by the usage of a \codeobj{RenderTexture} as well as a \codeobj{Texture2D}, which are created as private objects in \term{MiniMapScript}'s \codefunc{PrepareVision(int xLen, int yLen)}-method. This method is called form outside and expects as parameter the dimensionality of the produced matrix, which is set in the class \codeobj{Consts} in \term{AiInterface.cs}. 
% TODO dass das visiondisplay nur if needed prepared wird

Both cameras provide the public function \codefunc{GetVisionDisplay()}. When this function is called, it sets the above mentioned \codeobj{RenderTexture} as the camera's \codeobj{targetTexture}, forces the camera to \codefunc{Render()} to this texture, and then reads the rendered contents into the specified \codeobj{Texture2D}. After this process, it must reset the camera's \codeobj{targetTexture}, such that it renders back to the game's main display, such that it can be inspected visually. The \codeobj{Texture2D} however can be then be read pixel by pixel and thus converted to an array or string. As it was decided that the resulting display only differentiates between \term{track}, \term{curb} and \term{off}, the cameras use a \codeobj{Culling Mask} that visually filter out all other gameObjects.

\subsubsection{Recording training data\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/Recorder.cs}}}

\label{sec:exportdata}

For an agent to learn and perform in it, data of the environment must be recorded in regular intervals, to either be sent to the agent in the case of it playing the game or learning via reinforcement learning, or to be exported to a file, which an agent can perform pretraining on. In the following sections, I will not talk about what those data exactly looks like, but only how it is saved and sent. Because of that, I will refer to this data under the name \term{vectors}, which are in detail explained in section~\ref{ch:thevectors}. Collecting the lastest \term{vectors} happens in the function \codefunc{GetAllInfos()} of \term{AiInterface.cs}, which calls a number of functions and returns a string containing the combined result of those.

As calculating the data that needs to be exported can take relatively much time, this process cannot be performed every \codefunc{FixedUpdate()}-step. Because of that, the following function is used to perform a function in regular time intervals:
\begin{algorithm}[h]
\begin{lstlisting}[language=C#, style=CSharp, frame=none]
long currtime = AiInterface.UnityTime();
if (currtime - lasttrack >= Consts.trackAllXMS) {
	lasttrack = lasttrack + Consts.trackAllXMS; 
	SVLearnUpdateList ();
}
\end{lstlisting}%
\caption{Executing a function in regular intervals}
\label{alg:everyxseconds}
\end{algorithm}

Where \codefunc{AiInterface.UnityTime()} returns Unity's internal time by calling \codeobj{Time.time * 1000}. Using this definition of time has the advantage that it is maximally precise inside Unity, as the time of calling \codefunc{FixedUpdate()} is likewise dependant on \codeobj{Time.time}. A disadvantage of this measurement of time is however, that it can only be used in the main thread and asynchronous methods must rely on the system's time, for which no conversion method exists.

Once user selects the \term{Train AI supervisedly} mode, Recorder's \codefunc{void StartedSV\_SaveMode()} gets called, which enables the minimap-cameras and sets \codeother{SV\_SaveMode = true}. If that variable is \codeobj{true}, the recorder will in its \codefunc{StartList()} create a new \codeother{SVLearnLap = new List<TrackingPoint> ()}. \codeobj{TrackingPoint} itself is a class defined in \term{Recorder.cs}, that contains certain values about the state of the game, if provided at creation. While driving, the recorder checks every \codefunc{FixedUpdate()}-step with the mentioned method if it calls \codefunc{SVLearnUpdateList()}. This method then collects the recent values for the currently performed actions, laptime, progress and speed as well as the respectively latest \term{Vectors}, creates a \codeobj{TrackingPoint} from those and updates the \codeobj{SVLearnLap} with it. When the RecordingSystem's \codeobj{FinishList} function is called upon the next crossing of the start/finish line, the \codeobj{SVLearnLap} is saved to a file. As the vectors can contain multiple pixel matrices from the minimapcameras, this may however take quite long. To prevent the game from freezing everytime the car passes the start/finish line, the saving of the actual file is performed in a seperate thread.

Because the agent using this exported data is written in another programming language than the environment, the data cannot be exported as binary file. In this implementation, it was decided to save the data in the \term{XML}-format. It is worth mentioning that not only the List of \codeobj{TrackingPoint} is exported, but also additional meta-information, stating among others the interval of how often a \codeobj{TrackingPoint} was exported, which can be interpreted and used by an agent as well as manually inspected.

\subsubsection{Communicating with an agent\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/AiInterface.cs}}\textsuperscript{,}\footnote{\label{footnoteAiInt}\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/AsyncClient.cs}}}

\label{sec:codeCommunAgent}

%TODO to detect if the car hit a wall (which may require an agent to reset the environment), the wall has an attached WallColliderScript, that can notify an agent upon \inlinecode{onCollisionEnter} by sending the message "wallhit" via sockets.

As already mentioned, the game is running live and is in general not stopped by the agent, as done for example when interfacing with the openAI gym (section~\ref{ch:rlframeworks}). Because of that, the speed of communication between agent and environment is a bottleneck in how good an agent can perform, and needs to be as fast and efficiently implemented as possible. To ensure quick reaction times, it was also decided that agent and game must run on the same machine, as sending the data to another machine increases the needed time drastically\footnote{This is the reason this project was implemented entirely under Windows: There is no stable Unity Editor for Linux, and there is no contemporary GPU-supporting TensorFlow under Mac. The only common ground for which both are available ist therefore the Windows platform.}.

In the scope of this thesis, it was experimented a lot with the flow of communication between agent and environment, with the current version as its most efficient one so far. While there are multiple possibilities of how two different programming languages can communicate with each other (for example \textit{zero-mq}, \textit{named pipes} or \textit{shared memory}), it was decided to use \term{Sockets} for the communication. When both sockets are on the same machine and communicate over localhost instead of online, all unnecessary protocol layers are skipped, making a connection over sockets very efficient\footnote{As is explained by alleged former Microsoft networking developer Jeff Tucker in this Stackoverflow-thread: \href{http://stackoverflow.com/questions/10872557/how-slow-are-tcp-sockets-compared-to-named-pipes-on-windows-for-localhost-ipc}{http://stackoverflow.com/questions/10872557/how-slow-are-tcp-sockets-compared-to-named-pipes\,-on-windows-for-localhost-ipc}}. Using the current implementation, reaction times with an average of $30ms$ were archived (including the network inference), which is fast enough for all purposes.

Communication over sockets generally asymmetric: There must always be a \term{Server} and one or more \term{Clients}, both of which contain instances of the class \term{Socket}. Upon being started, the server registers a server-socket at a certain \term{Port} of the machine, where it can be found by clients. It is only possible for clients to connect to it as long as the server keeps up this socket, which is why it is advisable to do so in a separate thread. When a client is started, it needs to know the IP-adress of the server (in this case localhost), as well as the number of the port the corresponding socket waits at. Once the client finds a waiting server-socket behind the port, it is common practice that the server creates a new socket at another port. While this new socket represents a stable connection between server and client, the original server-socket can keep on waiting for new clients to connect to in the future. The length of each message is transmitted with it, to allow the socket to know when to stop reading the input stream.

In this project it was decided that the game functions as a client, whereas the server is written in python. In contrast to the implementation of \cite{loiacono_simulated_2013}, this also means that main loop happens in python, with the game only considered as an additional thread providing the agent's input (as can be seen in figure~\ref{fig:sequenceserver}). While this makes it harder to connect multiple agents simultaneously to the game engine, multiple copies of a game engine can in theory be used to train an agent.

If the \term{Drive\,AI} mode was selected in the game's \term{menu}, the \codeobj{AiInt} calls 
its function \codefunc{StartedAIMode()}, which, next to calling the known \codefunc{PrepareVision} of the cameras, creates a \codeobj{SenderClient} and a \codeobj{ReceiverClient}, both of which are instances of \codeobj{AsynchronousClient}, a class defined in \term{AsyncClient.cs}\textsuperscript{\ref{footnoteAiInt}}. Afterwards, it connects sender and receiver by calling their \codefunc{StartCleintSocket()}-methods and starts the \codeobj{ReceiverClient}'s receive-loop in an asynchronous thread via calling their respective method \codefunc{StartReceiveLoop()}. 

\paragraph{AsynchronousClient} is a wrapper-class that contains a C\# \codeobj{System.Net.Sockets.Socket}. While \codeobj{SenderClient} and \codeobj{ReceiverClient} have different tasks, much of their functionality overlaps, which is why the same class is used for both. It is useful to use two different sockets for sending and receiving data to ensure minumum latency: As they are in different threads, data can be sent and received simultaneously. As also the used \codeobj{Socket}s work asynchronously internally, they require callback-methods and \codeobj{ManualResetEvent}s in their methods to connect (\codefunc{StartClientSocket()}) as well as to \codefunc{Send} and \codefunc{Receive} data\footnote{A \codeobj{ManualResetEvent} is a C\#-object notifies waiting threads that an event occured. Before for example the client's \codefunc{beginConnect}-method is called, \codeobj{ManualResetEvent}s value, which can be accessed from outside threads, is set to false. The method itself is called with \codefunc{ConnectCallback} as argument. Once \codefunc{beginConnect} is done, \codefunc{ConnectCallback} sets the corresponding value to true again, such that other threads can wait for this event.}.

As soon as \codefunc{StartClientSocket()} is called, an \codeobj{AsynchronousClient} tries to connect to a server for a specified number of times \codeobj{MAXCONNECTTRIALS}. If this number is exceeded, it assumes there is no server and prevents further trials to connect by setting \codeother{serverdown = true}. Both clients can manually be reconnected by pressing \keystroke{C}, which resets this value and tries to connect again. Note that connecting the server necessarily goes along with sending the message \codeother{"resetServer"} to the server and starting the receivers receive-loop. 

Whenever data must be transmitted to the server, \codefunc{AiInt.SendToPython(string data)} is used. This method appends the current time to the data and calls \codefunc{SenderClient.SendInAnyCase(String data)} in an asynchronous thread. To be absolutely failsafe, this method tries to send the data over its socket, and if that fails, creates a new socket to send the data over. 

\codefunc{ReceiverClient.StartReceiveLoop()} runs constantly in another asynchronous thread to wait for messages sent by a server\footnote{Note that because of reasons stated above the length of every message is always prepended. As soon as it gets a message, the receiver starts by reading only this number. In the first callback-method \codefunc{ReceiveStringLengthCallback} then, it starts another receive-loop that reads the respective message bufferedly, calling \codeobj{ReceiveCallback} as soon as that is done.}. As soon as it got a message, the method \codefunc{ReceiveCallback} calls \codefunc{response.update()} with this method.

Messages from the server are stored in an object of type \codeobj{Response}, which is part of the \codeobj{ReceiverClient}. When the \codeobj{SenderClient} sends information to python, it always appends the current system-time. When the server sends a result basing on this input, it also returns the time of sending. Upon being updated, the \codeobj{response} takes another timestamp and calculates the response-time of the server and maintains a running average of these response times in \codeobj{lastRt}s. If this average reaction time is too high (\codeother{lastRTs.getAverage() > 2*Consts.MAX\_PYTHON\_RT}), the receiver-thread requests a QuickPause -- this stops the \codeobj{SenderClient} from sending further data, but not the asynchronous \codeobj{ReceiverClient}, which can thus catch up with its receiving progress. As soon as the latest chunk of data was returned by python (the time of which is known through \codeobj{AiInt.lastpythonupdate}), the receiver requests UnQuickPause.

\paragraph{Main communication loop} 
The main communication loop is specified in the methods \codefunc{Update()} and \codefunc{FixedUpdate()} of the AiInterface. As the former of those runs also when the game is in QuickPause-mode, this method is responsible to handle the \codeobj{othercommand}s sent from another socket. These commands may be to reset the car, or to activate/deactivate the QuickPause-mode. To differentiate these commands from steering-commands, the \codeobj{ReceiverClient.response} holds a respective field for \codeobj{othercommand}s.

The main loop of sending and receiving data to and from an agent however happens in the \codefunc{FixedUpdate()}-method. In this method, the data is both sent to a server and the response of a such is handled.

\subparagraph{Sending}
Every \codefunc{FixedUpdate()}-step, the \codeobj{AiInt} checks if enough time has passed to send the latest data to an agent (see alg.~\ref{alg:everyxseconds}) in the method \codefunc{LoadAndSendToPython}. If so, it calls its method \codefunc{load\_infos}. This method performs a quick check if the car's situation changed enough to require a reload of the data, simply passing the last result otherwise for efficiency-reasons. If the data is re-loaded, the method \codefunc{GetAllInfos()} is called, which aggregates all getter-function for the various vectors as well as the minimap-cameras into a string containing all the data. Note that speed as well as throttle-value as it is sent to a server is already possibly overwritten at this point. After aggregating it, the data is subsequently sent to python using the \codeobj{SenderClient} in a separate thread. In the current implementation, the game updates its agent in an interval of 10 FPS, a faster communication is however easily possible.

Furthermore, the \codeobj{AiInt} has the method \codefunc{notify\_wallhit}, which is called in the method \codefunc{onCollisionEnter} of a \codeobj{WallColliderScript} attached to the track's outline. This method notifies the server in form of a \textit{special command} if the car crashed into the track's wall to possibly reset it. Similar behaviour is triggered by the TimingScript upon completion of a lap.

\subparagraph{Receiving}
Every \codefunc{FixedUpdate()}-step, the \codeobj{AiInt} checks the newest response. If Unity is not set for a \codeobj{Consts.fixedresultusagetime}, it immediately sets the values \codeobj{nn\_throttle}, \codeobj{nn\_brake} and \codeobj{nn\_steer}, such that the \codeobj{CarController} can use it in the next iteration. Otherwise, it appends a copy of the latest response to a buffer in the form of a fixed-size queue. Every \codefunc{FixedUpdate()}-step, the \codeobj{AiInt} checks if a new item is in the buffer that was sent exactly \codeobj{Consts.MAX\_PYTHON\_RT} milliseconds and sets the respective values for the \codeobj{CarController}. This buffer can also be used to interpolate between the respective controls to counter jittering of the car's controls. These values are however only used if the variable \codeobj{HumanTakingControl} is \codeobj{false}, which is toggled with a hit of \keystroke{H}. The used setting is comparable to \cite{wawrzynski_control_2015}, in that there are two levels of control -- The agent tells the car the desired position/speed of rotation of the cars actuators, and the lower-level tries to make the joints follow them.




%TODO - ungefähres UML-diagramm DES SPIELS!!
%-ablaufdiagramm vom server-zeug dass zu dem von python passt

\subsection{The agent\footnote{I will rely heavily on UML sequence diagrams and class diagrams to explain program flow and classes of agents. In case a reader is unfamiliar with the standards of UML 2.0, it is referred to e.g. \url{https://www.ibm.com/developerworks/rational/library/3101.html} for sequence diagrams and \url{https://www.ibm.com/developerworks/rational/library/content/RationalEdge/sep04/bell/index.html} for class diagrams. In the main text, only small class-diagrams accompany the explanation of the code, with respective complete versions in  \colorbox{red}{appendix~\ref{BLABLABLA}.}}}

As already explained, the implementation at hand differs in its program flow from other implementations, as the environment cannot be specified in a separate class, on which the agent can perform a function like \codefunc{step()} (as in line \ref{algline:gym_envstep} of algorithm~\ref{alg:gym}). As the closest equivalent to such a class, the file  \filename{server.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/server.py}} (from now on called server) contains the thread-safe class \codeobj{InputValContainer} that provides explicit functions to read its state. Upon executing the server, an instance \codeobj{inputval} of type \codeobj{InputValContainer} is created. This instance is \term{know}n to objects that read or write from it through \codeobj{containers}, an instance of class \codeobj{Containers}. This class contains references to several objects and settings that must be known to multiple objects and functions (even in between threads), which can thus for example easily access the mentioned input-value as \codeobj{containers.inputval}.

It is further important to keep in mind, that the server receives \textit{raw data} from the client, and not a tuple consisting of \codeobj{observation, reward, done}. The raw data is forwarded to the agent, which calculates the respective \term{features} on its own. The next section will detail how the server initiates threads needed for the communication with the environment, before leading over to the main loop of the learning process.

\subsubsection{Communicating with an environment}

The behaviour that will be explained is represented graphically in the sequence diagram in figure~\ref{fig:sequenceserver}. 

The main loop of the agent is started by the \codefunc{main}-method of the server. Upon execution of this file, it interprets any passed arguments and starts the main-method. %TODO auf die argumente zurückkommen die Übergeben werden
To allow a fast communication with the game, the main loop of the server is multi-threaded, such that a stable socket-connection to Unity can be preserved while the agent performs its actions in another thread. This may lead to some objects being accessed by different threads (like for example \codeobj{outputval}), which requires a thread-safe implementation of them.

In its \codefunc{main}-method, the server creates two server-sockets\footnote{More precisely, the server uses a wrapper-class called \codeobj{MySocket} (defined in \filename{server.py}), which provides the additional behaviour of prepending the length of the future message to it, such that the socket on the other end knows how much it needs to read.}, one on the receiver-port (which is equal to Unity's sender-port), and another one on the sender-port (Unity's receiver-port). Afterwards, the server creates an agent of choice by initializing it and calling its \codefunc{initForDriving}, passing command-line-parameters. Afterwards, the \codeobj{InputValContainer} \codeobj{inputval} as well as an \codeobj{OutputValContainer} termed \codeobj{outputval} are created, and their references are added to \codeobj{containers}. Once those are created, the server starts two \term{thread}s, more precisely a \codeobj{ReceiverListenerThread} and a \codeobj{SenderListenerThread}. These threads use their respective server-socket they know through \codeobj{containers}, and wait on their port by calling the socket's \codefunc{accept()}-method over and over.

Once a client connects to one of those listener-sockets, a new connected socket is created. Using this socket, a \codeobj{receiver\_thread} or \codeobj{sender\_thread} is created respectively, representing a stable connection to Unity. After creating this thread, the ListenerThread lets \codeobj{containers} know about its reference (by appending it to \codeobj{containers.receiverthreads} or \codeobj{containers.senderthreads}) and starts its listening-loop over again. This means that during runtime, the two ListenerThreads are always active. If for some reason the old \codeobj{receiver\_thread} or \codeobj{sender\_thread} loses its connection, the ListenerThreads will simply establish a new one and create a new thread. Both \codeobj{receiver\_thread} and  \codeobj{sender\_thread} contain a method to destroy themselves if needed, such that at any time there is exactly one thread responsible for the connection with Unity.

It was experimented with an explicit learning-thread, which runs in parallel to all other threads. As however the very same TensorFlow-model is needed for learning and inference, doing so in parallel emerged to increase the reaction time of the server from about $30ms$ to about $300ms$. While forcing the inference to run on the CPU and the learning to run on GPU decreased the delay a bit, the performance was still far from satisfactory and the idea was dropped.

While the main thread is constantly running, it does not contribute to the agent's process after initializing all threads and objects. Instead it can, if the agent provides methods to fill it, run the mainloop of the GUI (defined in \filename{infoscreen.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/infoscreen.py}}), which is programmed with the package \codeobj{tkinter} and must be in the main thread. To allow for side threads to write content to the GUI, its respective widgets (\codeobj{Text} and \codeobj{Canvas}) are replaced by thread-safe wrapper-classes using \codeobj{queue}s. When a side thread wants it to print information, it appends the new content to the queue, whereas the main thread always pops the latest element from that queue to print it. Side threads know those wrappers over the dictionary \codeobj{containers.screenwidgets} and can call the \codefunc{write} or \codefunc{updateCol}-method of the respective element.

The main-thread listens for a termination of the user with \keystroke{CTRL}+\keystroke{C}. Once that occurs, it notifies all other threads to shut down by setting \codeother{containers.KeepRunning = False} and waits for their termination by \codefunc{join}ing them, such that the main thread is guaranteed to be the last to finish.

% TODO - Why I have inputval and outputval: INPUTval damit der immer nur die letzte action hinzügen muss und der die history DADRIN hat, und OUTPUTval damit das multithreading schneller geht. BEIDES hat  seinen sinn!! 
%TODO [multiple threads deshalb weil das geschwindigkeit optimiert - beim inputval können mehrere agents multi-threaded unabhängig/gleichzeitig lesen und performen (dann gäbs nen weiteren thread, der wäre dann nicht blau und würde auf inputval zugreifen), und beim outputvall können senderthread und receiverthread einfach gleichzeitig so schnell wie möglich arbeiten]

\begin{figure}[h!]
	\centering
	\resizebox{1.01\textwidth}{!}{
		\input{sequence_diagrams/server_main}
	}
	\caption{Sequence Diagram of the Server}
	\label{fig:sequenceserver}
	\medskip
	\scriptsize
	Reading instructions: Y-axis is time. Columns with a full colored bar are threads, other objects are objects. The color of a bar represents which thread a method runs in. An arrow from A to B means "A calls a method from B". %TODO Simultaneous things are horizontally aligned. Messages from thread to thread are without parantheses?
\end{figure}

\subsubsection{The main loop}

In the \codeobj{receiver\_thread}, the socket is constantly waiting for data. When it receives data, it first checks if it was the usual raw data used for the agent's observation/reward or a \textit{special command}. If it was the latter, the \codeobj{receiver\_thread} calls the agent's function \codefunc{handle\_commands} which will, according to its preferences, terminate the episode (for which it resets \codeobj{inputval} and \codeobj{outputval} and notifies the environment to reset itself, more on that later). 

If the data was no such command, the \codeobj{receiver\_thread} updates the \codeobj{inputval} and starts an inference of the agent by calling its method \codefunc{performAction}, passing the latest content of the \codeobj{inputval}. This method is where the agent calculates its observation/reward, adds this to its replay memory, calculates the action on basis of its model and performs learning steps as appropriate. Immediately after the action is calculated, the agent updates the \codeobj{outputval} with the the result. Upon being updated, the \codeobj{outputval} informs the \codeobj{sender\_thread} of this update. This thread can then read the latest content of \codeobj{outputval} and send it back to the client using its socket, which may be simultaneous to the agent performing its learning-step\footnote{Note that all of the agent's inference runs thus in the receiver-thread. It was experimented to use an additional agent-thread that in regular intervals looks if the \codeobj{inputval} was filled with new data, such that the inference of the agent and a waiting to receive new data can occur simulatenously. However, as the agent's inference runs fast enough if it does not perform a learning-step after an inference and too slow in any case if it does, this concept was abandoned.}. 

As mentioned above, the \codeobj{inputval} is the server's closest correspondance of the environment. Every time new raw data from the game is sent to the server, \codefunc{inputval.update} is called, which then incorporates the new information as well as timestamps of when they were send (used to calculate how long an inference took). Because the information that is sent over the sockets is textual, the method \codefunc{cutoutandreturnvectors} from the file \filename{read\_supervised.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/read_supervised.py}} is called to convert this text back to python-arrays. Note, that there will consistently be made a difference between the content of the minimap-cameras (vision-vector, see \ref{ch:minimap}) and the other \term{vectors}, as they are stored in a different type of object. While the vision-vectors are each stored as a two-dimensional \codeobj{numpy}-array, the other vectors are bundled to a class called \codeobj{Otherinputs}, a subclass of  \codeobj{namedtuple}\footnote{\url{https://docs.python.org/2/library/collections.html#collections.namedtuple}}, which provides easily readable dot-access to its members instead of solely by index. This allows to add new vectors easily, as only a name of the new feature must be added in the definition of \codeobj{Otherinputs} in the file \filename{read\_supervised.py}. Further, it increases the overseeability of functions using its components a lot.

The \codeobj{inputval} contains arrays to store the history of vision-vectors (\codeobj{vvec\_hist}), otherinputs (\codeobj{otherinput\_hist}) as well as an array for the history of performed actions (\codeobj{action\_hist}). In its \codefunc{update}-method, it normalizes the \codeobj{otherinputs} and appends it as well as the latest vision-vectors, where it merges both into the \codeobj{vvec\_hist}.
To reduce the computational load, the \codeobj{inputval} does not contain the entire history of the game. As already explained, it contains a superset of the observations of the actual agent, which likely contains much superflous information. To reduce the working memory load it causes, the \codeobj{inputval} only saves the latest information send from the game, which corresponds to the maximum of information an agent \textit{could know} about the last or second to last state. After appending the newest vectors to the value, the \codeobj{inputval} also checks if the car faced into the wrong direction for a certain amount of time and notifies the agent if so, such that the agent can optionally reset the environment.

Furthermore, the \codeobj{inputval} defines a \codefunc{read(pastState=False)}-method, such that an agent can access the information needed for its observation of the latest two states. This method unpacks and returns the respective vision-vector history of both minimap-cameras, the \codeobj{otherinput\_hist} as well as the \codeobj{action\_hist}. It is necessary that also the information about the penultimate state can be obtained, as the agent must add a full tuple of $\langle s_t, a_t, r_t, s_{t+1}, t+1==t_t \rangle$ to its replay memory (see chapter~\ref{ch:DQN}) to perform Q-learning. 

The \codeobj{outputval} is less complex than the inputval, and only contains the latest result of the inference. When its \codefunc{update}-method is called, it adds the action to the \codeobj{inputval} (because the corresponding action to the environment could otherwise not be known) and goes on notifying a \codeobj{sender\_thread} that new information can be sent to Unity in its method \codefunc{send\_via\_senderthread}. This method is also called when the client needs to be paused (by sending \codeother{"pleaseFreeze"}), needs to continue (\codeother{"pleaseUnFreeze"}) or needs to reset (by sending \codeother{"pleasereset"}). The function \codefunc{resetUnityAndServer} combines sending the reset-command to Unity and resetting \codeobj{inputval} and \codeobj{outputval}.

\subsubsection{Agent-independent features}

All of the above explained functionality is necessary for the agent to learn the given game, and has turned out this way due to certain design choices (like the agent deciding itself when to reset the environment) and peculiarities of the necessary proprietary communication with the game. As the game is its own thread, it is constantly running and does not wait while the agent calculates the action (the space between \codefunc{send(data)} and \codefunc{send(content)} in the leftmost column of figure~\ref{fig:sequenceserver}). While this makes it necessary for the agent be quick, it provides the advantage that a user inspect the live environment and inspect it at will. If inside Unity the \keystroke{H}-key is pressed, the actions the agent suggested are overwritten, however the agent still gets input from the environment. If an agent features a GUI, the Q-values and rewards of the states a user drives will be printed to screen, including the agent's suggested action.

When talking about the \codeobj{inputval}, it was mentioned that it contains as much information as an agent \textit{could} know. In this implementation, there are some features that are clearly specific to an agent, while others are general settings indepent of the current agent. Those are stored in an instance of the class \codeobj{Consts}, defined in \filename{config.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/config.py}}, while the settings of the agent are saved in the respective file of the agent. Note that the value \codeother{msperframe}, which specifies in what interval the agent performs actions, must always equal to its correspondance in Unity, \codeother{Consts.updatepythonintervalms}. The value for the maximum number of history frames, \codeother{history\_frame\_nr}, is currently set to four. When running the \filename{server}, an instance \codeobj{conf} of type \codeobj{Config} is created. The reference to this object is passed to basically all threads and objects, such that they can access all their settings clearly.


\subsubsection{Agents}

Until it gets to the agent, neither the observation nor the reward is set. Thus, the agent needs to specify its own functions for its observation (given environment-state) and reward (given state and action). Because of this, some methods of an agent receive the \codeobj{gameState} instead of the \codeobj{agentState}, in contrast to other approaches. Furthermore, the server notifies the agent of specific events that happened in the game, to which the agent could react by resetting the environment and starting a new episode. These methods, next to the obvious ones of performing an action or conducting a learning-step, must thus be specified by an agent. When describing the agent in this section, only those functions are relevant, as the inner workings of the server are hidden to the agent.

\paragraph{Inheritance relation}

To ease the implementation of agents, they all inherit from a superclass called \codeobj{AbstractAgent} as well as a class termed \codeobj{AbstractRLAgent}, both of which are defined in \filename{agent.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/agent.py}}. The key idea is, that all functions and features mutural to all agents are set in these classes, such that any specific agent must only implement/overwrite the functions in which it differs from its superclasses. Because not all agents use reinforcement learning, it was decided to use two superclasses instead of only one -- the class \codeobj{AbstractAgent} does for example not specify a memory, as a purely pre-trained agent does not need it.

In the scope of this thesis, multiple agents where developed, using different models, vectors and approaches. In the final version of the implementation there are five exemplary agents left, which can all be found in the directory \filename{agents}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/tree/master/agents}}. 

Figure~\ref{fig:agentMINI} depicts the inheritance-relation of an agent and its super-classes using an UML-diagram.
\begin{figure}[h!]
	\centering 
	\includegraphics[width=\textwidth]{uml_diagrams/agent_inText.1}  
	\caption[UML-diagram of an agent and its super-classes]{incomplete UML-diagram of an agent and its super-classes. A more complete version can be seen in \colorbox{red}{appendix~\ref{BLABLABLA}}}
	\label{fig:agentMINI}
\end{figure}

To add a new RL-agent, a class extending \codeobj{AbstractRLAgent} that specifies at least \codeother{name}, \codeother{ff\_inputsize}, an initialized \codeobj{model} and the function \codefunc{policyAction(agentState)} must be added to the agents-directory, an exemplary agent that does so is listed in appendix~\ref{AppendixD}. The agent's \codeother{name} is as directory-name to save its data to. A non-rl agent must only provide the method \codefunc{policyAction}, next to any kind of model and, if needed, a \codefunc{preTrain}-method.

To use any agent, its filename (without extension) must be supplied as argument when running the server, such that it gets automatically imported and initialized: \codeother{python server.py --agent ddpg\_rl\_agent}.

\paragraph{Agent-state and game-state}

Most of the functions an agent specifies are only for internal use -- the only ones which are called from outside (by the server) are the ones for initializing the agent, performing an action, reacting to (reset-) commands and performing pretraining.

As can be seen in the diagram, some methods require a \codeobj{gameState} (or \codeobj{pastState}) as argument, whereas others require an \codeobj{agentState}. Inside the implemention, they are both clearly defined. A \codeobj{gameState} is what is provided by the server, a tuple of $\langle$\codeobj{visionvectorHistory}, \codeobj{visionvector2History}, \codeobj{otherinputHistory}, \codeobj{actionHistory}$\rangle$. This tuple is used to calculate the agent's observation in the function \codefunc{getAgentState(gameState)}. An agent-state is defined as a tuple consisting of $\langle$\codeobj{convolutionalInputs}, \codeobj{otherInputs}, \codeobj{standsInput}$\rangle$. If used by an agent, \codeobj{convolutionalInputs} consists of stacked 2D-matrices originally stemming from the minimaps, whereas all non2D-input given in \codeobj{otherInputs}. It was decided to seperate the \codeobj{convolutionalInputs} from the \codeobj{otherInputs} such that hybrid ANNs, having both a convolutional as well as a feed-forward component, have a distinct input for either. A distinctiveness of this implementation to others is the additional usage of \codeobj{standsInput}, which is a boolean that is only true if the car currently stands. When having such an input, models can easily be hard-coded to prevent situations in which the car stands undesirably still. I will not provide the internals of the function to create the agent's observation (\codeobj{agentState}) from the \codeobj{gameState}, as it depends on the agent's \term{features} and is a result of experimentation, as such described in chapter~\ref{ch:implementationresults}.

Next to \codefunc{getAgentState(gameState)}, there are several other helper-functions specified in \codeobj{AbstractAgent}, like for example \codefunc{getAction(gameState)}. This function returns the agent's last action in a way that can be used by the agent's model. This function is necessary, because agents may use a non-continuous model (like DQN), for which the action must be discretized at first. As the \codeobj{AbstractAgent} was implemented with DQN-based agents in mind, it provides the function to discretize and dediscretize the action, which wraps such a function from the file \filename{read\_supervised.py}. The function \codefunc{makeInferenceUsable(state)} combines those functions for certain kinds of states, such that it is the only one needing to be called whenever the agent accesses its \codeobj{model}.

\paragraph{Methods called by the server}

In its \codefunc{\_\_init\_\_}, called when \codeobj{server} creates the \codeobj{agent}, the classes \codeobj{AbstractAgent} and \codeobj{AbstractRLAgent} define some assumptions about about the configuration of its \term{features}. These variables can be overwritten by its subclasses if their actual configeration differs, such that these variables are reliable information about the agent's configuration of \term{features}. When initializing its memory or its model, the agent also passes a reference to itself, such that the configuration-variables are accessible from inside those objects. 

The server calls not only the \codefunc{\_\_init\_\_}, but also the \codefunc{initForDriving}. In this function, the agent's memory is (according to its \term{features}) initialized, as well as an \term{evaluator}. The evaluator is an independent object, specified in \term{evaluator.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/evaluator.py}}. It provides functions to save information about the agent's driving performance in the form of running averages of the agent's rewards, q-values, progress per episode or others to an XML-file. The result of the respective file can be inspected by running \codeother{python show\_plot.py --agent AGENT}, which was also used to produce the plots from chapter~\ref{ch:resultsanalysis}. If the application is not run with the argument \codeother{-noplot}, the evaluator also plots these running averages live, using the library \codeobj{matplotlib} (note that the number of iterations as stated by the live-plotter restarts every new run). The agent uses the evaluator by calling its method \codefunc{eval\_episodeVals}, passing respective parameters. The performance of a human can be evaluated by using any kind of agent that uses the evalutor and overwriting the agent's actions by pressing \keystroke{H}. An exemplary plot can be inspected in figure~\ref{fig:plot}.

During runtime, the agent's \codefunc{performAction(gameState, pastState)} is called. It is provided by \codeobj{AbstractAgent} (and overwritten by \codeobj{AbstractRLAgent}), and does not need to be changed by agents in the general case. The function checks if an action should be taken, converts the given \codeobj{gameState} into an \codeobj{agentState} and provides the action. Where this action is taken from is decided in this function -- it can use the previous action (a variable \codeobj{action\_repeat} is specified in it with similar reasoning than that of the DQN\cite{mnih_human-level_2015}), a random action (by calling \codefunc{randomAction(agentState)}) or an action according to its model (\codefunc{policyAction(agentState)}). The implementation given in the \codeobj{AbstractRLAgent} assumes a simple $\epsilon$-greey approach, such that \codefunc{randomAction} with a probability $\epsilon$, where epsilon decreases over time. In agents where it is desired to overwrite the this exploration method, either the \codefunc{performAction}-function can be overwritten, or (as done in the provided DDPG-based agents) the \codefunc{randomAction} simply returns \codefunc{policyAction}, and the actual exploration-technique is integrated into the \codefunc{policyAction}-function. 
\codefunc{performAction(gameState, pastState)} cares for updating the server's \codeobj{outputval}, such that all issues of communication (next to eg. having to work with the \codeobj{gameState}) are abstracted away from \codeobj{policyAction} and \codeobj{randomAction}. 

As mentioned, the server does not only call the agent's \codefunc{performAction}, but also its function \codefunc{handle\_commands(command)} if it got a \textit{special command} from the client (environment). The \codefunc{handle\_commands}-function gets as input the string providing information about what this command was. In the current implementation, the existing commands are \codeobj{"wallhit"}, \codeobj{"lapdone"}, \codeobj{"timeover"} and  \codeobj{"turnedaround"}. The default reaction, specified in \codeobj{AbstractRLAgent} is to end the training episode (with \codefunc{endEpisode(reason, gameState)}) for either of those commands -- after explicitly giving negative reward for the last seen state using its function \codefunc{punishLastAction(howmuch)}. The necessity of these two functions is an artifact of the design decision to let the agent decide on when an episode ends. In this implementation, the main loop is not nested in an outer loop that recognizes when an epsiode end, instead the agent is only notified of the end of an episode after it stored the last state already in its replay memory. Using those two functions, the last addition to that can be changed in retrospect, such that the flag $t == t_t$ can be set to \codeother{true}, indicating a final state. \codefunc{endEpisode} also calls the evaluator to save and plot the respective running averages.

All \term{features} that are not needed for supervised agents are only specified in \codeobj{AbstractRLAgent}. This includes the methods to calculate reward, performing random actions, managing its replay memory and reinforced learning.

As especially \codefunc{calculateReward(gameState)}, \codeobj{randomAction(agentState)}, \codefunc{handle\_commands(command)} and \codeobj{policyAction(agentState)} are considered \term{features} of the agent, they are explained in chapter~\ref{ch:implementationresults}. 

An agent using a DDPG- or DQN-model requires a replay memory, such that its ANN can learn from uncorellated samples. The function \codefunc{addToMemory(gameState, pastState)} adds an agent's state to its memory, and the function \codefunc{learnANN} accesses its values, converted into a usable shape by \codefunc{create\_QLearnInputs\_from\_MemoryBatch}. 

\paragraph{Learning}

It is possible to use the \codeobj{"parallel"} learn-mode or the \codeobj{"between"} learn-mode (as specified in \codeobj{conf}). If the learning mode is set to the latter, \codefunc{performAction} starts the execution of a specified number of learning steps in a specified interval, according to \codeobj{conf}'s configuration of \codeobj{ForEveryInf} and \codeobj{ComesALearn}. This corresponds basically to the \term{update frequency} of the DQN\cite{mnih_human-level_2015}. Learning is specified in the method \codefunc{learnANN()}, itself executed by \codefunc{dauerlearnANN(steps)}. To perform the actual learning, the former extracts a batch of a specified number of transitions from its replay memory using \codefunc{create\_QLearnInputs\_from\_MemoryBatch} and performs the model's method \codefunc{q\_train\_step} on the extracted batch. 

Performing a q-train-step takes significantly longer than performing an inference. While the given agents are fast enough to perform an inference-step in between two updates with new information about the game (everything that happens in the main loop between the Unity-client sending data and doing it again in figure~\ref{fig:sequenceserver}), they loose a significant amount of time when also having perform a train-step. Because of this, additionally to Unity's client automatically pausing the game if Python's response is delayed too much (see section~\ref{sec:codeCommunAgent}), the class \codeobj{AbstractRLAgent} includes methods ask the client to freeze itself. In doing so, it uses another string-array \codeobj{freezeInfReasons} to which a reason is appended for every call of \codefunc{freezeInf(reason)}, and popped for every call of \codefunc{unFreezeInf(reason)}, with similar reasoning than described for the QuickPause-functionality of the game. If the respective conditions apply, a message \codeobj{"pleaseFreeze"} is sent to the client, such that the agent has time to perform its learning-steps. Once it is done, it notifies the client by sending \codeobj{"pleaseUnFreeze"}. As it was a design decision that the game runs as smoothly as possible, it was decided to not rigidly perform one learning-step every 
\term{update frequency} steps, but to increase the respective batchsizes (for example $100$ learn steps every $400$ steps), such that the game is not constantly interrupted due to learning steps. 

Because the agent was also developed with the learn mode \codeobj{"between"} in mind, respective functions can also be found in the agent and server. If this mode is set, then the method \codefunc{dauerlearnANN} is run by the server in a separate thread (started in its main-method). To keep the ratio of learning steps and inference roughly constant at all times, the methods \codefunc{checkIfAction} and \codefunc{dauerLearnANN} provide functionality to freeze either learning or inference, if the respective thread was faster than the other. For that, a \codefunc{freezeLearn}-method is necessary, which sets a respective boolean value to \codeobj{false}, stopping the learning. A ratio of $4:1$, as already done in the original DQN, showed to decrease the amount of time any thread must be frozen the most.


\paragraph{GUI}

As mentioned before, the implementation also features a basic GUI, to which current information about the agent's latest values can be sent to. The typical view of this GUI is shown in figure~\ref{fig:gui} in appendix~\ref{AppendixB}. For a respective agent to use this GUI, some functions must be overwritten to also include those printing-functions. To do so, an agent must import the infoscreen, such that itsfunctions can simply call their respective counterpart of the agent's super-class and print its result to the infoscreen using \codefunc{infoscreen.print}. The functions that are overwritten to show a GUI as in figure~\ref{fig:gui} are \codefunc{eval\_episodeVals}, \codefunc{punishLastAction}, \codefunc{addToMemory}, \codefunc{learnANN}, \codefunc{policyAction} and \codefunc{calulateReward}. Most agents of the given implementations to so, the reader is for example referred to \filename{agents/ddpg\_rl\_agent.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/agents/ddpg_rl_agent.py}}. Note that an exception is the added code of \codefunc{calculateReward}, which does not update a text-element of the GUI, but a color. This color is a gray-scale value whose saturation depends on a scalar value between zero and one. This feature is useful to inspect if the agent's reward-function is useful -- the lighter the color-area, the higher the current reward.

\subsubsection{Memory}

During its learning process, the agent stores every transition it encounters in its replay memory. Following the tradition of the DQN\cite{mnih_human-level_2015}, it is a buffer of limited size, consisting of tuples $\langle s_t, a_t, r_t, s_{t+1}, t==t_t \rangle$. This buffer is wrapped in a class of type \codeobj{Memory}, specified in either in the file \filename{inefficientmemory.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/inefficientmemory.py}} or \filename{efficientmemory.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/efficientmemory.py}}. Note that while the agent's \codefunc{addToMemory(gameState, pastState)} is called with the \codeobj{gameState}, this function converts both \codeobj{gameState} and \codeobj{pastState} to the respective \codeobj{agentState} to save on its memory consumption. Adding to its memory is the only situation where the agent's function \codefunc{calculateReward(gameState)} is called. The memory contains methods to change the last stored transition's reward in retrospect as well as to update if the episode ended in the last step -- the values for reward and if the state was terminal in \codefunc{addToMemory} are thus possibly only provisional.

When performing a q-training-step, a random sample of size \codeobj{conf.batch\_size} is sampled from the replay memory. It is established that \term{Prioritized Experience Replay}\cite{schaul_prioritized_2015} increases the agent's speed of learning, but while the implementation allows for such an addition, nothing comparable is done in the actual implementation. 

The memory-classes of this implementation can be saved to a file using Python's \codeobj{pickle}-library. The memory is generally saved together with the agent's model, using its function \codefunc{save\_memory()}. Because the memory can become very large (consisting of hundreds of thousands of transitions), both inference and learning are frozen while the memory is saved. To be more failsafe, the agent is always stored twice (with both files containing the exact same memory). If the computer runs out of storage while one of these files is being created, the respective file is corrupted -- however one backup-copy will always be save.

In the given project, there are two memory-classes provided which both containing the same methods, where the one (\filename{efficientmemory.py}) is used by certain agents, as it is more efficient in those ones, namely the ones using \codeother{history-frames}. 
The reason for this is the following: 
Let's assume an agent's observation consists of the current gameState as well as the three ones before that, corresponding to four history-frames: $agentstate_t = (state_t, state_{t-1}, state_{t-2}, state_{t-3})$. A tuple added to the memory contains its current state and the state before that: $agentstate_t$ and $agentstate_{t-1}$, which corresponds under the previous definition of an agent's state to the following environment-states: ($state_t, state_{t-1}, state_{t-2}, state_{t-3}$) and ($state_{t-1}, $ $state_{t-2}, state_{t-3}, state_{t-4}$). The eight saved states contain in fact only five different states. In fact, states are even more redundantly saved, as the next inference of this hypothetital agent saves the following states: 
$state_{t+1}, state_{t}, state_{t-1}, state_{t-2}$ and $state_{t}, state_{t-1}, state_{t-2}, state_{t-3}$, where only one state is in fact new. It is easy to see, that an implementation that always adds $agentstate_t$ and $agentstate_{t-1}$ is very inefficient, using roughly 8 times as much storage as needed. The class specified in \filename{efficientmemory.py} works around those problems by always adding only the latest frame to the replay-memory. While this file is internally far more efficient than the inefficient memory for agents using many history-frames, it can be interfaced just like \filename{inefficientmemory.py}, such that they easily be exchanged, providing the exact same values in all situations\footnote{It could be argued that the memory is different after a reset if it only adds the latest part of the transition, however a solution for that problem is simply overwriting the latest transitions in that case. A proof that both memorys behave exactly equal is given in an older branch of the repository: \url{https://github.com/cstenkamp/BA-rAIce-ANN/tree/newmemory}}.
If an agent uses many history-frames (especially relevant when using minimaps as input), it can load the efficient memory in its \codefunc{initForDriving}-function. If no such is given, the inefficientmemory is loaded by default. 

\subsubsection{Pretraining}


As stated in section~\ref{sec:exportdata}, not only the possibility for reinforcement learning is given in this implementation, but the game also contains functionalities to record manual driving data and export those via XML. By running the file \filename{pretrain.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/pretrain.py}} with the respective agent as command-line parameter (see server), all exported data which has the file-ending \filename{.svlap} and is stored in the directory \codeobj{conf.LapFolderName} is used to pre-train the agent.

The data the game exports for pretraining consists of the \term{vectors}, corresponding to the game's state, for every point in time it recorded -- because the data is independent of individual \codeobj{agentState}s, it can be used by all agents. Additionally, the data in the XML saves the time of recording (system's time as well as Unity-Time), speed, progress, and explicit user-input (\codeobj{throttlePedalValue}, \codeobj{brakePedalValue}, \codeobj{steeringValue}). The file \filename{read\_supervised.py} contains (among others) the necessary classes and methods such that this XML-data can be converted into a format an agent can use for pretraining. For that, it contains a definition of a \codeobj{TrackingPoint}-class. This class is a counterpart of the game's class of the same name, defined in \filename{AiInterface.cs}. Furthermore, \filename{read\_supervised.py} contains a class \codeobj{TPList}, which corresponds to a complete usable dataset of driving-data.

Upon execution of \filename{pretrain.py}, the dataset \codeobj{trackinpoints}, an instance of type \codeobj{TPList}, is created. In its \codefunc{\_\_init\_\_}, it reads all \filename{.svlap}-files and creates a \codeobj{TrackingPoint} for every point in time saved in either of those files. There are two specific characteristics in this implementation. First of all, it seems natural to provide a \codeobj{TrackingPoint} with state an corresponding action from the same point in time. However, all actions from the server will be delayed due to the time of sending the state, deciding on an action and executing it. Because of that, the function \codefunc{condider\_delay} remaps an action to a gameState a few milliseconds before the respective action. The second characteristic is, that supervised data is generally exported in a higher frequency than what the agent performs at. The millisecond-time between two datapoints is saved in the XML, such that only every $n^{th}$ datapoint will be used using the function \codefunc{extract_appropriate}. This is performed separately for each file, such that the dataset consists of samples with equal distance of time. 

To iterate over its contents, the class \codeobj{TPList} provides the methods \codefunc{reset\_batch} and \codefunc{next\_batch}. In its main method, the application \filename{pretrain.py} calls the method \codefunc{preTrain} of an agent, passing a \codeobj{TPList}-dataset. This function, as specified in \codeobj{AbstractRLAgent}, loops over the entire dataset for a specified number of iterations. Every iteration, it takes minibatches of size \codeobj{conf.pretrain\_batch\_size} with the method \codefunc{next\_batch}, to be used as batch for the model to learn on. While it can be specified with the command-line-argument \codeother{"-supervised"} to use the model's \codefunc{sv\_train\_step}-function instead of the \codefunc{q\_train\_step}-method, only the implemented DQN-model includes this function. To convert a batch of \codeobj{gameState}s (as provided by \codefunc{next\_batch}) into a batch of \codeobj{agentState}s, the agent uses the dataset's static function \codefunc{create\_QLearnInputs\_fromBatch}, passing a reference to itself. Passing its reference is necessary because this method uses the agent's observation-functions on the batches from the dataset.

In the actual implementation, the \codeobj{AbstractRLAgent}'s \codefunc{preTrain}-method is implemented a bit differently, the agent cannot learn useful policies with q-training on an existing dataset created by a completely different policy.


\colorbox{red}{dass pretrian.py auch ne fake\_real (aka vom memory) methode hat}


\subsubsection{Models}

The class \codeobj{AbstractAgent} does not access any functions of the agent's model. Thus, an agent that only extends this class can use any kind of model in its \codefunc{policyAction(agentState)}-function. The \filename{random\_agent.py}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/agents/random\_agent.py}} for example simply assigns random values as its actions.

An agent however that extends \codeobj{AbstractRLAgent} must provide a model that implements the (because of python's duck typing only hypothetical) interface specified in figure~\ref{fig:modelsInt}. In the functions specified in that diagram, the abbreviations [s], [a], [r] and [t] stand for arrays of \codeobj{agentState}, \codeobj{action}, \codeobj{reward} and the boolean flag $t = t_t$ (\codeobj{terminal}). The methods provided by the interface are all the ones necessary to perform adequate Q-learning.

\begin{figure}[h!]
	\centering 
	\includegraphics[width=.4\textwidth]{uml_diagrams/models_inText2.1}
	\caption{UML-diagram of the interface a model must implement}
	\label{fig:modelsInt}
\end{figure}

It is the responsibility of a model to save the information about how many inferences it already conducted as well as its number of learning steps for pretraining as well as reinforced training. To differentiate the former from the latter, both models contain a flag \codeother{isPretrain}, which is passed upon initialization and specifies if the pretrain-episodes and steps are assigned to the respective counters for pre-training or actual training. In the \codeother{isPretrain}-mode, the model cannot perform inferences and must therefore be re-created with out the \codeother{isPretrain}-flag. If the model is initialized by the server, the flag is set to false, whereas if the model is initialized by the application \filename{pretrain.py}, it is set to true.

When an agent initializes its model, it must pass a reference to itself to it. The model thus \term{know}s the model and can access some of its variables representing the configuration of its features.


\subsubsection{Functionality provided by AbstractRLAgent}

As can be seen in figure~\ref{fig:agentMINI}, an agent that extends the class \codeobj{AbstractRLAgent} only needs to specify a model (implementing the interface \ref{fig:modelsInt}), a name, the feed-forward inputsize for its model and the method \codefunc{policyAction(agentState)}. This is because a basic version all other methods, even though they are \term{features} of the agent, is already implemented in the \codeobj{AbstractAgent}. 

The following lists describes situations in which some of this functionality must be changed.
\begin{itemize}
	\item An agent that uses any kind of model is likely to overwrite the \codefunc{\_\_init\_\_}-function to initialize its model. 
	\item An agent that uses a GUI must overwrite the functions described in the respective section. 
	\item If the agent features a different observation-function than provided, it must overwrite the function \codefunc{getAgentState(gameState)} and possibly \codefunc{makeNetUsableOtherinputs}. 
	\item If the agent's model does not discretized, an agent must overwrite the method \codefunc{makeNetUsableAction}.
	\item If the agent uses another exploration-technique than $\epsilon$-greedy, it must either overwrite \codefunc{randomAction(agentState)} or \codefunc{performAction(gameState, pastState)}. This may include changing \codeobj{startepsilon}, \codeobj{minepsilon} and \codeobj{finalepsilonframe}.
	\item If the agent uses another method to calculate random actions than the one provided, it must overwrite \codefunc{randomAction(agentState)}.
	\item If the agent calculates the reward in a different manner than provided, it must overwrite the method \codefunc{calculateReward(gameState)}.
	\item If the agent performs pretraining differently than specified, it must oferwrite the method \codefunc{preTrain(dataset, iterations, supervised)}.
	\item If the agent uses another memory, it must assign \codeobj{self.memory} in \codefunc{initForDriving}.
	\item If the agent is supposed to reset the environment due to different events, the method \codefunc{handle\_commands(command)} must be overwritten. This may include changing \codeobj{wallhitPunish}, \codeobj{wrongDirPunish} or \codeobj{time\_ends\_episode}.
\end{itemize}

In the next chapter, I will provide the implementation of the agent's \term{features}. For that, I will start with \term{possible vectors} that are sent to any agent and can be used. Afterwards, I will explain the agents that where implemented in the scope of this thesis. I will explain the \term{features} of all agents, including why they where selected that way. These features include the used \codeobj{model}s, the exploration-functions, their specific observation-functions (using the \term{possible features}), their methods of incorporating pretraining as well as their reward-functions. 

It is worth to note, that some of these functions (as for example the \codefunc{calculateReward(gameState)}-function) are implemented not in the individual agent, but in \codeobj{AbstractRLAgent}. This is however only done for convenience, such that these methods do not have to be implemented the same way several times.



%[dass die alle plotter HABEN und die serverkompomenten über containers KENNen etc]... doch nochmal in nem großem uml-diagram? dann auch als notiz rein dass bei nem actual agent der gameState weg-abstrahiert ist ... und inklusive agent has evaluator, evaluator knows agent, ... same for memory, ...

%[die architektur von meinem duelDQN muss nochmal inen eigenen graphen yay]

%%-alle FOREVERYINF schritte führt der agent dann COMESALEARN learn-schritte durch (definiert in der dauerLearnANN/learnANN des agents.py, welche wiederum q_learn von ddddqn.py callen)
%
%Was der Fehler sooooo lange war: Das Auto fährt immer direkt gegen die Wand. Im laufe der Exploration fährt es halt anfangs ein bisschen random, unter anderem den schnellsten weg an die Wand (1, 0, 0.875). Nach dem trianing denkt es sich dann hey, schnellster weg gegen die wand, perfekt, genau das will ich. Hat durchgehend den höchsten Q-wert, vom losfahren bis direkt vor der wand stehen.
%
%
%State    action    reward  state2  is_terminal
%0.253   (1, 0, 0.857)   1.085   1.052   False
%1.052   (1, 0, 0.857)   1.07488 1.965   False
%1.965   (1, 0, 0.857)   1.06112 2.989   False
%2.989   (1, 0, 0.857)   1.0428   4.118   False
%4.118   (1, 0, 0.857)   0   5.334   False
%5.334   (1, 0, 0.857)   0   6.626   False
%6.626   (1, 0, 0.857)   0   7.955   False
%7.955   (1, 0, 0.857)   0   9.322   False
%9.322   (1, 0, 0.857)   0   10.0  False
%10.0   (0, 0, 0.857)   -5   10.0   True




%
%[tf gibt automatic differentiation, multi-gpu support, python interface]
%


%mehr pseudocodes!!!!
%TODO nochmal auf den program flow im vergleich mit dem DQN-pseudocode im anahng eingehen; UNBEDINGT

\colorbox{red}{nochmal auf den program flow im vergleich mit dem DQN-pseudocode im anahng eingehen; UNBEDINGT}

% TODO dden -help command-line parameter erwähnen