\chapter{Program Architecture}

\label{ch:program}

\newcommand{\term}[1] {{\spaceskip=.95\fontdimen2\font minus \fontdimen4\font
	\xspaceskip=0pt\relax \large\texttt{#1}}}


[irgendwas von wegen "general to specific"] 


\section{Characteristics and design decisions}

As stated in chapter~\ref{ch:rlframeworks}, the usual framework for solving reinforcement learning tasks is fairly rigid. However, the task of this work differed in some respects from the usual implementation of a reinforcement learning agent, which led to the necessity of differing from the usual framework in numerous situations. In the following sections, I will provide an overview of the difference between this project and other work in the reinforcement learning domain. Furthermore, I will discuss some of the design decisions that found their way into the final version of the program as well as challenges that occured during its implementation and their respective solution. For that, I will explain general principles in the first section of this subchapter, and go into detail about some of the specific details about the implementation in the subsequent section.

\subsection{Characteristics of this project} \label{ch:projectcharacteristics}

There are several differences in this project than in known literature in the reinforcement learning community. The most important difference to the agents presented in chapter~\ref{ch:RL} is, that those are developed as general-purpose problem solvers, with the intention to solve any arbitrary task given to them. In this approach, that is not the case -- the goal is to solve the specific, given game. This allows in principle to incorporate expert knowledge of the task domain, to for example forbid certain combinations of the output or to use specific types of exploration, which are specifically useful in this scenario. Specifically, the game which is supposed to be played is a racing game, which has implications in several domains, for example making the standard $\epsilon$-greedy approach for exploration much worse as in many other domains. Next to that, the game which is supposed to be played is, in contrast to games solved in openAI's gym-environment (\ref{ch:rlframeworks}), live. While the game could in theory be manually paused for every RL step, it is worth trying to let the agent run ``live'', such it runs fluently and its progress can manually be inspected. Another challenge was the fact, that the game is coded in a programming language for which no efficient deep learning architectures exist, which led to the necessity of a propietary communication between the environment and its agent.

The fact that there is only one game, which is supposed to be learned, also extended the entire problem domain: While usually the focus of developing RL agents can lie purely on the agent, assuming that the environment and consequently its definition of state/observation and reward is fixed, the focus of this work was to learn this one game as well as possible -- in other words it is also necessary to test what a useful definition of observation or reward looks like. Many of the subsequent design decisions are made with the idea in mind, that it must be as easy as possible to compare different agents using a unique combination of state-definition, reward-definition, model and exploration technique. 

Furthermore, the question of how important supervised pretraining is addressed in this thesis. For that, the game must provide a way of recording manual actions and their corresponding state and to record that in a way, that a learner can read it and train on this data. The natural questions arising are how good an agent relying purely on supervised training is in comparison to others, as well as if there is a way to combine pre-training with reinforcement training.

%differences from mine to others with their implications
%-geht nur um 1 spiel 
%	-settozero sinnvoll (expert knowledge)
%	-andere exploration möglich
%-spiel ist live (nochmal im diagramm zeigen)
%	-dass das härter für den agent ist/er schneller ist
%	-humanTakingControl möglich, man kann sich Q-werte/rewards/.. in situationen angucken
%-verschiedene sprachen
%	-proprietäre kommunikation
%	-spiel ändert sich ebenfalls, was anderes als proprietär wäre dumm
%-human taking control ("wie sind die q-werte vor der wand",..)
%-geht um ein RENNspiel 
%	-epsilon-greedy sucks
%	-sieht tendenziell viel mehr den anfang
%- dieses EINE spiel soll möglichst gut gelernt werden
%	-geht ebenfalls darum was für vektoren sinnvoll sind (freier als torcs)
%	-geht ebenfalls darum was für reward definitionen sinnvoll sind
%	-wie viel FPS er hbaen muss
%	-agent ist für resetten verantwortlich
%	-state und reward gehen nicht fix an python, sondern agent sucht aus
%		-sowas wie annealing reward möglich
%		-man könnte sogar den reward selbst lernen lassen anhand von "mit welcher reward-funktion minimiert er nach 2millionen iterationen die rundenzeit am besten"
%	-Pretraining möglich, vergleichbarkeit mit pretraining, kombination
%
%
%Dinge die also verglichen werden
%	-reward
%	-inputs
%	-model
%	-was den state beendet
%	-exploration techniques
%	-pretraining ja/nein


\subsection{Characteristics of the game}

%at the end of this section a clear distinction between agent and environment should be known. Where does the agent stop, where does the environment begin. 

%-3 actions as input (and braking is necessary)
%-no opponents
%-realistic physics
%-slip
%-dass throttle und brake nur torques applien, was halt realistisch ist, thanks unity
%-indeterministic
%-partially observable
%-huge, non-tabular solvable, continuous state space
%-of course, at best, contiuous action space
%-goal is to finish 1 round in as short time as possible, while staying on track
%-der agent ended mit den actions - das auto ist teil der environment
%-track consists of track, off-track and curb with different friction properties

The given game is a simple but realistic racing simulation. As of now, it consists of one car driving one track. While it is certainly necessary to implement additional AI drivers or different tracks, no such thing is considered in the current implementation. The main focus of the given simulation lies on realistics physics. There are important differences to simpler racing games, like many of those implemented for the \term{Atari}-console, on which the original \term{DQN} was tested.

The input expected from an agent consists of three continuous actions: The \keyword{throttle} increases the simulated motor-torque, which leads faster rotation of the tires and thus applying forward force to the car, which then accelerates general case. The \keyword{brake} simulates a continuous brake force slowing down the rotation of the tires. It is not possible to finish a while constantly accelearing as much as possible without breaking. It is important to note that slip and spin is also handled in the game: While the rotation of the tires can be accelerated or decelerated fairly abrupt due to their small mass, the car itself has a higher mass and thus more intertia, with forbids abrupt changes in movement. As the cars are rigidly attached to the car, they lose grip on the street, which lessens the impact of consequent forces applied to the tires. The last command an agent must output is the \keyword{steering}, which turns the front tires of the car, leading the car applying force to the respective side the tires steered towards.

It is the task of an agent to provide commands for the values of throttle, brake and steering, which are continous in their respective domains: $throttle \in [0,1]$, $brake \in [0,1]$ and $steer \in[-1,1]$. Of course, as is the general case in reinforcement learning problems, the agent does not know the implications of its actions in advance but must learn them via trial and error. It should however also be kept in mind that the agent provides those actions \textit{to the car}, and that the car must be seen as part of the environment as well, as the actions do not have a reliable impact on its behaviour. For instance, if the car is moving at fast paces, hitting the brake will not lead to an abrupt stop, as the car's inertia still applies a forward force. Also, the impact of the steering changes with the movement of the car. For example, the turning circle of the car has a much slower radius in smaller speeds. Another consequence of the realism of the physics implemented in this simulation is the fact that simultaneous braking and steering at high velocities has almost no effect: Because of the high speed, the car has a strong forward force. That leads in combination with the braking to the front tires slipping a lot, which reduces the impact on forces applied to them on the actual car -- it will not follow the direction of the steering but continue its forward pace determined by the stronger force, namely the inertia.

The track itself is a circular course with a combination of unique curves, requiring the car to steer left as well as right. Along every point of the course, there are three different surfaces with different frictions -- the street itself provides the most grip, while the off-track surface is far more slippery. Between the track-surface and the off-track-surface there the curb of the track which manifests as a small bump with separate friction properties. At a distinct distance from the middle of the track there is a wall which cannot be traversed.

\subsubsection{The game as a reinforcement learning problem}

As detailed in chapter~\ref{ch:RL}, reinforcement learning agents are solvers for (possibly only Partially Observable) Markov Decision Processes with unkown transition relations. In other words, for the agent to successfully learn how to operate in an unknown environment, this environment must correspond precisely to a tuple of $\langle S, A, P, R, \gamma \rangle$ (details explained in section \ref{ch:mdps}). Because in this simulation only the case of a single agent without any other cars on the track is considered, the racing problem can be formalized as a Markov Decision Processes with a similar reasoning than \citet[chapter 4]{wymann_torcs_2000} put forward for \term{TORCS}. As is the general case in simulations, while every update-step of the physics aims to simulate a continous process, those updates must be discretized in the temporal domain. As however both agent and environment run live, the temporal discretization of the agent can not always correspond to that of the environment if an update-step in the agent takes longer than in the environment. To put that into numbers, the fixed timestep for the environment's physics is at 500 updates per second, while the agent discretizes to maximally 20 actions per second.

As mentioned in the previous section, the action space required by the agent is continuous with $\mathcal{A} \subset \mathds{R}^3$. 

The environment's state is a linear combination of different factors, as for example the car's speed, absolute position, current slip-values and much more. While certainly finite, it consists of many high-dimensional values $\mathcal{S}_e \subset \mathds{R}^\mathds{N}$. Because of certain factors in the implementation of the environment itself, the environment is indeterministic\footnote{The game is programmed in Unity, which has non-predictable physics. As discussed for example in \url{https://forum.unity3d.com/threads/is-unity-physics-is-deterministic.429778/}, this is because of the fact that any random-number-generator depends on the current system time, which is never fully equal in subsequent trials. Because the calculation of some states can be more complicated than others, this effect can snowball even more -- longer calculation in one step leads to later timing of a subsequent update step, which can lead to a whole other trajectory along the state space, even though the start state was equal.}. As can in principle be argued that the environment's state corresponds to all information stored in the game's section of the computer's RAM, the game trivially fulfills the markov property. As the environment is only a single-agent system, the transition function of the environments follows directly from state and action: $\mathcal{S}_e \times \mathcal{A} \rightarrow \mathcal{S}_e$.

In the basic definition of the game, there is no formal definition of a reward returned by the enviroment. While it could in theory be argued, that the inverse of the time needed to complete a lap could be taken as the reward, it is obvious that it is infeasible due to many reasons: Finishing one lap takes several seconds, which means there are easily hundreds of iterations between the start state and this final state. Next to the obviously arising \keyword{credit assignment problem}, the chance of an agent even getting to such a final state, without all intermediate rewards equal to zero, is practically zero. Instead, it makes sense to give more dense rewards. As mentioned in section~\ref{ch:projectcharacteristics}, the reward also is subject of experiments in the scope of this thesis. For the game to correspond to a proper environment, this reward must be a scalar value, depending on state and action.

Though it is in theory possible to implement an agent that takes the entire underlying state of the environment as its input, an approach like that is far from feasible, as this state also contains much information only necessary for rendering the game. Instead, in the chosen approach the agent only receives an $observation$ of the environment's state. 

Summarizing all those factors, it becomes obvious that the given game can clearly be defined in terms of a \keyword{Partially Observed Indeterministic Markov Decision Process}.

It is worth noting, that while the dimensionality of the observation is likely much smaller than the dimensionality of the state, any feasible observation will be high-dimensional or real-valued. The chance for any particular combination of parameters to appear multiple times vanishingly low, which makes the use of function approximation necessary.

While the notion of \keyword{POMDP}s itself contains no definition of final states, it is necessary to have final as a hard limit of the horizon in the calculation of state-values. Another design decision in the course of the implementation of this project was, that the agent itself can define what it wants to see as the end of an episode. It is a matter of testing after how many seconds the agent shall give up on a trial, or if steering into a wall should be seen as the end of a training episode. Because of that, the environment provides only candidates of what could terminate an episode to the agent, such that the agent can decide to reset the environment or not. \\

The game itself has three different game modes the user can choose between at the start of the game: In the \term{Drive} mode, users can manually drive the car with the computer's keyboard. If the \term{Train AI supervisedly} mode is activated, the car must still be driven manually, and the game exports information about the game that can be used as (pre-) training data for other driving agents. The last mode is the \term{Drive AI} mode, where car can be controlled by an agent, as long as the User doesn't actively interfere. A screenshot of the start menu can be found in figure~\ref{fig:overviewshot} in appendix~\ref{AppendixB}. Note that the background of the menu is a bird-eye view of the track.

\subsection{Characteristics of the agent}
\label{ch:agentchars}

As stated above, it was a design decision to leave as many options open to the agent as possible, such that several agent can each have their own respective definitions of certain features. To summarize from the above chapters, the features unique to every agent are:
\begin{itemize}
	\item The definition of the agent's \keyword{observation-function}, providing its internal state $s = o(s_e)$
	\item Definition of the \keyword{reward-function}, returning a scalar from state, action and subsequent state: $\mathcal{S}_e \times \mathcal{A} \times \mathcal{S}_e \rightarrow \mathds{R}$
	\item Definition of its internal \keyword{model}, basing on which it calculates its policy
	\item Under which conditions an episode is considered to be terminated 
	\item Definition of the agent's \textit{exploration technique}
	\item If and how the agent relies on pretraining with supervised data	
\end{itemize}
\begin{flushright}
	\scriptsize
	In the following sections, I will refer to the definitions of these functions/options as the \keyword{features} of the agent. In the implementation, there are many possible features, not all of which are actually used by an agent. I will refer to those as \keyword{possible features}. When I talk specifically about the possible observations influencing the observation-function, I refer to those as \keyword{(possible) vectors}.
\end{flushright}

It is due to this design decision that the program flow of this project cannot follow the exact same structure than the one put forward in algorithm~\ref{alg:gym}. One big structural difference is for example that the outer loop (line~\ref{algline:gym_episode}) becomes obsolete, as the agent decides under what conditions the environment must be reset.\\

\noindent I will go into detail about the implemented features and vectors in a later section after describing the specific implementation of agent and environment, as it is easier explained with that in mind.


\section{Implementation}

The associated programs are, as long as not explicitly stated otherwise, written by the author of this work and are licensed under the GNU General Public License (GNU GPLv3). Their source codes are attached in the appendix of this work and can additionally be found digitally on the enclosed CD-ROM as well as online. Version control of this project relied on \term{GitHub}\footnote{\url{https://github.com/}}, and was split into three repositories: The source code of the actual game written with the game engine \term{Unity 3D} (\textit{BA-rAIce}\footnote{\url{https://github.com/cstenkamp/BA-rAIce}}), the source code of the implementation, written in \term{Python} (\textit{Ba-rAIce-ANN}\footnote{\url{https://github.com/cstenkamp/BA-rAIce-ANN}}), as well as the present text, written in \LaTeX ~ (\textit{BAText}\footnote{\url{https://github.com/cstenkamp/BAText}}). To ease connections between the following descriptions and their correspondenced in the actual source code, footnotes will refer as a hyperlink to the files on GitHub, using their relative path (every Python-file belongs to the agent, all other files to the game). In order to ensure that no work after the deadline is considered, it is referred to the signed commits \colorbox{red}{THE, SIGNED, COMMITS}.
%TODO: signed github commit!

The game is programmed using Unity Personal with a free license for students\footnote{\url{https://store.unity.com/products/unity-personal}}. It is tested under version \term{2017.1.0f3}\footnote{As of now, \today, there is a bug in Unity that causes it to crash due to a memory leak if UI components are updated too often (which happens after a few hours of running). Because of that, in the current release of this project, all updates of the Unity UI are disabled in AI-Mode. A bug report to Unity was filed (case \href{https://fogbugz.unity3d.com/default.asp?935432_h1bir10rkmbc658k}{935432}) on \formatdate{27}{7}{2017}, and it was promised that this issue will soon be fixed. Once that is the case, the variable \inlinecode{DEBUG_DISABLEGUI_AIMODE} in \href{https://github.com/cstenkamp/BA-rAIce/blob/ef2dc018f36cd9ad65df90e65d8ab840c822567e/Assets/Scripts/AiInterface.cs\#L12-L13}{AiInterface.cs} can be set to \inlinecode{false}.}.
Scripts belonging to the game are coded using the programming language \term{C\#}. The agent was programmed with \term{Python 3.5}, relying on the open-source machine learning library \term{TensorFlow}\cite{abadi_tensorflow:_2015}\footnote{\url{https://www.tensorflow.org/}} in version \term{1.3}. For a listing of all further used python-packages and their versions, it is referred to the \href{https://github.com/cstenkamp/BA-rAIce-ANN/blob/master/requirements.txt}{requirements.txt}-file in the respective repository. 

While the author of this work contributed most of the work to change the given game such that it can be learned and played by a machine, the original version of the game was given in advance, coded by \leon of this work. While it will be explicitly stated what was already given later in this chapter, it is also referred to the respective branch on Github (\textit{Ba-rAIce -- LeonsVersion}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/tree/LeonsVersion}}). The implementation of the agent was however not influenced by any other people than the author of this work.\\

\noindent As already hinted at, the program flow of the implementation differs from that of other implementations. The game, making up the environment, is completely independent of the agent and runs as a separate \keyword{process} on the machine. The agent is written in another programming language and must thus make up a distinct process as well. Because of that, it is necessary that agent and environment communicate over a protocoll that allows inter-process-communication. In this work, it was decided to use \keyword{Sockets} as means of communication. While explained in following section, it is for now important to know that sockets are best implemented as running in a separate \keyword{thread}, where they can send textual information to another socket running in another process.

\subsection{The game as given}
\label{ch:gamedescription}

The program flow of the game is encapsuled by the framework that the game engine \term{Unity 3D} provides. To ease the implemention of games, Unity provides numerous \term{game objects} with pre-implemented properties like friction or gravity, as well as drag-and-drop functionality to add 3D components or cameras to the Graphical User Interface (\textbf{GUI}). Another advantage of Unity is, that it targets many graphics APIs, which takes a lot of work from the programmer to implement an efficient graphics pipeline.

To implement additional behaviour or features not predefined by Unity, it allows for scripts, written in object-oriented \keyword{C\#}. Scripts that are supposed to be used by Unity must provide a class that extends\footnote{In the following sections, I will use the terms \term{extends}, \term{implements}, \term{knows} and \term{has}. When I use those, I mean them in the strict sense in the context of programming languages (and the concepts inheritance, interfaces, and references).} its class \inlinecode{MonoBehaviour} or be used by such a one. 
For the file to be instanciated during runtime, it must be specified in Unity's \term{Object Hierarchy}. After staring the program, Unity will create all objects specified in the hierarchy. To enable the possibility of instanciations \term{knowing} each other during runtime, they must provide public variables of the type of the respective subclass of \inlinecode{MonoBehaviour}, which can via drag-and-drop be assigned to the respective future instance specified in the Object Hierachy. \inlinecode{MonoBehaviour} provides a number of functions that are automatically called by Unity at different times during runtime. The most important ones are \inlinecode{void Start()}, called when first instanciating the respective object, as well as \inlinecode{void Update()} and \inlinecode{void FixedUpdate()}, called every Update-step or Physics-update-step, respectively\footnote{Concerning the difference between Update and Fixedupdate: \textbf{Update} is called once per frame, in other words as often as possible. It is generally not used to update physics, as its call frequency depends on the current \textbf{FPS} -- if calculations here take to long, the FPS of the game will decrease. \textbf{FixedUpdate} is called precisely in a fixed interval of game-internal time. If calculations in FixedUpdate are too slow, the progress of the game-internal time is delayed until FixedUpdate catches up. The update-interval of FixedUpdate can be freely chosen and is $0.002$ seconds in the current implementation. If Unity's \term{Time scale} is set to zero, FixedUpdate() will not be called at all.}. If a subclass of \inlinecode{MonoBehaviour} is attached to a game object, it can provide specific additional functions that are called when specific events occur during runtime -- an example would be \inlinecode{OnCollisionEnter(Collision)}.\\

In the following, I will describe the respective classes implemented in the game. As not all of those are originally created by the author of this thesis, I will refer to classes created by \leon of this thesis with a superscripted~\ref{byLeon}. Some of the functionality is optional. As those options are relevant not to a User of the game but to its developers, those options are specified in the code, more precisely in a static class called \inlinecode{Consts} in \term{AiInterface.cs}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/AiInterface.cs}}\textsuperscript{,}\footnote{\label{byLeon} Not originally created by the author of this thesis.}.\\

%TODO was hiervon noch übrig ist \noindent It is hard to describe the game in a way that is understandable for somebody who has not seen any of the code before, as many functions depend on each other, sometimes in circular ways. As however there has to be a start somewhere, it may be the case that some concepts are referenced to before they are fully explained. An example for this is the \inlinecode{QuickPause()}-function: This function's purpose is to pause all physics processes of the game, such that other functionalities running in parallel to it get time to catch up with their calculations. Also, for the game to be played by an agent a \inlinecode{ResetCar}-functionality that can be triggered by another piece of code is useful. While it will be referenced to those functions, their purpose may become clear only later in the text. For an informal description of what each specific file does, it is referred to the table in 


\newcommand{\byLeon}{\textsuperscript{,\ref{byLeon}}{ }}

\subsubsection{Game modes}

On the surface, the game has three different game modes that can be active: \term{Drive}, \term{Drive AI} and \term{train AI supervisedly}. These are the three modes the User can choose from in the game's main menu (the UI of which can be seen in figure~\ref{fig:overviewshot} in appendix~\ref{AppendixB}). In the actual implementation however, the game mode is handled a bit different. \inlinecode{mode} is a public string-array of the globally known object \inlinecode{Game} (an instanciation of a subclass of \inlinecode{MonoBehaviour} specified in \term{GameScript.cs}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/GameScript.cs}}\byLeon). \inlinecode{mode} contains one or more elements of the following group: \inlinecode{driving}, \inlinecode{keyboarddriving}, \inlinecode{train\_AI}, \inlinecode{drive\_AI} and \inlinecode{menu}. If the game's main menu is opened, \inlinecode{mode = [menu]}, and in all other cases its a set consisting of \inlinecode{driving} as well as the respectively obvious elements. This implementation is advantagous, because some behaviour needs to be triggered in multiple modes -- the car's movement is for instance only calculated if \inlinecode{Game.mode.Contains("driving")}, which is the case in all three of the above mentioned modes. The current implementation also makes it easier to add further behaviour: If for example an AI-agent shall also function to generate supervised data, one can simply add the respective mode in the \term{Gamescript.cs} file.

The functionality for switching the game mode is specified in the \inlinecode{SwitchMode(newMode)} function, found in \term{GameScript.cs}. After setting the respective mode as described above, this function disconnects any connected Sockets and  activates the required cameras for the mode, updates the UI's Display indicating the mode and calls some further initializing functions (\inlinecode{AiInt.StartedAIMode()} or \inlinecode{Rec-StartedSV\_SaveMode()}), if applicable. Because particularly the initialization of the \inlinecode{drive\_AI}-mode involves connecting with an external Socket, it is done in part in a side thread. This means that the main thread does not wait for the initialization to be finished -- because of that, the \inlinecode{StartedAIMode()}-function sets the variable \inlinecode{AiInt.AIMode} to \inlinecode{true} once it is done. Any behaviour that depends on a successful initialization of the can thus simply check for this variable instead.

The object \inlinecode{Game} is responsible for switching the \inlinecode{mode} to \inlinecode{menu} in its \inlinecode{Start()}-method or after the press of the \keystroke{Esc}-button at any time during the runtime of the game. Besides this, its definition in \term{GameScript.cs} also contains the methods \inlinecode{QuickPause(string reason)} and \inlinecode{UnQuickPause(string reason)}. This purpose of QuickPause is to pause all physics processes of the game, such that other functionalities running in parallel to it get time to catch up with their calculations. For that, the \inlinecode{QuickPause(string reason)}-function sets the Game's \inlinecode{Time.timeScale} to zero, which all freezes Unity's internal physics by stopping future \inlinecode{FixedUpdate()}-calls, which includes the calulation of the car's new position. Further, this function removes \inlinecode{driving} from \inlinecode{mode}, so that other driving-related functions not triggered by \inlinecode{FixedUpdate()} are also stopped. Lastly, the QuickPause-mode changes the game's GUI to make the difference visible to the User. While the public function \inlinecode{QuickPause()} could in principle be called from every method of the game, it contains functionalities that are (due to design concepts of Unity itslef) not threadsafe, meaning that only the main-thread can call them safely. To enable the possibility of asynchronous threads activating the QuickPause-mode, \inlinecode{Game} contains a public variable \inlinecode{shouldQuickpauseReason}. This variable can be set to a value from asynchronous threads, and the \inlinecode{Game} checks every \inlinecode{Update()}-step (guaranteed to be main-thread and to also be called if QuickPause is active) if another side thread requested to invoke this method.
Once the method that originally invoked QuickPause wants the game to continue its normal process, it must call \inlinecode{UnQuickPause(string reason)} (or request for it to be called). However, QuickPause could have been enabled by another method as well, that is not ready for the normal game process yet. To prevent such a scenario, the methods to start and to end QuickPause must both be called with a string \inlinecode{reason}. In every call of \inlinecode{QuickPause(string reason)}, the function pushes \inlinecode{reason} on \inlinecode{Game}'s List \inlinecode{FreezeReasons}, and in every call of \inlinecode{UnQuickPause(string reason)}, it removes the respective reason from the list and only continues the normal game process if the list becomes empty.

\subsubsection{User Interface}

The job of the Game's \inlinecode{UI} (an instance of type UIScript, specified in \mbox{\term{UIScript.cs}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/UIScript.cs}}\byLeon)} is to update the game's user interface. This UI is overlayed over the current scene, such that its components can be seen simultaneously to the image of an active camera. In its \inlinecode{MenuOverLayHandling()}-Method, \inlinecode{UI} specifies the view of the game's menu-mode (as can be seen in figure~\ref{fig:overviewshot} in appendix~\ref{AppendixB}), as well as the key bindings to activate the required mode. In the method \inlinecode{DrivingOverlayHandling()}, it sets visibility, content, color or position of numerous UI components (defined in Unitys Object Hierachy), that are seen in non-menu-modes. Both \inlinecode{DrivingOverlayHandling()} and \inlinecode{MenuOverLayHandling()} are called every \inlinecode{Update()}, such that the view elements are always contemporary. Further, the \inlinecode{UI} specifies the \inlinecode{void onGUI()}-function, which Unity calls every time it re-renders the GUI. This function overlays Debug information on the screen and changes the view if the QuickPause-mode is activated.\\

\noindent The User Interface of the game while driving can be seen in the figures~\ref{fig:humandriveshot} and~\ref{fig:aidriveshot} in appendix~\ref{AppendixB}, which are screenshots for the \inlinecode{keyboarddriving} and \inlinecode{drive_AI} mode, respectively. The latter of those screenshots is annotated with labelled boxes around each UI component, with page~\pageref{fig:aidriveshot} explaining each component a bit further\footnote{Note that the entire UI, besides the content behind labels A, F, H and I, was already implemented like this by \leon.}.

\subsubsection{Controls}

In \inlinecode{mode}s containing \inlinecode{keyboarddriving}, the game is steered with the arrow keys \keystroke{$\leftarrow$} and \mbox{\keystroke{$\rightarrow$}.} The throttle is triggered via the \keystroke{A}-key, whereas the brake is called via \keystroke{Y}. The \keystroke{R} - key flips the reverse gear. Note that as long as the \inlinecode{pedalType} in \term{CarController.cs}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/CarController.cs}}\byLeon is set to \inlinecode{digital}, throttle and brake are binary when controlled via keyboard. 

In \inlinecode{mode}s containing \inlinecode{drive\_AI}, the car is usually controlled by the agent, it is however possible to re-gain control over it via pressing the \keystroke{H}-key. Once that occurs, the variable \inlinecode{AiInt.HumanTakingControl} is set to true, indicating the program that keyboard-inputs must be accepted. This is useful for example if one wants to check if rewards or Q-values are realistic. If human interference of the \inlinecode{drive\_AI} mode is active, it is possible to simulate speeds to the agent (meaning that not the actual speed of the car, but a specified value is sent via Sockets). This can be done with the Number keys, where the pretended speed is evenly spread between $0 kph$ (\keystroke{0}) and $250 kph$ (\keystroke{9}). The \keystroke{P} key is reserved to simulate a full throttle value. To hand control back to the agent, \keystroke{H} must be pressed again. 

In the \inlinecode{drive\_AI}-mode, a user can also manually disconnect or attempt a connection-trial with an agent. The keys to do that are \keystroke{D} and \keystroke{C}, respectively. During any \inlinecode{mode} containing \inlinecode{driving}, \keystroke{Q} can be pressed to activate the \term{QuickPause}-mode, which allows to spectate the current screen. QuickPause is ended with another hit of \keystroke{Q}. 

\subsubsection{The car}

The \inlinecode{car} itself is a \inlinecode{Rigidbody}, which is a Unity-gameObject with certain properties like spatial expanse, mass and gravity. Attached to the \inlinecode{car} is, next to no further mentioned visible components, a Unity \inlinecode{BoxCollider} as well as four \inlinecode{WheelColliders} gameObjects. While the \inlinecode{BoxCollider}'s purpose is to trigger the call of particular functions in scripts attached to other gameObjects upon simulated physical contact, the \inlinecode{WheelColliders} are predefined with certain physical properties, allowing for precise simulation of the behaviour of actual tires. How the car moves is specified in an instance of the class CarController (specified in \term{CarController.cs}), which is attached to the respective Rigidbody.

All functions of the \inlinecode{CarController} are only called in modes containing \inlinecode{driving}. In its \inlinecode{FixedUpdate()}-step, this instance adjusts the WheelCollider's friction according to the current surface the wheels are on, and it is checked if the car moved outside the street's surface. Furhtermore the car's velocity as well as some other values are calculated. Finally, the torques for acceleration and braking are applied and the front wheels are turned according to the steering-value. The amount of those torques and angles depend on three values: $steeringValue \in [-1,1]$, $throttlePedalValue \in [0,1]$ and $brakePedalValue \in [0,1]$. If \inlinecode{Game.mode.Contains("keyboarddriving")} or \inlinecode{AiInt.HumanTakingControl == true}, those values depend on the User's keyboard input. Otherwise, if \inlinecode{AiInt.AIMode} is enabled, the values are defined as \term{know}n values from the \inlinecode{AiInt}, namely \inlinecode{nn\_steer}, \inlinecode{nn\_throttle} and \inlinecode{nn_throttle}.

In \inlinecode{CarController.Update()}, the outer appearance of the car is updated, consisting of wheel height, wheel rotation and wheel rotation.

As explained in section~\ref{ch:agentchars}, a connected agent must be able to reset the car at any time during runtime. To allow for that, the \inlinecode{CarController} provides a \inlinecode{ResetCar(bool send_python)} method. Additionally, there is a \inlinecode{ResetToPosition}-function that resets the \inlinecode{car} to any specified position and rotation. Because the car must stand still after a reset, it is necessary to completely kill its inertia. To do so, it is not enough to apply zero motorTorque and infinite brakeTorque to the car and call \inlinecode{car.ResetInertiaTensor()} inside \inlinecode{ResetToPosition}, because those values would simply be overwritten in the next call of \inlinecode{FixedUpdate()}. This is why in the given implementation the function \inlinecode{ResetToPosition} sets a boolean variable \inlinecode{justrespawned} to \inlinecode{true}. In every call of \inlinecode{CarController.FixedUpdate()}, it is checked if the car did just reset, and performs the necessary forces to remove all of the car's inertia right there, before setting \inlinecode{justrespawned = false}.



% TODO hier wallcolliderscript

\subsubsection{Position tracking}

To successfully learn useful driving policies, the agent must get precise knowledge of the car's position which goes beyond its mere coordinates. Additional useful information is for example the car's position in relation to the street, or information about the course of the road ahead of the it. To allow for that, the game incorporates a \inlinecode{TrackingSystem}, which is an instance of the class \inlinecode{PositionTracking}, specified in \term{PositionTracking.cs}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/PositionTracking.cs}}\byLeon. The \inlinecode{TrackingSystem} knows the GameObject \inlinecode{trackOutline} and converts it to an array of coordinates located regularly along the track, each one respectively located at the middle of the street -- the \inlinecode{Vector3[] anchorVector}. Using this array, much high-level information about the track can be calculated. As almost all of the respective functionality was however not implemented was not implemented by the author of this thesis, a short scetch of how it can be used shall suffice.

To calulate the progress of the car in percent, one needs the total length of the street as well as the distance the car advanced so far. The first of those can be calculated by summing up the distances between a coordinate and its successor. To get the approximate distance the car advanced, one needs iterate through the \inlinecode{anchorVector}-array to find the one closest to the car (which happens in \inlinecode{GetClosestAnchor(Vector3 position)}). The cumulated distance of successive vectors until the one closest to the car corresponds roughly to its progress in percent. 

As every successive coordinate in \inlinecode{anchorVector} is in the middle of the street, one can calculate the direction of the street at position \inlinecode{p} by calculating\\ \inlinecode{anchorVector[GetClosestAnchor(p)+1] - anchorVector[GetClosestAnchor(p)]}. This can be used as basis for many further calculations: For instance, the car's distance to the center of the street can be found via by calculating the norm of the orthogonal projection from the car's coordinate onto this vector (from now on called the \term{pendicular}). The direction of the car relative to the street can be found by calculating the angle between its \inlinecode{direction}-vector and the previosly explained vector.

Besides defining the \inlinecode{anchorVector}-array and other helper-arrays depending on it in its \inlinecode{Start()}-method, the \term{TrackingSystem} calculates the car's current \inlinecode{progress} at every \inlinecode{Update()}-step and triggers the \inlinecode{UpdateList()}-method of the \inlinecode{RecordingSystem} in regular progress-intervals. Furthermore, the \term{TrackingSystem} provides certain public methods that can be used by an agent, namely \inlinecode{getCarAngle()}, \inlinecode{GetSpeedInDir()} and \inlinecode{GetCenterDist()}, the precise content of which will be explained in a later section.

\subsubsection{Tracking time}

As visible in the game's screenshots (more precisely annotations B, C, P and Q of Figure~\ref{fig:aidriveshot}), the game displays information about the current laptime, the last laptime as well as the time needed for the fastest lap. Futhermore the game provides a Feedback basing on the time needed for a specific section of the street in comparison to the time that was needed for this section in the fastest lap so far (annotation E). This is possible because the game records current laptime multiple times throughout the course. As mentioned above, \inlinecode{RecordingSystem.UpdateList()}-method gets called regularly by the \inlinecode{TrackingSystem}. \inlinecode{RecordingSystem} is an instance of the type \term{Recorder}, specified in \term{Recorder.cs}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/Recorder.cs}}\byLeon. It contains three lists of \inlinecode{PointInTime}s, for \inlinecode{thisLap}, \inlinecode{lastLap} and \inlinecode{fastestLap}. A \inlinecode{PointInTime} is a \term{serializable} object (also defined in \term{Recorder.cs}) that contains two floats, for a progress and a corresponding time.

An instance of a separate class, \term{TimingScript} (found in \term{TimingScrip.cs}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/TimingScript.cs}}\byLeon) is attached to a permeable Collider right on the start/finish line that functions as trigger. As a subclass of \term{MonoBehaviour}, \term{TimingScript} has a \inlinecode{void OnTriggerExit(Collider other)}, that is invoked as soon as another Collider stops contact with it. As the only movable collider is the car's BoxCollider, this method is called as soon as the car starts a lap. A lap is considered valid under two conditions: First, the car needs to pass a second collider (ConfirmCollider, with its attached \term{ConfirmColliderScript}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/ConfirmColliderScript.cs}}\byLeon), which ensures that the car did in fact drive a complete lap instead of backing up right back on the start/finish line. The second condition for validity is, that at no time all four tires of the car hit left the street's surface.

If a lap is considered valid, the \term{TimingSystem}'s \inlinecode{onTriggerExit}-procedure calls \term{RecordingSystem}'s \inlinecode{Rec.FinishList()}-method. Afterwards and under no restrictions, it prepares the start of a new lap by calling \inlinecode{Rec.StartList()}. Once \term{StartList} is called, the \term{RecordingSystem} creates a new List of \inlinecode{PointInTime}s, to which the \term{TrackingSystem} then regularly adds new tuples of progress and corresponding time. Once FinishList is called, the \term{RecordingSystem} sees if the lap just now was the fastest so far, and saves it on the computer's disk. In its methods \inlinecode{GetDelta()} and \inlinecode{GetFeedback()}, which are called every \inlinecode{Update()}-step of the \inlinecode{UI}, it can then compare the time of the currently latest progress with the corresponding time of \inlinecode{fastestLap}. \\

\subsection{The game -- extensions to serve as an environment}

The code explained so far is sufficient for the game to work in the \inlinecode{keyboarddriving}-mode. The framework for this mode was working entirely when the author of this thesis received it, as it was implemented by \leon. The major additions implemented in the scope of this thesis that were mentioned so far is the behaviour following game modes other than \inlinecode{keyboarddriving} or \inlinecode{menu}, the \term{QuickPause}-functionality, the already mentioned additions to the User Interface, the means to \term{reset} the car as well as the functions \inlinecode{GetCarAngle()} and \inlinecode{GetSpeedInDir()} of the \term{TrackingSystem}, which will be more thoroughly explained lateron.

\subsubsection{The minimap cameras}

The content of the minimap cameras can be seen behind annotations \textbf{H} and \textbf{I} of figure~\ref{fig:aidriveshot}. They are implemented to serve as an exclusive- or additional input to agents, in the hope of providing enough information to sucessful policies. As can be seen, the minimap cameras provide a bird-eye view of the track ahead of the car, by filming vertically down. In contrast to the foreshortened main-camera, the minimap cameras are orthogonal, which means that distances are true to scale, irrespectively of their position. Because the cameras are attached to the \inlinecode{car}'s Rigidbody, they are always in the same position relative to the car. In the current implementation, up to two cameras can be used (it is however possible to disable one or both cameras by setting a corresponding value in the class \inlinecode{Consts} in \term{AiInterface.cs}). When both cameras are active, one of them is mounted further away from the car, such that one provides high accuracy whereas the other provides a greater field of view. If only one camera is enabled, its distance is to a value between that, resulting in a tradeoff of accuracy and field of view. As both cameras must be handled separately, this happens in the \inlinecode{Start()}-method of the \term{Gamescript.cs}.

While a previous implementation of a similar functionality was provided by \leon using a complex and inefficient raytracing, in this implementation the minimaps base on Unity \inlinecode{camera}-objects, which are efficiently calculated on the computer's GPU. Attached to each camera is a respective instance of the script specified in \term{MiniMapScript.cs}\footnote{\url{https://github.com/cstenkamp/BA-rAIce/blob/master/Assets/Scripts/MiniMapScript.cs}}. While ordinarily the content of Unity's cameras is directly rendered to the game's main screen, this script contains methods to convert the image of the camera to a format that can be sent to an agent. That is made possible by the usage of a \inlinecode{RenderTexture} as well as a \inlinecode{Texture2D}, which are created as private objects in \term{MiniMapScript}'s \inlinecode{PrepareVision(int xLen, int yLen)}-method. This method is called form outside and expects as parameter the dimensionality of the produced matrix, which is set in the class \inlinecode{Consts} in \term{AiInterface.cs}. 
% TODO dass das visiondisplay nur if needed prepared wird

Both cameras provide the public function \inlinecode{GetVisionDisplay()}. When this function is called, it sets the above mentioned \inlinecode{RenderTexture} as the camera's \inlinecode{targetTexture}, forces the camera to \inlinecode{Render()} to this texture, and then reads the rendered contents into the specified \inlinecode{Texture2D}. After this process, it must reset the camera's \inlinecode{targetTexture}, such that it renders back to the game's main display, such that it can be inspected visually. The \inlinecode{Texture2D} however can be then be read pixel by pixel and thus converted to an array or string. As it was decided that the resulting display only differentiates between \term{street}, \term{curb} and \term{off}, the cameras use a \inlinecode{Culling Mask}, that visually filter out all other gameObjects.

\subsubsection{Recording training data}

For the game to be played by an AI agent, data of the environment must be recorded in regular intervals, to either be sent to the agent in the case of it playing the game or learning via reinforcement learning, or to be exported to a file, which an agent can import perform pretraining on. In the following sections, I will not talk about what those data exactly looks like, but only how it is saved and sent. Because of that, I will refer to this data under the name \term{vectors}, which are in detail explained in section~\ref{ch:thevectors}. Collecting the lastest \term{vectors} happens in the function \inlinecode{GetAllInfos()} of \term{AiInterface.cs}, which calls a number of functions, amont others defined in \term{CarController.cs} as well as \term{TrackingSystem} and returns a string containing the result of all those.

As calculating the data that needs to be exported can take relatively much time, this process cannot be performed every \inlinecode{FixedUpdate()}-step. Because of that, the following function is used to perform a function in regular time intervals:
\begin{lstlisting}[language=C#,frame=none]
long currtime = AiInterface.UnityTime();
if (currtime - lasttrack >= Consts.trackAllXMS) {
	lasttrack = lasttrack + Consts.trackAllXMS; 
	SVLearnUpdateList ();
}
\end{lstlisting}%

Where \inlinecode{AiInterface.UnityTime()} returns Unity's internal time by calling \inlinecode{Time.time * 1000}. Using this definition of time has the advantage that it is maximally precise inside Unity: As the time of calling \inlinecode{FixedUpdate()} is likewise dependant on \inlinecode{Time.time}. A disadvantage of this measurement of time is however, that it can only be used in the main thread, which is why asynchronous methods must rely on the system's time, for which no conversion method exists.

The process of steps required to record training-data are the following:

Once user selects the \term{Train AI supervisedly} mode, Recorder's \inlinecode{void StartedSV\_SaveMode()} gets called, which enables the minimap-cameras and sets \inlinecode{SV\_SaveMode = true}. If that variable is \inlinecode{true}, then the recorder will in its \inlinecode{StartList()} create a new List \inlinecode{SVLearnLap = new List<TrackingPoint> ()}. \inlinecode{TrackingPoint} itself is a class defined in \term{Recorder.cs}, that upon creation requires certain values about the state of the game as input. While driving, the recorder checks every \inlinecode{FixedUpdate()}-step with the mentioned method if it calls \inlinecode{SVLearnUpdateList()}. This method then collects the recent values for the currently performed actions, laptime, progress and speed as well as the respectively latest \term{Vectors}, creates a \inlinecode{TrackingPoint} from those and Updates the \inlinecode{SVLearnLap} with it. When the RecordingSystem's \inlinecode{FinishList} function is called upon the next crossing of the start/finish line, the \inlinecode{SVLearnLap} is saved to a file. As the vectors can contain multiple pixel matrices from the minimapcameras, this may however take quite long. To prevent the game from freezing everytime the car passes the start/finish line, the saving of the actual file is performed in a seperate thread.

Because the agent using this exported data is written in another programming language than the environment, the data cannot be exported in a binary file. In this implementation, it was decided to save the data in the \term{XML}-format. It is worth mentioning, that not only the List of \inlinecode{TrackingPoint} is exported, but also additional meta-information, stating among others the interval of how often a \inlinecode{TrackingPoint} was exported, which can be interpreted and used by an agent.

\subsubsection{Communicating with an agent}

As already mentioned, the game is running live and is in general not stopped by the agent, as done for example when interfacing with the openAI gym (section~\ref{ch:rlframeworks}). Because of that, the speed of communication between agent and environment is a bottleneck in how good an agent can perform, and needs to be as fast and efficiently implemented as possible. To ensure quick reaction times, it was also decided that the agent and the game must run on the same machine, as sending the data to another machine increases the needed time drastically\footnote{This is the reason this project was implemented entirely under Windows: There is no stable Unity Editor for Linux, and there is no contemporary GPU-supporting TensorFlow under Mac. The only common ground for which both are available ist therefore the Windows platform.}.

In the scope of this thesis, it was experimented a lot with the flow of communication between agent and environment, with the current process as its most efficient version so far.x




%%TODO - ungefähres UML-diagramm DES SPIELS!!
%-ablaufdiagramm vom server-zeug dass zu dem von python passt
%-Aufbau des exportierten XMLs (dabieschreiben dass action und speed 2 mal drin sind weil die von fake_speed affektiert sein könnten)
%-beschreibung der optionen-konstanten
%-dass bei ResetToPosition das AIInt pyhtoon drauf hinweist


-nochmal drauf eingehen dass das spiel live ist
-den sockets post
-das von leon gemalte ablaufdiagramm

\begin{figure}
	\centering
	\resizebox{1.1\textwidth}{!}{
		\input{sequence_diagrams/server_main}
	}
\end{figure}


%
%
%RL. normalerweise env und agent. Normalerweise ist env klasse, wie in gym. Geht bei mir nicht. closest pendant ist server 
%
%pseudocode machen, wie die kommunikation mit openAI gym: environment erzeugen, for episodes blablabla, dann macht [im gymcode farbig markiert] der agent dann dasunddass, dann environment, ..
%design choice gemacht das pyhton bspw für resetten verantwortlich ist [schneller ändern, nach zeit decreasen, soll eh nur das spiel lernen, ...] (nimmt done und outer loop vom gym pseudocode]. Deswegen ist auch reward nicht fixed. wir wollen das spiel möglichst gut lernen und wollen dafür u.a. verschiedene rewards vergleichen.
%
%
%Server life, kann eingreifen [selber fahren, qwerte plotten]
%nach initialisieren ist receiverthread durchgehend mit unity verbunden
%..und schreibt neue sachen in den inputval. Warum? closest what you get to environment, contains the maximal information an agent COULD know (because you know agents make o(s)), und da ich verschiedene agents teste ... [eines der wichtigsten dinge, agents gegeneinander testen! wodrin unterscheiden die sich alle] (aktuell letzten 4, in config lässt sich einstellen ob mehr (macht sinn dass in config, denn: unabhängig von agents!).
%zum thema special commands: unity sendet bspw ob wallhit, und python entscheided ob das nen reset ist. [gegen wand, umgedreht, runde geschafft, zeit ende, connection failed]
%
%könnte mehrere unity-instanzen haben die sich am reclistenport anmelden, an nen eigenen receiverthread weitergeleitet werden, eigenen inputval haben
%inputval ist stabil blabla
%receiver ruft performaction VOM agent auf. Könnte wenn geschwindigkeit es verlangt in nem extra thread sein [in performaction/im agent steckt halt alles]
%[ursprünglich wars so das receiver in inputval schreibt, agent (andere farbe wäre das) würde inputvla lesen und outputval schreiben, sender outputval lesen --> ist aber in practice LANGSAMER gewesen 
%inputval ist ne intra-threaded class damit es daten akkumuliert, outputval ist intra-thread  damit schneller mit 2 threads die gleichzeitig drauf zugreifen
%
%[man sieht - nur die 3 threads in grün hinterlegt sind relevant, die beiden listenthreads sind noch an falls verbindung abkackt, main für gui weil wegen tkinter
%
%speed ist wichtig, weil: in gym steht die environment zwischen send und receive still, bei mir eben ncith.
%
%wollen dieses spiel lernen, blabla, deswegen ist reward und welche vektoren er kriegt und welches model RELATIV, ERSETZBAR (\#design decision)
%
%im gegensatz zu anderen kriegt der agent hier bei aufruf erstmal den ENVIRONMENTS-state statt agentstate was sinnvoll ist, weil er halt selbst entscheided was er wissen soll
%
%bei UML-diagrammen im anhang vollständige diagramme haben und im fließtext die mit nur wichtigen/erwähnten funktionen(!!)
%[dass abstractRLagent noch memory hat]
%[jede funktion aus dem Diagramm mit grund erklären warum sie da ist, die die ich ncith erklär auch nur im Diagramm im Anhang haben!]
%
%[dann die einzelnen un-abstrakten agents... novision_agent ist halt dqn-basiert, hat daher nen dddqn-model, ist novision, hat also keine conv, muss also getAgentstate übershcreiben (kann auch ruhig inefficientmemroy nutzen da die daten klein sind, ...)
%
%[EIGENTLICH sollte abstractRLAgent klappen mit pur random actions... und mit einem param vielleicht als evaluate\_humans agent fungieren??]
%
%[dass die alle plotter HABEN und die serverkompomenten über containers KENNen etc]
%[warum haben dqnbasierte agents nochmal randomaction drin? übernehmen sie nicht die vom abstractRLagent?]
%
%[dass dddqn auch supervisedly lernen kann ist superpraktisch, weil so kann man mit dem dqn\_sv\_agent mit der pretrian methode gucken ob das dataset überhaupt seperable ist!
%
%wie die othervecs in den agent reingehen, als namedtuple, und wie man sich dann aussuchen kann was man davon nehmen möchte
%
%überhaupt pretrianen und dass man die exportieren kann!
%
%[die architektur von meinem duelDQN muss nochmal inen eigenen graphen yay]
%
%[DDDQN\_model braucht update-target-network methode im UML]
%[wird evaluator vom standard-agent gecallt?]
%[wennde testen wilslt ob dataset seperable ist, guckste in sv\_agent, wennde wissen willst ob das model funktioniert, guckste in gym\_text]
%
%[unity exportiert halt die pretraindata, gneauso wie es die python senden würde, als human-readable xml]

%
%
%ultra-kurze version: -Server.py ist was gerannt wird. Server kommuniziert mit Unity. Unity sendet alle 0.1 sekunden den aktuellen Vektor an python (eigentlich auch vision, reicht aber davon auszugehen dass nur distanz zur straßenmitte und winkel entlang der straße geschickt werden, und dass das network von da an feed-forward ist)
%-Alle 0.1 Sekunden bekommt also der server den vektor von  unity und führt dann im agent "runInference" aus.
%-der Agent steht in agent.py (soll ne abstrakte Klasse sein), und wird bspw implementiert von dqn_novision_rl_agent.py
%-in runInference runnt dann das ANN/random action. Dazu nutzt es sein model. Das model ist halt das DDDQN, welches in der Klasse dddqn.py steht
%-das dddqn-model hat halt online und target version. Wenn inference vom agent gecallt wird, nutzt es das targetnetwork
%-Agent lässt dann server das netzwerk-resultat über den server wieder an Unity senden, und wartet auf den nächsten state von unity
%-wenn er den bekommen hat, fügt er das seinem replay memory hinzu, welches in inefficientmemory.py steht
%-alle FOREVERYINF schritte führt der agent dann COMESALEARN learn-schritte durch (definiert in der dauerLearnANN/learnANN des agents.py, welche wiederum q_learn von ddddqn.py callen)
%
%
%Das Kommando, was er an unity schicken soll, ist auf einfachste weise diskretisiert: 7 werte für steering, gas und bremse sind binär. also 7 werte mit gas, 7 werte mit bremse, 7 werte mit weder noch. Führt zu 21 Q-werten in jedem State.
%
%Das Problem ist: Das Auto fährt immer direkt gegen die Wand. Im laufe der Exploration (epsilon-greedy mit zufallswerten, den Ornstein-Uhlenbeck hab ich fürs erste rausgneommen bis es nicht mehr so blöd ist) fährt es halt anfangs ein bisschen random, unter anderem den schnellsten weg an die Wand (1, 0, 0.875). Nach dem trianing denkt es sich dann hey, schnellster weg gegen die wand, perfekt, genau das will ich. Hat durchgehend den höchsten Q-wert, vom losfahren bis direkt vor der wand stehen.
%
%Current Q Vals
%0, 0, -0.857:   -0.0359664
%0, 0, -0.571:   -0.108884
%0, 0, -0.286:     0.0335033
%0, 0, 0.0:   -0.162055
%0, 0, 0.286:    0.163034
%0, 0, 0.571:    0.165725
%0, 0, 0.857:    0.256491
%1, 0, -0.857:   0.132185
%1, 0, -0.571:    0.109252
%1, 0, -0.286:   -0.121606
%1, 0, 0.0:   -0.148062
%1, 0, 0.286:   0.0419721
%1, 0, 0.571:    0.048473
%1, 0, 0.857:    0.277924   <— "schnellster weg gegen die wand"
%0, 1, -0.857:    0.0503001
%0, 1, -0.571:   -0.356878
%0, 1, -0.286:    0.101346
%0, 1, 0.0:   -0.0989533
%0, 1, 0.286:   -0.059001
%0, 1, 0.571:   -0.117535
%0, 1, 0.857:    0.104965
%
%Hab jetzt zig verschiedene reward-möglichkeiten ausprobiert, die aktuelle ist nur positiver reward, hilft auch nix.
%
%Man sollte auch grundsätzlich davon ausgehen, dass der aktuelle Qmax (plotte ich in jedem state) grundsätzlich niedriger wird wenn er vor der Wand steht, wird er aber nicht, ist durchgehend der gleiche.
%
%Und jah, mit den Terminals undso ist alles richtig, hier ein beispiel der letzten replay-memory-einträge bevor er gegen die Wand fährt:
%
%State    action    reward  state2  is_terminal
%0.253   (1, 0, 0.857)   1.085   1.052   False
%1.052   (1, 0, 0.857)   1.07488 1.965   False
%1.965   (1, 0, 0.857)   1.06112 2.989   False
%2.989   (1, 0, 0.857)   1.0428   4.118   False
%4.118   (1, 0, 0.857)   0   5.334   False
%5.334   (1, 0, 0.857)   0   6.626   False
%6.626   (1, 0, 0.857)   0   7.955   False
%7.955   (1, 0, 0.857)   0   9.322   False
%9.322   (1, 0, 0.857)   0   10.0  False
%10.0   (0, 0, 0.857)   -5   10.0   True
%
%Wobei ich vom state jetzt nur entfernung zur straßenmitte plotte, und reward auch von entfernung zur straßenmitte aktuell (plus speed in street-direction, die ist aber durchgehend quasi null).
%
%
%Man sollte meinen daraus lernt er dass sehr weit weg von der straßenmitte stheen ein nicth erstrebenswertes ziel ist, aber nope, das ist der mit abstand höchste q-wert, unerreichbar für die anderen.
%
%
%Hab jetzt auch zig verschiedene sachen probiert wie ich q-werte initialisieren (alle negativ, alle positiv, alle ungefähr null), aber nix klappt wirklich
%
%
%




%
%
%[Dass man mit supervised training super gucken kann ob dieseundjene state-definition sinnvoll ist, und dass man mit der gym_test super gucken kann ob man das model verbuggt hat]
%
%
%\footnote{I will rely heavily on UML to explain my code and stuff. Version UML 2.0. to get an overview of how that look like, I refer the reader to https://www.ibm.com/developerworks/rational/library/3101.html and https://www.ibm.com/developerworks/rational/library/content/RationalEdge/sep04/bell/index.html for sequence diagrams and class diagrams, respectively.}
%
%[tf gibt automatic differentiation, multi-gpu support, python interface]
%
%Important to know in sequence diagrams: There are both threads and object-instances. Threads are recognized by the block starting at their top. Threads call methods OF the instances. Which thread called something from the instance in a specific case is recognizable by the color, as colors correspond to threads. Simultaneous things are horizontally aligned. Messages from thread to thread are without parantheses.
%
%Why I have inputval and outputval: INPUTval damit der immer nur die letzte action hinzügen muss und der die history DADRIN hat, und OUTPUTval damit das multithreading schneller geht. BEIDES hat  seinen sinn!!
%[multiple threads deshalb weil das geschwindigkeit optimiert - beim inputval können mehrere agents multi-threaded unabhängig/gleichzeitig lesen und performen (dann gäbs nen weiteren thread, der wäre dann nicht blau und würde auf inputval zugreifen), und beim outputvall können senderthread und receiverthread einfach gleichzeitig so schnell wie möglich arbeiten]

%Das was ich beim Iris-erklären bezüglich der Sequence Diagramme aufgeschrieben hab

\includegraphics[width=\textwidth]{uml_diagrams/class_diagram.1}\\
\newpage
\includegraphics[height=\textheight]{uml_diagrams/class_diagramtwo.1}  

%\begin{algorithm}[H]
%	\KwData{this text}
%	\KwResult{how to write algorithm with \LaTeX2e }
%	initialization\;
%	\While{not at end of this document}{
%		read current\;
%		\eIf{understand}{
%			go to next section\;
%			current section becomes this one\;
%		}{
%		go back to the beginning of current section\;
%	}
%}
%\caption{How to write algorithms}
%\end{algorithm}





\subsection{The agent}

-Unbedingt auf jeden Fall UML-diagramm
-Hier im Fließtext nur kleine Versionen der UML-Diagramme, und im Anhang dann die vollständingen versionen!

\subsubsection{Challenges and Solutions}



\subsubsection{Pretraining}

\subsubsection{The different agents}

DQN vs DDPG, sehend vs nicht-sehend, ...  

\subsubsection{Network architecture}

1. dqn-algorithm
- anzahl layer, Batchnorm, doubles dueling
- clipping wieder rein, reference auf das dueling
- grundsätzlich gegen batchnorm entschieden, siehe reddit post
- MIT GRAFIK
- Adam und tensorflow quoten, siehe zotero
2. ddpg
- anzahl layer, Batchnorm
- MIT GRAFIK

-schöne grafik.
-auf meien DQN-config eingehen und(!!!) ne DDPG-config machen, using the "experiment details" vom ddpg paper  


In the original DDPG-algorithm \cite{lillicrap_continuous_2015}, the authors used \keyword{batch normalization} \cite{ioffe_batch_2015} to have the possibility of using the same network hyperparameters for differently scaled input-values. In the learning step when using minibatches, \batchnorm normalizes each dimension across the samples in a batch to have unit mean and variance, whilst keeping a running mean and variance to normalize in non-learning steps. In Tensorflow, batchnorm can be added with an additional layer and an additional input, specifying the phase (learning step/non-learning step)\footnote{cf. \url{https://www.tensorflow.org/api\_docs/python/tf/contrib/layers/batch_norm}}. Though Lillicrap et al. seemed to have success on using \batchnorm, in practice it lead to unstability, even on simple physics tasks in openAI's gym. As I am not the only one having this issue \footnote{redditlink}, I left out batch normalization for good.





\section{Possible features}

\subsection{The vectors}

\label{ch:thevectors}
-die mit * oder so markieren die von leon kommen!!


\subsection{Exploration}

\subsection{Reward}

\subsection{Pre-training}

\subsection{Performance measure}

