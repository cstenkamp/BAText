#see agent
class DDDQN_model():
	def __init__(self, sess, inputsize, num_action):
		self.sess = sess		
		self.onlineQN = DuelDQN("onlineNet", inputsize, num_action)
		self.targetQN = DuelDQN("targetNet", inputsize, num_action)
		self.sess.run(tf.global_variables_initializer())
		self.sess.run(netCopyOps(self.targetQN, self.onlineQN))
		
	#see agent	
	def inference(self, statesBatch): #called for every step t
		#model is not accessed if random action
		return self.sess.run([self.targetQN.predict,self.targetQN.Qout],feed_dict={self.targetQN.inputs:statesBatch,self.targetQN.stands_input:False})
	#see agent
	#see agent
	#see agent
	def q_train_step(self, batch): #also called for every step t
		oldstates, actions, rewards, newstates, terminals = batch
		act = self.sess.run(self.onlineQN.predict, {self.onlineQN.inputs:newstates})
		folgeQ = self.sess.run(self.targetQN.Qout, {self.targetQN.inputs:newstates})
		doubleQ = folgeQ[range(len(terminals)),act]  
		consider_stateval = -(terminals - 1)
		targetQ = rewards + (0.99 * doubleQ * consider_stateval)
		self.sess.run(self.onlineQN.q_OP, feed_dict={self.onlineQN.inputs:oldstates, self.onlineQN.targetQ:targetQ, self.onlineQN.targetA:actions})
		self.sess.run(netCopyOps(self.onlineQN, self.targetQN, 0.001)) 
		return
		
		
class Agent():
	def __init__(self, inputsize):
		self.inputsize = inputsize
		self.model = DDDQN_model #or DDPG_model
		self.memory = Memory(500000, self)  #for definition see code
		self.action_repeat = 4
		self.update_frequency = 4
		self.batch_size = 32
		self.replaystartsize = 1000
		self.epsilon = 0.05
		self.last_action = None
		self.repeated_action_for = self.action_repeat

	def performAction(self, gameState, pastState):
		self.numsteps += 1
		self.repeated_action_for += 1
		self.addToMemory(gameState, pastState)
		if self.stepsAfterStart <= self.conf.headstart_num:
			toUse, toSave = self.headstartAction() 
		elif self.repeated_action_for < self.action_repeat:
			toUse, toSave = self.last_action 
		else:
			agentState = self.getAgentState(*gameState) #may be overridden
			if len(self.memory) >= self.conf.replaystartsize:
				self.epsilon = decrease(epsilon)
				if np.random.random() < self.epsilon:
					toUse, toSave = self.randomAction(agentState)
				else:
					toUse, toSave = self.policyAction(agentState)
			else:
				toUse, toSave = self.randomAction(agentState)
			self.last_action = toUse, toSave
		self.containers.outputval.update(toUse, toSave) 
		if self.numsteps % self.conf.ForEveryInf == 0:
			self.learnStep(self.conf.ComesALearn)		

	def learnStep(self, iterations):
		for i in range(iterations):
			QLearnInputs = self.memory.sample(self.batch_size)
			self.model.q_train_step(QLearnInputs)   
		
	def addToMemory(self, gameState, pastState): 
			s = self.getAgentState(*pastState)  #for definition see code
			a = self.getAction(*pastState)	    #for definition see code
			r = self.calculateReward(*gameState)#for definition see code
			s2= self.getAgentState(*gameState)  #for definition see code
			t = False #will be updated if episode did end
			self.memory.append([s,a,r,s2,t])		
		
class DuelDQN():
	def __init__(self, name, inputsize, num_actions):  
		with tf.variable_scope(name, initializer = tf.random_normal_initializer(0, 1e-3)):
			#for the inference
			self.inputs = tf.placeholder(tf.float32, shape=[None, inputsize], name="inputs")
			self.fc1 = tf.layers.dense(self.inputs, 400, activation=tf.nn.relu)
			#modifications from the Dueling DQN architecture
			self.streamA, self.streamV = tf.split(self.fc1,2,1) 
			xavier_init = tf.contrib.layers.xavier_initializer()
			neutral_init = tf.random_normal_initializer(0, 1e-50)
			self.AW = tf.Variable(xavier_init([200,self.num_actions]))
			self.VW = tf.Variable(neutral_init([200,1]))
			self.Advantage = tf.matmul(self.streamA,self.AW)
			self.Value = tf.matmul(self.streamV,self.VW)
			self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))			
			self.Qmax = tf.reduce_max(self.Qout, axis=1) 
			self.predict = tf.argmax(self.Qout,1)		   
			#for the learning
			self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)
			self.targetA = tf.placeholder(shape=[None],dtype=tf.int32)
			self.targetA_OH = tf.one_hot(self.targetA, self.num_actions, dtype=tf.float32)
			self.compareQ = tf.reduce_sum(tf.multiply(self.Qout, self.targetA_OH), axis=1) 
			self.td_error = tf.square(self.targetQ - self.compareQ) 
			self.q_loss = tf.reduce_mean(self.td_error)
			q_trainer = tf.train.AdamOptimizer(learning_rate=0.00025)
			q_OP = q_trainer.minimize(self.q_loss)		 
		self.trainables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)		
		
def netCopyOps(fromNet, toNet, tau = 1):
	op_holder = []
	for idx,var in enumerate(fromNet.trainables[:]):
		op_holder.append(toNet.trainables[idx].assign((var.value()*tau) + ((1-tau)*toNet.trainables[idx].value())))	
	return op_holder