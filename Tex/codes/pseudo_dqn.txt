Initialize replay memory *($D$*) to capacity N



Initialize action-value function *($Q(s,a;\theta)$*) with random weights *($\theta$*)


Initialize target action-value function *($Q(s,a;\theta^-)$*) with weights *($\theta^- = \theta$*)
For episode = 1,M do
	Initialize sequence *($s_1 = \{x_1\}$*) and preprocessed sequence *($\phi_1 = \phi(s_1)$*)
	For 1 = 1,T do
		With probability *($\epsilon$*) select random action *($a_t$*)
		otherwise select *($a_t = argmax_a Q(\phi(s_t),a;\theta)$*)
		
		Execute action *($a_t$*) in emulator and observe reward *($r_t$*) and image *($x_{t+1}$*)
		Set *($s_{t+1} = s_t,a_t,x_{t+1}$*) and preprocess *($\phi_{t+1} = \phi(s_{t+1})$*)
		Store transition *($(\phi_t,a_t,r_t,\phi_{t+1})$*) in *($D$*)
		
		Sample random minibatch of transitions *($(\phi_j,a_j,r_j,\phi_{j+1})$*) from *($D$*)
		*(\textcolor{blue}{ Define $a^{max}(\phi_{j+1};\theta) = argmax_{a'} Q(\phi_{j+1},a';\theta) $}*) 
		*(\textcolor{blue}{ Define $Q^{j+1} = Q(\phi_{j+1},a^{max}(\phi_{t+1};\theta);\theta^-)$}*) *($\label{sourcecode_ddqn}$*)
		
		If episode terminates at step *($j+1$*) then set *($y_j = r_j$*),              Otherwise set *($y_j = r_j + \gamma * Q^{j+1}$*) 
		Perform a gradient descent step on *($\big(y_j - Q(\phi_j,a_j;\theta)\big)^2$*) with respect to the network parameters *($\theta$*)
		Update target network: *(\textcolor{blue}{$ \theta^- \leftarrow \tau * \theta + (1-\tau) \theta^- $}*)
	End For
End For