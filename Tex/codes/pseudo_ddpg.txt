


Randomly initialize critic network Q(s,a|*($\theta^Q$*)) with weights *($\theta^Q$*) 
	and actor *($\pi$*)(s|*($\theta^\pi$*)) with weights *($\theta^\pi$*).
	
Initialize target network Q' weights *($\theta^{Q'} \leftarrow \theta^Q$*)
	and *($\pi$*)' with weights *($\theta^{\pi'} \leftarrow \theta^\pi$*)
Initialize replay buffer R
for episode = 1, M do
	Initialize a random process *($\mathcal{N}$*) for action exploration
	Receive initial observation state s*($_1$*)
	for t = 1, T do
		Select action *($a_t = \pi(s_t|\theta^\pi) + \mathcal{N}_t$*) according to the current policy and exploration noise
		Execute action *($a_t$*) and observe reward *($r_t$*) and observe new state *($s_{t+1}$*)
		Store transition *($(s_t, a_t, r_t, s_{t+1})$*) in R
		
		Sample a random minibatch of N transitions *($(s_t, a_t, r_t, s_{t+1})$*) from R
		targetActorAction = *($\pi'(s_{i+1}|\theta^{\pi'})$*)
		targetCriticQ = *($ Q'(s_{i+1},$*)targetActorAction*($|\theta^{Q'}) $*)
		Set *($y_i = r_i + \gamma *$*)targetCriticQ  #only in nonterminal states
		
		Update critic by minimizing the loss: L = *($\frac{1}{N}\sum_i(y_i - Q(s_i, a_i|\theta^Q))^2$*)
		Find the sampled policy gradient:
				a_i = *($\pi(s_i|\theta^\pi) $*)
				*($\nabla_{\theta^\pi}J \approx \frac{1}{N}\sum_i \nabla_a Q(s_i,a_i|\theta^Q) \nabla_{\theta^\pi} \pi(s_i|\theta^\pi)$*)
		Update the actor policy using the sampled policy gradient
		Update the target networks: 
				*($ \theta^{Q'} \leftarrow \tau * \theta^Q + (1-\tau) \theta^{Q'} $*)
				*($ \theta^{\pi'} \leftarrow \tau * \theta^Q + (1-\tau) \theta^{\pi'} $*)
	end for
end for