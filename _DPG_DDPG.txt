best original Quote: "The critic estimates the action-value function while the actor ascends the gradient of the action-value function"
DPG
[define policy gradient first]
-General Idea: adjust the policy parameters in the direction of the policy gradient. 
-Deterministic policy gradient = expected gradient of Qfunction. Can be calculated efficiently, in contrast to the usual stochastic policy gradient
-In Continuous action spaces, one must use policy gradien algorithms.  Deterministic policy a = müh_theta(s)
-The gradient of the deterministic policy follows the gradient of the action-value function
-deterministic policy = limit of stochastic policy gradient, as variance tends to zero
-for stochastic policies, to get the gradient one must integrate over both state and action space, in determinstic case only the action space. --> less samples
-to explore full state and action space, stochastic policy is necessary --> off-policy. Choose actions according to stochastic policy for exploration, but learn about the determinstic gradient
--> estimate Q-function with ANNs, then update the policy parameters in the direciton of the approximate acton-value gradient

policy gradient algorithms are the most popular class of continuous action RL algorithms
performance objective J(pi) ist der expectedReward when following this policy
we adjust the parameters of the policy in the direction of th performance gradient delta_theta J(pi_theta). This gradient does not depend on the gradient of the state distribution

actor-critic BASES on the policy gradient theorem.
The actor adjusts the paremeters theta of the stoachstic policy by stochastic gradient AScent of equation (2), showing what you'll get with this policy. For that, it uses an approximating Q-function Q^w(s,a).
The CRITIC estimates the action-value function Q^w, using stuff like temporal differnece learning. The weights Q^w must be chosen, such that they minimize the MSE E...[Q^w(s,a)-Q^pi(s,a))²] (kurz vor (3)!!
if both of the conditions for the critic were fully fulfilled, then we wouldnt need to use a critic at all

all that was stochastic policy gradient, we don't care for that!!
----------------------- 

normally, we'd estimate Q* by Qpi/Qmüh, and then improve the policy w.r.t. the estimated actual Q*-function greedily by setting the action to perform in the situation as the argmax of the action.
In continuous action spaces, thats not possible, because we'd need a global maximization at every step.
WHat we can do instead is to move the policy into the direcition of the gradient of Q. If we have batches, we must of course average.
If we however apply the chain rule, we see that the policy improvement may be decomposed into the gradient of the action-value wrt the actions and the gradient of the policy wrt the policy params
They show, that there is no need to compute the gradient of the state distribution
Important now is, that the gradient of the valuefunction is the gradient of the policy * gradient of the Q-function
that works on-policy and off-policy, in the latter case for example with a Q-learning critic
-The critic estimates the action-value function while the actor ascends the gradient of the action-value function. Specifically, an actor adjusts the parameters  of the deterministic policy müh_theta(s) by stochastic gradient ascent of Equation 9.
The critic contains a differentiable approximation of Q*.

DDPG will be an off-policy deterministic actor-critic. That learns the determinsitci target policy müh_theta(s) from trajeectories generated by an arbitrary stochastic behaviour policy pi(s,a)
[EQUATION 15 IST HIER DIE RELEVANTE!!!] gradient der value-function following our policy = E[gradient der policy * gradient der Q-fucntion]

We now develop an actor-critic algorithm that updates the policy in the direction of the off-policy deterministic policy gradient.
Critic estimates the Q^w-function off-policy from trajectories of another policy
off-policy deterministic actor-critic...
critic lernt richtig schön off-policy style den Temporal difference error...
die weights des critics werden dann angepasst auf den gradient
die weigts des actors werden dann angepasst auf gradient derer selbst mal gradient vom critic 
dann haben die halt große arbeit nen critic zu finden, sodass die gradients wrt dem Q*-critic mit denen von deren critic ersetzt werden können, without affecting the DPG
so what they in the end need, as they can't have the original Q*, is a compatible off-policy deterministic actorcritic, which has also a learning critic. Hier noch nen LINEAR function approximator. That one learns via Q-learning 



One may view our deterministic actor-critic as analogous,
in a policy gradient context, to Q-learning (Watkins and
Dayan, 1992). Q-learning learns a deterministic greedy
policy, off-policy, while executing a noisy version of the

------------------------------------------------------------------------------------------------------------------- 
NUR BIS KAPITEL 3 LESEN!!!!

DDPG
start with However, while DQN solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action space. Cannot be straight-forward applied, as it relies on finding the action that maximizes the Q-function, which in the continuous case would require an iterative optimization process at each step
discreizing is bad anyway and leads to curse of dimensionality blabla [erst später, nicht in der itnro!!!]
uses batchnorm, da werde ich aber nicht drauf eingehen
policy pi maps states to propdist over actions! S->P(A)
"the goal of RL is to learn a policy which maximizes the expected return from the start distr. J = E_...[R1]... hmmm
"The DPG algorithm maintains a parameterized actor function müh(s|theta^müh) which specifies the current policy by deterministically mapping states to a specific action."
-deren (6), gucken welcher gleichung vom DPG das entsprciht und das einbauen
-equation 4 von denen ist schlichtes Q-learning!!
-dass die sowohl ne target-policy brauchen als auch nen target Q network 
-batchnorm haben sie genutzt damit sie die gleichen hyperparameter bei anderen größenordnungen an inputs haben können


Batchnorm: This technique normalizes each dimension across the samples in a minibatch to have unit mean and variance. In addition, it maintains a running average of the mean and variance to use for normalization during testing

-transitionfunction of the environment als p(s_t+1|s_t,a_t) ausdrücken??
-!!! REWARD FUNCTION EINFACH ALS R(s_t,a_t) ausdrücken!! WENN DER STATEÜBERGANG INDETERMINISTISCH IST IST DER REWARD _DARAN_ DETERMINISTISCH
-JA, bellman equation again auf Q!!!
-Am anfang schreiben dass ich immer auf deterministische target policies eingehen werde, doch eigentlich durch das off-policy sein sogar stimmt? wir lernen über die deterministische policy, in jedem fall
-die tatsache dass "that most optimization algorithms assume that the samples are independently and identically distributed" -> minibatches und replay buffer -dass durch das target-network das Q-learnen (sowohl bei DQN als auch bei DDPG) das problem "closer to supervised learning" ist, a problem for which robust soutions exist 
Satz über exploration: "An advantage of offpolicies algorithms such as DDPG is that we can treat the problem of exploration independently from the learning algorithm." --> das am anfang schreibne, dass ich das auch auf jeden fall mache!!
CODE: find_normalizers wieder zurück!!! das war doch richtig so, UNBEDINGT!!!
"For the exploration noise process we used temporally correlated noise in order to explore well in physical environments that have momentum."... MOMENTUM, BITCH!!